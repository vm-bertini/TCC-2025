{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "175d0cbc",
   "metadata": {},
   "source": [
    "# Predição de séries temporais de carga elétrica\n",
    "Este notebook coleta, organiza e visualiza dados públicos da ENTSO‑E para construir conjuntos de treino e avaliação de modelos de previsão temporal.\n",
    "Autor: Victor Mario Bertini (RA: 194761)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35751882",
   "metadata": {},
   "source": [
    "# Capítulo 1 — Coleta de dados\n",
    "Nesta etapa baixamos dados da ENTSO‑E via API REST e salvamos em formato Parquet dentro da pasta `data/`. O objetivo é permitir reexecuções parciais: cada subetapa persiste artefatos para evitar refazer todo o fluxo.\n",
    "Fonte: https://transparency.entsoe.eu/content/static_content/Static%20content/web%20api/Guide.html\n",
    "Escopo desta versão do notebook:\n",
    "- Países europeus selecionados (DE, FR, IT, ES, PT, CZ, NL, BE, AT, PL)\n",
    "- Datasets principais:\n",
    "  - Load — Actual Total (carga realizada agregada)\n",
    "  - Market — Energy Prices (preços de energia)\n",
    "- Período: até 180 dias retroativos\n",
    "Subcapítulos desta etapa:\n",
    "1. Coleta e salvamento bruto (Parquet)\n",
    "2. Visualização exploratória (carga e preço)\n",
    "3. Preparação de dados para treino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304b731b",
   "metadata": {},
   "source": [
    "## Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb96e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checagem/instalação leve de dependências\n",
    "print(\"Verificando dependências (pyarrow para Parquet)...\")\n",
    "\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    print(f\"PyArrow disponível: {pa.__version__}\")\n",
    "except Exception:\n",
    "    print(\"Instalando pyarrow...\")\n",
    "    !pip install --upgrade \"pyarrow>=18\" --quiet\n",
    "    import importlib\n",
    "    importlib.invalidate_caches()\n",
    "    import pyarrow as pa\n",
    "    print(f\"PyArrow instalado: {pa.__version__}\")\n",
    "\n",
    "# fastparquet é opcional\n",
    "try:\n",
    "    import fastparquet  # noqa: F401\n",
    "    print(\"fastparquet disponível (opcional)\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Outras bibliotecas sob demanda\n",
    "for lib in [\n",
    "    \"numpy\", \"python-dotenv\", \"pandas\", \"matplotlib\", \"seaborn\",\n",
    "    \"scikit-learn\", \"tensorflow\", \"keras\", \"lxml\", \"pytz\", \"requests\", \"optuna\"\n",
    "]:\n",
    "    try:\n",
    "        __import__(lib)\n",
    "    except ImportError:\n",
    "        print(f\"Instalando {lib}...\")\n",
    "        !pip install {lib} --quiet\n",
    "\n",
    "print(\"Dependências prontas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c2a758",
   "metadata": {},
   "source": [
    "## Coleta de dados brutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc09e94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports para a API e utilidades\n",
    "import os\n",
    "import requests\n",
    "import pandas\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta, date\n",
    "import pytz\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Carregar variáveis de ambiente do .env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a08fe5",
   "metadata": {},
   "source": [
    "### Definição de funções de coleta de dados e de salvamento em parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b0353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from lxml import etree\n",
    "\n",
    "# ---------------- CONFIG ---------------- #\n",
    "COUNTRY_DOMAINS = {\n",
    "    \"FR\": {\"domain\": \"10YFR-RTE------C\"},\n",
    "    \"ES\": {\"domain\": \"10YES-REE------0\"},\n",
    "    \"PT\": {\"domain\": \"10YPT-REN------W\"},\n",
    "    \"CZ\": {\"domain\": \"10YCZ-CEPS-----N\"},\n",
    "    \"NL\": {\"domain\": \"10YNL----------L\"},\n",
    "    \"BE\": {\"domain\": \"10YBE----------2\"},\n",
    "    \"PL\": {\"domain\": \"10YPL-AREA-----S\"},\n",
    "}\n",
    "\n",
    "DATA_ITEMS = [\n",
    "    {'key': 'load_total', 'documentType': 'A65', 'processType': 'A16', 'domainParam': 'outBiddingZone_Domain', 'parser': 'load'},\n",
    "    {'key': 'market_prices', 'documentType': 'A44', 'processType': 'A07', 'domainParamIn': 'in_Domain', 'domainParamOut': 'out_Domain', 'parser': 'price'}\n",
    "]\n",
    "\n",
    "ENTSOE_TOKEN = os.environ.get(\"ENTSOE_SECURITY_TOKEN\")\n",
    "BASE_URL = \"https://web-api.tp.entsoe.eu/api\"\n",
    "MAX_WORKERS = 100\n",
    "RAW_DIR = os.path.join(\"data\", \"raw\")\n",
    "PARQUET_COMPRESSION = \"zstd\"\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "\n",
    "# ---------------- HELPERS ---------------- #\n",
    "def build_params(item, domain, start_dt, end_dt):\n",
    "    \"\"\"Build API query parameters.\"\"\"\n",
    "    return {\n",
    "        \"securityToken\": ENTSOE_TOKEN,\n",
    "        \"documentType\": item['documentType'],\n",
    "        \"periodStart\": start_dt.strftime(\"%Y%m%d%H%M\"),\n",
    "        \"periodEnd\": end_dt.strftime(\"%Y%m%d%H%M\"),\n",
    "        **({\"processType\": item['processType']} if item.get('processType') else {}),\n",
    "        **({item['domainParamIn']: domain, item['domainParamOut']: domain} if item.get('domainParamIn') else {item.get('domainParam'): domain})\n",
    "    }\n",
    "\n",
    "def parse_xml_points(xml_bytes: bytes, parser_type: str) -> pd.DataFrame:\n",
    "    \"\"\"Parse ENTSO-E XML response into a DataFrame with proper datetime.\"\"\"\n",
    "    root = etree.fromstring(xml_bytes)\n",
    "    period_elem = root.find(\".//{*}Period\")\n",
    "    if period_elem is None:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    start_elem = period_elem.find(\"{*}timeInterval/{*}start\")\n",
    "    period_start = start_elem.text if start_elem is not None else None\n",
    "\n",
    "    res_elem = period_elem.find(\"{*}resolution\")\n",
    "    resolution = res_elem.text if res_elem is not None else None\n",
    "\n",
    "    rows = []\n",
    "    for point in period_elem.findall(\"{*}Point\"):\n",
    "        pos_elem = point.find(\"{*}position\")\n",
    "        if pos_elem is None or pos_elem.text is None:\n",
    "            continue\n",
    "        pos = int(pos_elem.text)\n",
    "\n",
    "        if parser_type == \"load\":\n",
    "            val_elem = point.find(\"{*}quantity\")\n",
    "            if val_elem is None or val_elem.text is None:\n",
    "                continue\n",
    "            rows.append({\n",
    "                'position': pos,\n",
    "                'quantity_MW': float(val_elem.text),\n",
    "                'period_start': period_start,\n",
    "                'resolution': resolution\n",
    "            })\n",
    "        elif parser_type == \"price\":\n",
    "            val_elem = point.find(\"{*}price.amount\")\n",
    "            if val_elem is None or val_elem.text is None:\n",
    "                continue\n",
    "            rows.append({\n",
    "                'position': pos,\n",
    "                'price_EUR_MWh': float(val_elem.text),\n",
    "                'period_start': period_start,\n",
    "                'resolution': resolution\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    if not df.empty and 'period_start' in df.columns and 'resolution' in df.columns:\n",
    "        # extract minutes from resolution string (positions 2:4)\n",
    "        df['minutes'] = df['resolution'].str[2:4].astype(int)\n",
    "        df['datetime'] = pd.to_datetime(df['period_start'], utc=True) + pd.to_timedelta((df['position'] - 1) * df['minutes'], unit='minutes')\n",
    "        df.drop(columns=['minutes'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def fetch_day(session, item, country, day: datetime, retries=3, delay=5):\n",
    "    \"\"\"Fetch a single day of data for a given country and item.\"\"\"\n",
    "    domain = COUNTRY_DOMAINS[country]['domain']\n",
    "    start_dt = day\n",
    "    end_dt = start_dt + timedelta(days=1)\n",
    "    params = build_params(item, domain, start_dt, end_dt)\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            r = session.get(BASE_URL, params=params, timeout=30)\n",
    "            r.raise_for_status()\n",
    "            df = parse_xml_points(r.content, item['parser'])\n",
    "            if df.empty:\n",
    "                return pd.DataFrame()\n",
    "            df['country'] = country\n",
    "            return df\n",
    "        except (requests.exceptions.RequestException, etree.XMLSyntaxError) as e:\n",
    "            print(f\"[WARNING] Attempt {attempt+1} failed for {country} {item['key']} {day}: {e}\")\n",
    "            time.sleep(delay * (2 ** attempt))\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def daterange(start: datetime, end: datetime):\n",
    "    \"\"\"Yield datetime objects for each day between start and end, inclusive of start, exclusive of end.\"\"\"\n",
    "    current = start\n",
    "    while current < end:\n",
    "        yield current\n",
    "        current += timedelta(days=1)\n",
    "\n",
    "def fetch_last_days(lookback_days: int, reference_datetime: datetime = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch ENTSO-E data for the last `lookback_days` days, rounding reference time down to the previous full hour.\n",
    "    \"\"\"\n",
    "    if reference_datetime is None:\n",
    "        reference_datetime = datetime.now()\n",
    "\n",
    "    # Round down to previous hour\n",
    "    end_dt = reference_datetime.replace(minute=0, second=0, microsecond=0)\n",
    "    start_dt = end_dt - timedelta(days=lookback_days)\n",
    "\n",
    "    print(f\"[INFO] Fetching data from {start_dt} to {end_dt} (rounded to hour)\")\n",
    "\n",
    "    all_dfs = []\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = []\n",
    "        with requests.Session() as session:\n",
    "            for item in DATA_ITEMS:\n",
    "                for country in COUNTRY_DOMAINS:\n",
    "                    for single_day in daterange(start_dt, end_dt):\n",
    "                        futures.append(executor.submit(fetch_day, session, item, country, single_day))\n",
    "            for f in as_completed(futures):\n",
    "                df = f.result()\n",
    "                if not df.empty:\n",
    "                    all_dfs.append(df)\n",
    "\n",
    "    if all_dfs:\n",
    "        merged = pd.concat(all_dfs, ignore_index=True)\n",
    "        merged = merged.pivot_table(index=['datetime', 'country'],\n",
    "                                    values=['quantity_MW', 'price_EUR_MWh'],\n",
    "                                    aggfunc='first').reset_index()\n",
    "        merged['quantity_MW'] = merged.groupby('country')['quantity_MW'].ffill()\n",
    "        merged['price_EUR_MWh'] = merged.groupby('country')['price_EUR_MWh'].ffill()\n",
    "\n",
    "        filename = f\"raw_dataset.parquet\"\n",
    "        path = os.path.join(RAW_DIR, filename)\n",
    "        merged.to_parquet(path, engine=\"pyarrow\", compression=PARQUET_COMPRESSION, index=False)\n",
    "        print(f\"[INFO] Saved merged data to {path}\")\n",
    "        return merged\n",
    "    else:\n",
    "        return pd.DataFrame(columns=['datetime', 'country', 'quantity_MW', 'price_EUR_MWh'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdc8f17",
   "metadata": {},
   "source": [
    "## Coletando dados\n",
    "Nesta seção vamos buscar dados históricos de carga (A65) e preços de energia (A44). O período é exclusivamente passado (até ontem):\n",
    "- Carga (A65): outBiddingZone_Domain = <EIC do país>\n",
    "- Preços de energia (A44): in_Domain = <EIC do país> e out_Domain = <EIC do país>\n",
    "Os resultados serão salvos como arquivos Parquet em `data/` para reutilização nas próximas etapas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18ea6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback_days = 365\n",
    "df = fetch_last_days(lookback_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d269fae",
   "metadata": {},
   "source": [
    "## Visualização dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d8f1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------- CONFIG ---------------- #\n",
    "RAW_DIR = \"data/raw\"\n",
    "PARQUET_FILE = os.path.join(RAW_DIR, \"raw_dataset.parquet\")\n",
    "\n",
    "for country in COUNTRY_DOMAINS.keys():\n",
    "    # ---------------- LOAD DATA ---------------- #\n",
    "    if not os.path.exists(PARQUET_FILE):\n",
    "        raise FileNotFoundError(f\"Parquet file not found at {PARQUET_FILE}. Run fetch_last_days() first.\")\n",
    "\n",
    "    df = pd.read_parquet(PARQUET_FILE)\n",
    "\n",
    "    # Filter by country and sort by datetime\n",
    "    df = df[df['country'] == country].sort_values('datetime')\n",
    "\n",
    "    # ---------------- PLOTTING ---------------- #\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\n",
    "\n",
    "    # Plot Quantity MW\n",
    "    axes[0].plot(df['datetime'], df['quantity_MW'], color='blue', label='Quantity MW')\n",
    "    axes[0].set_ylabel('Quantity MW')\n",
    "    axes[0].set_title(f'{country} - Quantity MW')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    # Plot Price EUR/MWh\n",
    "    axes[1].plot(df['datetime'], df['price_EUR_MWh'], color='red', label='Price EUR/MWh')\n",
    "    axes[1].set_ylabel('Price EUR/MWh')\n",
    "    axes[1].set_title(f'{country} - Price EUR/MWh')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "\n",
    "    # Common X-axis\n",
    "    plt.xlabel('Datetime')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9d9e78",
   "metadata": {},
   "source": [
    "## Capitulo 2 — Definição dos Problemas (Carga e Mercado)\n",
    "\n",
    "Nesta etapa definimos três problemas, agora baseados em carga (load) e mercado (preço de energia). Cada problema possui variantes A/B/C para complexidade crescente.\n",
    "\n",
    "- Nível 1 — Previsão de carga - feature de carga (A65)\n",
    "  - A: 1 país (ex.: AT), lookback curto (7 dias), horizonte 1 dia (96 passos de 15 min)\n",
    "  - B: 1 país, lookback longo (30 dias), horizonte 3 dias\n",
    "  - C: Multi-país (ex.: AT/DE/FR) com lookbacks máximo (60 dias) horizonte de 7 dias \n",
    "\n",
    "- Nível 2 — Previsão de carga e preço de mercado- feature de carga + preço de mercado (A44 + A65)\n",
    "  - A/B/C como acima, mas prevendo simultaneamente preço e carga\n",
    "\n",
    "Abaixo, criamos construtores de datasets (builders) que leem os Parquets salvos em `data/` e montam janelas de treino com passo de 15 minutos, iniciando sempre à meia‑noite do dia. Os builders retornam tuplas (X, Y, feat_cols, target_cols, country).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

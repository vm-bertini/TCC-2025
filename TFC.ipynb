{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a283bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\OneDrive\\Documentos\\TCC\\TCC-2025\\tfc_venv\\Lib\\site-packages\\pytorch_forecasting\\models\\base\\_base_model.py:28: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports necessários para TFT (PyTorch Forecasting)\n",
    "# Tenta instalar automaticamente pacotes ausentes\n",
    "import sys, subprocess, importlib\n",
    "\n",
    "def ensure_any(mod_name: str, pip_candidates: list[str]):\n",
    "    try:\n",
    "        return importlib.import_module(mod_name)\n",
    "    except Exception:\n",
    "        last_err = None\n",
    "        for candidate in pip_candidates:\n",
    "            try:\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", candidate, \"-q\"])  # instala silenciosamente\n",
    "                return importlib.import_module(mod_name)\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "        raise last_err if last_err else ImportError(f\"Falha ao instalar {mod_name}\")\n",
    "\n",
    "# Garantir dependências principais (sem TimesFM)\n",
    "ensure_any('torch', ['torch'])\n",
    "ensure_any('pytorch_forecasting', ['pytorch-forecasting'])\n",
    "ensure_any('pytorch_lightning', ['pytorch-lightning'])\n",
    "\n",
    "# Importa libs\n",
    "import torch\n",
    "import pytorch_forecasting as ptf\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_lightning import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c55770",
   "metadata": {},
   "source": [
    "# Predição de séries temporais de carga elétrica\n",
    "\n",
    "Este notebook coleta, organiza e visualiza dados públicos da ENTSO‑E para construir conjuntos de treino e avaliação de modelos de previsão temporal. O foco atual está em carga realizada e preços de energia (A65 e A44) para países europeus selecionados.\n",
    "\n",
    "Autor: Victor Mario Bertini (RA: 194761)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98572619",
   "metadata": {},
   "source": [
    "# Etapa 1 — Coleta de dados\n",
    "\n",
    "Nesta etapa baixamos dados da ENTSO‑E via API REST e salvamos em formato Parquet dentro da pasta `data/`. O objetivo é permitir reexecuções parciais: cada subetapa persiste artefatos para evitar refazer todo o fluxo.\n",
    "\n",
    "Fonte: https://transparency.entsoe.eu/content/static_content/Static%20content/web%20api/Guide.html\n",
    "\n",
    "Escopo desta versão do notebook:\n",
    "- Países europeus selecionados (DE, FR, IT, ES, PT, CZ, NL, BE, AT, PL)\n",
    "- Datasets principais:\n",
    "  - Load — Actual Total (carga realizada agregada)\n",
    "  - Market — Energy Prices (preços de energia)\n",
    "- Período: até 180 dias retroativos\n",
    "\n",
    "Subcapítulos desta etapa:\n",
    "1. Coleta e salvamento bruto (Parquet)\n",
    "2. Visualização exploratória (carga e preço)\n",
    "3. Preparação de dados para treino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1149ce94",
   "metadata": {},
   "source": [
    "## Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce23fd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando dependências (pyarrow para Parquet)...\n",
      "PyArrow disponível: 21.0.0\n",
      "Instalando python-dotenv...\n",
      "Instalando scikit-learn...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependências prontas\n"
     ]
    }
   ],
   "source": [
    "# Checagem/instalação leve de dependências\n",
    "print(\"Verificando dependências (pyarrow para Parquet)...\")\n",
    "\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    print(f\"PyArrow disponível: {pa.__version__}\")\n",
    "except Exception:\n",
    "    print(\"Instalando pyarrow...\")\n",
    "    !pip install --upgrade \"pyarrow>=18\" --quiet\n",
    "    import importlib\n",
    "    importlib.invalidate_caches()\n",
    "    import pyarrow as pa\n",
    "    print(f\"PyArrow instalado: {pa.__version__}\")\n",
    "\n",
    "# fastparquet é opcional\n",
    "try:\n",
    "    import fastparquet  # noqa: F401\n",
    "    print(\"fastparquet disponível (opcional)\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Outras bibliotecas sob demanda\n",
    "for lib in [\n",
    "    \"numpy\", \"python-dotenv\", \"pandas\", \"matplotlib\", \"seaborn\",\n",
    "    \"scikit-learn\", \"tensorflow\", \"keras\", \"lxml\", \"pytz\", \"requests\"\n",
    "]:\n",
    "    try:\n",
    "        __import__(lib)\n",
    "    except ImportError:\n",
    "        print(f\"Instalando {lib}...\")\n",
    "        !pip install {lib} --quiet\n",
    "\n",
    "print(\"Dependências prontas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171668e5",
   "metadata": {},
   "source": [
    "## Capítulo 1 — Coleta de dados brutos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cc7e78",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d89d36dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports para a API e utilidades\n",
    "import os\n",
    "import requests\n",
    "import pandas\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta, date\n",
    "import pytz\n",
    "\n",
    "# Carregar variáveis de ambiente do .env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3099e006",
   "metadata": {},
   "source": [
    "### Definição de funções de coleta de dados e de salvamento em parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06c042f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "from datetime import datetime, timedelta, date\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from lxml import etree\n",
    "\n",
    "# ---------------- CONFIG ---------------- #\n",
    "COUNTRY_DOMAINS = {\n",
    "    \"DE\": {\"domain\": \"10Y1001A1001A83F\"},\n",
    "    \"FR\": {\"domain\": \"10YFR-RTE------C\"},\n",
    "    \"IT\": {\"domain\": \"10YIT-GRTN-----B\"},\n",
    "    \"ES\": {\"domain\": \"10YES-REE------0\"},\n",
    "    \"PT\": {\"domain\": \"10YPT-REN------W\"},\n",
    "    \"CZ\": {\"domain\": \"10YCZ-CEPS-----N\"},\n",
    "    \"NL\": {\"domain\": \"10YNL----------L\"},\n",
    "    \"BE\": {\"domain\": \"10YBE----------2\"},\n",
    "    \"AT\": {\"domain\": \"10YAT-APG------L\"},\n",
    "    \"PL\": {\"domain\": \"10YPL-AREA-----S\"},\n",
    "}\n",
    "\n",
    "DATA_ITEMS = [\n",
    "    {'key': 'load_total', 'documentType': 'A65', 'processType': 'A16', 'domainParam': 'outBiddingZone_Domain', 'parser': 'load'},\n",
    "    {'key': 'market_prices', 'documentType': 'A44', 'processType': None, 'domainParamIn': 'in_Domain', 'domainParamOut': 'out_Domain', 'parser': 'price'}\n",
    "]\n",
    "\n",
    "CHUNK_DAYS = 30\n",
    "PARQUET_COMPRESSION = \"zstd\"\n",
    "ENTSOE_TOKEN = os.environ.get(\"ENTSOE_SECURITY_TOKEN\")\n",
    "BASE_URL = \"https://web-api.tp.entsoe.eu/api\"\n",
    "RAW_DIR = os.path.join(\"data\", \"raw\")\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "\n",
    "# ---------------- HELPERS ---------------- #\n",
    "def chunk_period(start: date, end: date, chunk_days: int = 30):\n",
    "    cur = start\n",
    "    delta = timedelta(days=chunk_days - 1)\n",
    "    while cur <= end:\n",
    "        yield cur, min(end, cur + delta)\n",
    "        cur = cur + timedelta(days=chunk_days)\n",
    "\n",
    "def build_params(item, domain, start, end):\n",
    "    p = {\n",
    "        \"securityToken\": ENTSOE_TOKEN,\n",
    "        \"documentType\": item['documentType'],\n",
    "        \"periodStart\": start.strftime(\"%Y%m%d%H%M\"),\n",
    "        \"periodEnd\": end.strftime(\"%Y%m%d%H%M\"),\n",
    "    }\n",
    "    if item.get('processType'):\n",
    "        p['processType'] = item['processType']\n",
    "    if item.get('domainParamIn') and item.get('domainParamOut'):\n",
    "        p[item['domainParamIn']] = domain\n",
    "        p[item['domainParamOut']] = domain\n",
    "    elif item.get('domainParam'):\n",
    "        p[item['domainParam']] = domain\n",
    "\n",
    "    # Force PT15M for market prices\n",
    "    if item['parser'] == \"price\":\n",
    "        p['resolution'] = \"PT15M\"\n",
    "    \n",
    "    return p\n",
    "\n",
    "\n",
    "def parse_xml_points(xml_bytes: bytes, parser_type: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse ENTSO-E XML points and include resolution and period info.\n",
    "    parser_type: 'load' or 'price'\n",
    "    \"\"\"\n",
    "    root = etree.fromstring(xml_bytes)\n",
    "    \n",
    "    # Find period info\n",
    "    period_elem = root.find(\".//{*}Period\")\n",
    "    resolution = None\n",
    "    period_start, period_end = None, None\n",
    "    if period_elem is not None:\n",
    "        res_elem = period_elem.find(\"{*}resolution\")\n",
    "        if res_elem is not None:\n",
    "            resolution = res_elem.text\n",
    "        start_elem = period_elem.find(\"{*}timeInterval/{*}start\")\n",
    "        end_elem = period_elem.find(\"{*}timeInterval/{*}end\")\n",
    "        if start_elem is not None and end_elem is not None:\n",
    "            period_start = start_elem.text\n",
    "            period_end = end_elem.text\n",
    "    \n",
    "    # Parse points\n",
    "    rows = []\n",
    "    for point in period_elem.findall(\"{*}Point\"):\n",
    "        pos_elem = point.find(\"{*}position\")\n",
    "        if pos_elem is None or pos_elem.text is None:\n",
    "            continue\n",
    "        pos = int(pos_elem.text)\n",
    "        \n",
    "        if parser_type == \"load\":\n",
    "            val_elem = point.find(\"{*}quantity\")\n",
    "            if val_elem is None or val_elem.text is None:\n",
    "                continue\n",
    "            rows.append({\n",
    "                'position': pos,\n",
    "                'quantity_MW': float(val_elem.text),\n",
    "                'resolution': resolution,\n",
    "                'period_start': period_start,\n",
    "                'period_end': period_end\n",
    "            })\n",
    "        elif parser_type == \"price\":\n",
    "            val_elem = point.find(\"{*}price.amount\")\n",
    "            if val_elem is None or val_elem.text is None:\n",
    "                continue\n",
    "            rows.append({\n",
    "                'position': pos,\n",
    "                'price_EUR_MWh': float(val_elem.text),\n",
    "                'resolution': resolution,\n",
    "                'period_start': period_start,\n",
    "                'period_end': period_end\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "\n",
    "def fetch_chunk(session, item, domain, start, end, retries=3, delay=5):\n",
    "    \"\"\"Fetch a single chunk with retry mechanism.\"\"\"\n",
    "    params = build_params(item, domain, start, end)\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            r = session.get(BASE_URL, params=params, timeout=30)\n",
    "            r.raise_for_status()\n",
    "            df = parse_xml_points(r.content, item['parser'])\n",
    "            if df.empty:\n",
    "                return pd.DataFrame()\n",
    "            # Add metadata columns\n",
    "            df['country'] = domain\n",
    "            df['dataset'] = item['key']\n",
    "            if item['parser'] == 'price':\n",
    "                df['currency'] = 'EUR'\n",
    "            # Also save period for later datetime generation\n",
    "            df['period_start'] = start\n",
    "            df['period_end'] = end\n",
    "            return df\n",
    "        except (requests.exceptions.RequestException, etree.XMLSyntaxError) as e:\n",
    "            print(f\"Warning: attempt {attempt+1} failed for {domain} {item['key']} ({start} to {end}): {e}\")\n",
    "            time.sleep(delay * (2 ** attempt))  # exponential backoff\n",
    "    return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673235f0",
   "metadata": {},
   "source": [
    "## Coletando dados\n",
    "\n",
    "Nesta seção vamos buscar dados históricos de carga (A65) e preços de energia (A44). O período é exclusivamente passado (até ontem):\n",
    "- Carga (A65): outBiddingZone_Domain = <EIC do país>\n",
    "- Preços de energia (A44): in_Domain = <EIC do país> e out_Domain = <EIC do país>\n",
    "\n",
    "Os resultados serão salvos como arquivos Parquet em `data/` para reutilização nas próximas etapas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1d0275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: attempt 1 failed for 10YNL----------L load_total (2025-05-21 to 2025-06-19): ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[SAVED] 8,448 rows\n",
      "[SAVED] 33,792 rows\n",
      "[SAVED] 25,040 rows\n",
      "Warning: attempt 1 failed for 10Y1001A1001A83F market_prices (2024-10-23 to 2024-11-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202410230000&periodEnd=202411210000&in_Domain=10Y1001A1001A83F&out_Domain=10Y1001A1001A83F&resolution=PT15M\n",
      "[SAVED] 33,792 rows\n",
      "[SAVED] 33,792 rows\n",
      "[SAVED] 27,048 rows\n",
      "Warning: attempt 1 failed for 10YFR-RTE------C market_prices (2024-10-23 to 2024-11-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202410230000&periodEnd=202411210000&in_Domain=10YFR-RTE------C&out_Domain=10YFR-RTE------C&resolution=PT15M\n",
      "Warning: attempt 1 failed for 10YIT-GRTN-----B market_prices (2024-10-23 to 2024-11-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202410230000&periodEnd=202411210000&in_Domain=10YIT-GRTN-----B&out_Domain=10YIT-GRTN-----B&resolution=PT15M\n",
      "Warning: attempt 1 failed for 10YES-REE------0 market_prices (2024-10-23 to 2024-11-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202410230000&periodEnd=202411210000&in_Domain=10YES-REE------0&out_Domain=10YES-REE------0&resolution=PT15M\n",
      "[SAVED] 32,158 rows\n",
      "Warning: attempt 1 failed for 10YPT-REN------W market_prices (2024-10-23 to 2024-11-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202410230000&periodEnd=202411210000&in_Domain=10YPT-REN------W&out_Domain=10YPT-REN------W&resolution=PT15M\n",
      "[SAVED] 33,792 rows\n",
      "Warning: attempt 2 failed for 10Y1001A1001A83F market_prices (2024-10-23 to 2024-11-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202410230000&periodEnd=202411210000&in_Domain=10Y1001A1001A83F&out_Domain=10Y1001A1001A83F&resolution=PT15M\n",
      "[SAVED] 33,792 rows\n",
      "Warning: attempt 1 failed for 10YCZ-CEPS-----N market_prices (2024-10-23 to 2024-11-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202410230000&periodEnd=202411210000&in_Domain=10YCZ-CEPS-----N&out_Domain=10YCZ-CEPS-----N&resolution=PT15M\n",
      "[SAVED] 33,792 rows\n",
      "Warning: attempt 1 failed for 10YNL----------L market_prices (2024-10-23 to 2024-11-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202410230000&periodEnd=202411210000&in_Domain=10YNL----------L&out_Domain=10YNL----------L&resolution=PT15M\n",
      "Warning: attempt 1 failed for 10YBE----------2 market_prices (2024-10-23 to 2024-11-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202410230000&periodEnd=202411210000&in_Domain=10YBE----------2&out_Domain=10YBE----------2&resolution=PT15M\n",
      "Warning: attempt 2 failed for 10YFR-RTE------C market_prices (2024-10-23 to 2024-11-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202410230000&periodEnd=202411210000&in_Domain=10YFR-RTE------C&out_Domain=10YFR-RTE------C&resolution=PT15M\n",
      "Warning: attempt 2 failed for 10YIT-GRTN-----B market_prices (2024-10-23 to 2024-11-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202410230000&periodEnd=202411210000&in_Domain=10YIT-GRTN-----B&out_Domain=10YIT-GRTN-----B&resolution=PT15M\n",
      "Warning: attempt 2 failed for 10YES-REE------0 market_prices (2024-10-23 to 2024-11-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202410230000&periodEnd=202411210000&in_Domain=10YES-REE------0&out_Domain=10YES-REE------0&resolution=PT15M\n",
      "Warning: attempt 2 failed for 10YPT-REN------W market_prices (2024-10-23 to 2024-11-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202410230000&periodEnd=202411210000&in_Domain=10YPT-REN------W&out_Domain=10YPT-REN------W&resolution=PT15M\n",
      "Warning: attempt 2 failed for 10YCZ-CEPS-----N market_prices (2024-10-23 to 2024-11-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202410230000&periodEnd=202411210000&in_Domain=10YCZ-CEPS-----N&out_Domain=10YCZ-CEPS-----N&resolution=PT15M\n",
      "Warning: attempt 2 failed for 10YNL----------L market_prices (2024-10-23 to 2024-11-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202410230000&periodEnd=202411210000&in_Domain=10YNL----------L&out_Domain=10YNL----------L&resolution=PT15M\n",
      "Warning: attempt 2 failed for 10YBE----------2 market_prices (2024-10-23 to 2024-11-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202410230000&periodEnd=202411210000&in_Domain=10YBE----------2&out_Domain=10YBE----------2&resolution=PT15M\n",
      "Warning: attempt 3 failed for 10Y1001A1001A83F market_prices (2024-10-23 to 2024-11-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202410230000&periodEnd=202411210000&in_Domain=10Y1001A1001A83F&out_Domain=10Y1001A1001A83F&resolution=PT15M\n",
      "Warning: attempt 3 failed for 10YFR-RTE------C market_prices (2024-10-23 to 2024-11-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202410230000&periodEnd=202411210000&in_Domain=10YFR-RTE------C&out_Domain=10YFR-RTE------C&resolution=PT15M\n",
      "Warning: attempt 3 failed for 10YIT-GRTN-----B market_prices (2024-10-23 to 2024-11-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202410230000&periodEnd=202411210000&in_Domain=10YIT-GRTN-----B&out_Domain=10YIT-GRTN-----B&resolution=PT15M\n",
      "Warning: attempt 3 failed for 10YES-REE------0 market_prices (2024-10-23 to 2024-11-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202410230000&periodEnd=202411210000&in_Domain=10YES-REE------0&out_Domain=10YES-REE------0&resolution=PT15M\n",
      "Warning: attempt 3 failed for 10YPT-REN------W market_prices (2024-10-23 to 2024-11-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202410230000&periodEnd=202411210000&in_Domain=10YPT-REN------W&out_Domain=10YPT-REN------W&resolution=PT15M\n",
      "Warning: attempt 3 failed for 10YCZ-CEPS-----N market_prices (2024-10-23 to 2024-11-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202410230000&periodEnd=202411210000&in_Domain=10YCZ-CEPS-----N&out_Domain=10YCZ-CEPS-----N&resolution=PT15M\n",
      "Warning: attempt 3 failed for 10YNL----------L market_prices (2024-10-23 to 2024-11-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202410230000&periodEnd=202411210000&in_Domain=10YNL----------L&out_Domain=10YNL----------L&resolution=PT15M\n",
      "Warning: attempt 3 failed for 10YBE----------2 market_prices (2024-10-23 to 2024-11-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202410230000&periodEnd=202411210000&in_Domain=10YBE----------2&out_Domain=10YBE----------2&resolution=PT15M\n",
      "Warning: attempt 1 failed for 10Y1001A1001A83F market_prices (2024-11-22 to 2024-12-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202411220000&periodEnd=202412210000&in_Domain=10Y1001A1001A83F&out_Domain=10Y1001A1001A83F&resolution=PT15M\n",
      "Warning: attempt 1 failed for 10YFR-RTE------C market_prices (2024-11-22 to 2024-12-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202411220000&periodEnd=202412210000&in_Domain=10YFR-RTE------C&out_Domain=10YFR-RTE------C&resolution=PT15M\n",
      "Warning: attempt 1 failed for 10YIT-GRTN-----B market_prices (2024-11-22 to 2024-12-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202411220000&periodEnd=202412210000&in_Domain=10YIT-GRTN-----B&out_Domain=10YIT-GRTN-----B&resolution=PT15M\n",
      "Warning: attempt 1 failed for 10YES-REE------0 market_prices (2024-11-22 to 2024-12-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202411220000&periodEnd=202412210000&in_Domain=10YES-REE------0&out_Domain=10YES-REE------0&resolution=PT15M\n",
      "Warning: attempt 1 failed for 10YPT-REN------W market_prices (2024-11-22 to 2024-12-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202411220000&periodEnd=202412210000&in_Domain=10YPT-REN------W&out_Domain=10YPT-REN------W&resolution=PT15M\n",
      "Warning: attempt 2 failed for 10Y1001A1001A83F market_prices (2024-11-22 to 2024-12-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202411220000&periodEnd=202412210000&in_Domain=10Y1001A1001A83F&out_Domain=10Y1001A1001A83F&resolution=PT15M\n",
      "Warning: attempt 1 failed for 10YCZ-CEPS-----N market_prices (2024-11-22 to 2024-12-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202411220000&periodEnd=202412210000&in_Domain=10YCZ-CEPS-----N&out_Domain=10YCZ-CEPS-----N&resolution=PT15M\n",
      "Warning: attempt 1 failed for 10YNL----------L market_prices (2024-11-22 to 2024-12-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202411220000&periodEnd=202412210000&in_Domain=10YNL----------L&out_Domain=10YNL----------L&resolution=PT15M\n",
      "Warning: attempt 1 failed for 10YBE----------2 market_prices (2024-11-22 to 2024-12-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202411220000&periodEnd=202412210000&in_Domain=10YBE----------2&out_Domain=10YBE----------2&resolution=PT15M\n",
      "Warning: attempt 2 failed for 10YFR-RTE------C market_prices (2024-11-22 to 2024-12-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202411220000&periodEnd=202412210000&in_Domain=10YFR-RTE------C&out_Domain=10YFR-RTE------C&resolution=PT15M\n",
      "Warning: attempt 2 failed for 10YIT-GRTN-----B market_prices (2024-11-22 to 2024-12-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202411220000&periodEnd=202412210000&in_Domain=10YIT-GRTN-----B&out_Domain=10YIT-GRTN-----B&resolution=PT15M\n",
      "Warning: attempt 2 failed for 10YES-REE------0 market_prices (2024-11-22 to 2024-12-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202411220000&periodEnd=202412210000&in_Domain=10YES-REE------0&out_Domain=10YES-REE------0&resolution=PT15M\n",
      "Warning: attempt 2 failed for 10YPT-REN------W market_prices (2024-11-22 to 2024-12-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202411220000&periodEnd=202412210000&in_Domain=10YPT-REN------W&out_Domain=10YPT-REN------W&resolution=PT15M\n",
      "Warning: attempt 2 failed for 10YCZ-CEPS-----N market_prices (2024-11-22 to 2024-12-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202411220000&periodEnd=202412210000&in_Domain=10YCZ-CEPS-----N&out_Domain=10YCZ-CEPS-----N&resolution=PT15M\n",
      "Warning: attempt 2 failed for 10YNL----------L market_prices (2024-11-22 to 2024-12-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202411220000&periodEnd=202412210000&in_Domain=10YNL----------L&out_Domain=10YNL----------L&resolution=PT15M\n",
      "Warning: attempt 2 failed for 10YBE----------2 market_prices (2024-11-22 to 2024-12-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202411220000&periodEnd=202412210000&in_Domain=10YBE----------2&out_Domain=10YBE----------2&resolution=PT15M\n",
      "Warning: attempt 3 failed for 10Y1001A1001A83F market_prices (2024-11-22 to 2024-12-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202411220000&periodEnd=202412210000&in_Domain=10Y1001A1001A83F&out_Domain=10Y1001A1001A83F&resolution=PT15M\n",
      "Warning: attempt 3 failed for 10YFR-RTE------C market_prices (2024-11-22 to 2024-12-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202411220000&periodEnd=202412210000&in_Domain=10YFR-RTE------C&out_Domain=10YFR-RTE------C&resolution=PT15M\n",
      "Warning: attempt 3 failed for 10YIT-GRTN-----B market_prices (2024-11-22 to 2024-12-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202411220000&periodEnd=202412210000&in_Domain=10YIT-GRTN-----B&out_Domain=10YIT-GRTN-----B&resolution=PT15M\n",
      "Warning: attempt 3 failed for 10YES-REE------0 market_prices (2024-11-22 to 2024-12-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202411220000&periodEnd=202412210000&in_Domain=10YES-REE------0&out_Domain=10YES-REE------0&resolution=PT15M\n",
      "Warning: attempt 3 failed for 10YPT-REN------W market_prices (2024-11-22 to 2024-12-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202411220000&periodEnd=202412210000&in_Domain=10YPT-REN------W&out_Domain=10YPT-REN------W&resolution=PT15M\n",
      "Warning: attempt 3 failed for 10YCZ-CEPS-----N market_prices (2024-11-22 to 2024-12-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202411220000&periodEnd=202412210000&in_Domain=10YCZ-CEPS-----N&out_Domain=10YCZ-CEPS-----N&resolution=PT15M\n",
      "Warning: attempt 3 failed for 10YNL----------L market_prices (2024-11-22 to 2024-12-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202411220000&periodEnd=202412210000&in_Domain=10YNL----------L&out_Domain=10YNL----------L&resolution=PT15M\n",
      "Warning: attempt 3 failed for 10YBE----------2 market_prices (2024-11-22 to 2024-12-21): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202411220000&periodEnd=202412210000&in_Domain=10YBE----------2&out_Domain=10YBE----------2&resolution=PT15M\n",
      "Warning: attempt 1 failed for 10Y1001A1001A83F market_prices (2024-12-22 to 2025-01-20): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202412220000&periodEnd=202501200000&in_Domain=10Y1001A1001A83F&out_Domain=10Y1001A1001A83F&resolution=PT15M\n",
      "Warning: attempt 1 failed for 10YFR-RTE------C market_prices (2024-12-22 to 2025-01-20): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202412220000&periodEnd=202501200000&in_Domain=10YFR-RTE------C&out_Domain=10YFR-RTE------C&resolution=PT15M\n",
      "Warning: attempt 1 failed for 10YIT-GRTN-----B market_prices (2024-12-22 to 2025-01-20): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202412220000&periodEnd=202501200000&in_Domain=10YIT-GRTN-----B&out_Domain=10YIT-GRTN-----B&resolution=PT15M\n",
      "Warning: attempt 1 failed for 10YES-REE------0 market_prices (2024-12-22 to 2025-01-20): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202412220000&periodEnd=202501200000&in_Domain=10YES-REE------0&out_Domain=10YES-REE------0&resolution=PT15M\n",
      "Warning: attempt 1 failed for 10YPT-REN------W market_prices (2024-12-22 to 2025-01-20): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202412220000&periodEnd=202501200000&in_Domain=10YPT-REN------W&out_Domain=10YPT-REN------W&resolution=PT15M\n",
      "Warning: attempt 2 failed for 10Y1001A1001A83F market_prices (2024-12-22 to 2025-01-20): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202412220000&periodEnd=202501200000&in_Domain=10Y1001A1001A83F&out_Domain=10Y1001A1001A83F&resolution=PT15M\n",
      "Warning: attempt 1 failed for 10YCZ-CEPS-----N market_prices (2024-12-22 to 2025-01-20): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202412220000&periodEnd=202501200000&in_Domain=10YCZ-CEPS-----N&out_Domain=10YCZ-CEPS-----N&resolution=PT15M\n",
      "Warning: attempt 1 failed for 10YNL----------L market_prices (2024-12-22 to 2025-01-20): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202412220000&periodEnd=202501200000&in_Domain=10YNL----------L&out_Domain=10YNL----------L&resolution=PT15M\n",
      "Warning: attempt 1 failed for 10YBE----------2 market_prices (2024-12-22 to 2025-01-20): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202412220000&periodEnd=202501200000&in_Domain=10YBE----------2&out_Domain=10YBE----------2&resolution=PT15M\n",
      "Warning: attempt 2 failed for 10YFR-RTE------C market_prices (2024-12-22 to 2025-01-20): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202412220000&periodEnd=202501200000&in_Domain=10YFR-RTE------C&out_Domain=10YFR-RTE------C&resolution=PT15M\n",
      "Warning: attempt 2 failed for 10YIT-GRTN-----B market_prices (2024-12-22 to 2025-01-20): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202412220000&periodEnd=202501200000&in_Domain=10YIT-GRTN-----B&out_Domain=10YIT-GRTN-----B&resolution=PT15M\n",
      "Warning: attempt 2 failed for 10YES-REE------0 market_prices (2024-12-22 to 2025-01-20): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202412220000&periodEnd=202501200000&in_Domain=10YES-REE------0&out_Domain=10YES-REE------0&resolution=PT15M\n",
      "Warning: attempt 2 failed for 10YPT-REN------W market_prices (2024-12-22 to 2025-01-20): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202412220000&periodEnd=202501200000&in_Domain=10YPT-REN------W&out_Domain=10YPT-REN------W&resolution=PT15M\n",
      "Warning: attempt 2 failed for 10YCZ-CEPS-----N market_prices (2024-12-22 to 2025-01-20): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202412220000&periodEnd=202501200000&in_Domain=10YCZ-CEPS-----N&out_Domain=10YCZ-CEPS-----N&resolution=PT15M\n",
      "Warning: attempt 2 failed for 10YNL----------L market_prices (2024-12-22 to 2025-01-20): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202412220000&periodEnd=202501200000&in_Domain=10YNL----------L&out_Domain=10YNL----------L&resolution=PT15M\n",
      "Warning: attempt 2 failed for 10YBE----------2 market_prices (2024-12-22 to 2025-01-20): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202412220000&periodEnd=202501200000&in_Domain=10YBE----------2&out_Domain=10YBE----------2&resolution=PT15M\n",
      "Warning: attempt 3 failed for 10Y1001A1001A83F market_prices (2024-12-22 to 2025-01-20): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202412220000&periodEnd=202501200000&in_Domain=10Y1001A1001A83F&out_Domain=10Y1001A1001A83F&resolution=PT15M\n",
      "Warning: attempt 3 failed for 10YFR-RTE------C market_prices (2024-12-22 to 2025-01-20): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202412220000&periodEnd=202501200000&in_Domain=10YFR-RTE------C&out_Domain=10YFR-RTE------C&resolution=PT15M\n",
      "Warning: attempt 3 failed for 10YIT-GRTN-----B market_prices (2024-12-22 to 2025-01-20): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202412220000&periodEnd=202501200000&in_Domain=10YIT-GRTN-----B&out_Domain=10YIT-GRTN-----B&resolution=PT15M\n",
      "Warning: attempt 3 failed for 10YES-REE------0 market_prices (2024-12-22 to 2025-01-20): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202412220000&periodEnd=202501200000&in_Domain=10YES-REE------0&out_Domain=10YES-REE------0&resolution=PT15M\n",
      "Warning: attempt 3 failed for 10YPT-REN------W market_prices (2024-12-22 to 2025-01-20): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202412220000&periodEnd=202501200000&in_Domain=10YPT-REN------W&out_Domain=10YPT-REN------W&resolution=PT15M\n",
      "Warning: attempt 3 failed for 10YCZ-CEPS-----N market_prices (2024-12-22 to 2025-01-20): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202412220000&periodEnd=202501200000&in_Domain=10YCZ-CEPS-----N&out_Domain=10YCZ-CEPS-----N&resolution=PT15M\n",
      "Warning: attempt 3 failed for 10YNL----------L market_prices (2024-12-22 to 2025-01-20): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202412220000&periodEnd=202501200000&in_Domain=10YNL----------L&out_Domain=10YNL----------L&resolution=PT15M\n",
      "Warning: attempt 3 failed for 10YBE----------2 market_prices (2024-12-22 to 2025-01-20): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202412220000&periodEnd=202501200000&in_Domain=10YBE----------2&out_Domain=10YBE----------2&resolution=PT15M\n",
      "Warning: attempt 1 failed for 10Y1001A1001A83F market_prices (2025-01-21 to 2025-02-19): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202501210000&periodEnd=202502190000&in_Domain=10Y1001A1001A83F&out_Domain=10Y1001A1001A83F&resolution=PT15M\n",
      "Warning: attempt 1 failed for 10YFR-RTE------C market_prices (2025-01-21 to 2025-02-19): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202501210000&periodEnd=202502190000&in_Domain=10YFR-RTE------C&out_Domain=10YFR-RTE------C&resolution=PT15M\n",
      "Warning: attempt 1 failed for 10YIT-GRTN-----B market_prices (2025-01-21 to 2025-02-19): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202501210000&periodEnd=202502190000&in_Domain=10YIT-GRTN-----B&out_Domain=10YIT-GRTN-----B&resolution=PT15M\n",
      "Warning: attempt 1 failed for 10YES-REE------0 market_prices (2025-01-21 to 2025-02-19): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202501210000&periodEnd=202502190000&in_Domain=10YES-REE------0&out_Domain=10YES-REE------0&resolution=PT15M\n",
      "Warning: attempt 1 failed for 10YPT-REN------W market_prices (2025-01-21 to 2025-02-19): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202501210000&periodEnd=202502190000&in_Domain=10YPT-REN------W&out_Domain=10YPT-REN------W&resolution=PT15M\n",
      "Warning: attempt 2 failed for 10Y1001A1001A83F market_prices (2025-01-21 to 2025-02-19): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202501210000&periodEnd=202502190000&in_Domain=10Y1001A1001A83F&out_Domain=10Y1001A1001A83F&resolution=PT15M\n",
      "Warning: attempt 1 failed for 10YCZ-CEPS-----N market_prices (2025-01-21 to 2025-02-19): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202501210000&periodEnd=202502190000&in_Domain=10YCZ-CEPS-----N&out_Domain=10YCZ-CEPS-----N&resolution=PT15M\n",
      "Warning: attempt 1 failed for 10YNL----------L market_prices (2025-01-21 to 2025-02-19): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202501210000&periodEnd=202502190000&in_Domain=10YNL----------L&out_Domain=10YNL----------L&resolution=PT15M\n",
      "Warning: attempt 1 failed for 10YBE----------2 market_prices (2025-01-21 to 2025-02-19): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202501210000&periodEnd=202502190000&in_Domain=10YBE----------2&out_Domain=10YBE----------2&resolution=PT15M\n",
      "Warning: attempt 2 failed for 10YFR-RTE------C market_prices (2025-01-21 to 2025-02-19): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202501210000&periodEnd=202502190000&in_Domain=10YFR-RTE------C&out_Domain=10YFR-RTE------C&resolution=PT15M\n",
      "Warning: attempt 2 failed for 10YIT-GRTN-----B market_prices (2025-01-21 to 2025-02-19): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202501210000&periodEnd=202502190000&in_Domain=10YIT-GRTN-----B&out_Domain=10YIT-GRTN-----B&resolution=PT15M\n",
      "Warning: attempt 2 failed for 10YES-REE------0 market_prices (2025-01-21 to 2025-02-19): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202501210000&periodEnd=202502190000&in_Domain=10YES-REE------0&out_Domain=10YES-REE------0&resolution=PT15M\n",
      "Warning: attempt 2 failed for 10YPT-REN------W market_prices (2025-01-21 to 2025-02-19): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202501210000&periodEnd=202502190000&in_Domain=10YPT-REN------W&out_Domain=10YPT-REN------W&resolution=PT15M\n",
      "Warning: attempt 2 failed for 10YCZ-CEPS-----N market_prices (2025-01-21 to 2025-02-19): 400 Client Error: Bad Request for url: https://web-api.tp.entsoe.eu/api?securityToken=89e296f4-6b1d-4276-a141-290239d0a1ed&documentType=A44&periodStart=202501210000&periodEnd=202502190000&in_Domain=10YCZ-CEPS-----N&out_Domain=10YCZ-CEPS-----N&resolution=PT15M\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def collect_item_country(item, country, start_date, end_date):\n",
    "    domain = COUNTRY_DOMAINS[country]['domain']\n",
    "    sess = requests.Session()\n",
    "    frames = []\n",
    "    for s, e in chunk_period(start_date, end_date, CHUNK_DAYS):\n",
    "        df_chunk = fetch_chunk(sess, item, domain, s, e)\n",
    "        if not df_chunk.empty:\n",
    "            frames.append(df_chunk)\n",
    "    if not frames: return 0\n",
    "    df_all = pd.concat(frames, ignore_index=True)\n",
    "    out_path = os.path.join(RAW_DIR, f\"{item['key']}_{country}_{start_date}_{end_date}.parquet\")\n",
    "    df_all.to_parquet(out_path, index=False, engine='pyarrow', compression=PARQUET_COMPRESSION)\n",
    "    return len(df_all)\n",
    "\n",
    "# ---------------- RUN ---------------- #\n",
    "START_DATE = date.today() - timedelta(days=365)\n",
    "END_DATE = date.today() - timedelta(days=1)\n",
    "COUNTRY_CODES = list(COUNTRY_DOMAINS.keys())\n",
    "\n",
    "tasks = [(item, country, START_DATE, END_DATE) for item in DATA_ITEMS for country in COUNTRY_CODES]\n",
    "\n",
    "MAX_WORKERS = 8\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as exe:\n",
    "    futures = {exe.submit(collect_item_country, *t): t for t in tasks}\n",
    "    for fut in as_completed(futures):\n",
    "        rows = fut.result()\n",
    "        if rows > 0:\n",
    "            print(f\"[SAVED] {rows:,} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49a866d",
   "metadata": {},
   "source": [
    "## Visualização dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ef8457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização otimizada: leitura do dataset UNIFICADO em data/raw\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "if len(DATE_RANGES) == 0:\n",
    "    raise ValueError(\"DATE_RANGES vazio. Configure períodos na célula de configuração.\")\n",
    "\n",
    "max_range = max(DATE_RANGES, key=lambda r: (r[1] - r[0]).days)\n",
    "start_date, end_date = max_range\n",
    "start_tag = start_date.strftime('%Y%m%d')\n",
    "end_tag = end_date.strftime('%Y%m%d')\n",
    "\n",
    "print(f\"Visualizando CARGA e PREÇO (de {start_date} a {end_date}, {(end_date - start_date).days} dias)\")\n",
    "\n",
    "# Leitura do arquivo unificado\n",
    "raw_dir = os.path.join('data', 'raw')\n",
    "unified_path = os.path.join(raw_dir, f\"entsoe_raw_{start_tag}_{end_tag}.parquet\")\n",
    "if not os.path.exists(unified_path):\n",
    "    raise FileNotFoundError(f\"Arquivo unificado não encontrado: {unified_path}. Execute a célula de coleta.\")\n",
    "\n",
    "unified = pandas.read_parquet(unified_path, engine='pyarrow')\n",
    "if 'datetime' in unified.columns:\n",
    "    unified['datetime'] = pandas.to_datetime(unified['datetime'], utc=True)\n",
    "for cat_col in ['country', 'currency']:\n",
    "    if cat_col in unified.columns:\n",
    "        unified[cat_col] = unified[cat_col].astype('category')\n",
    "\n",
    "# País alvo\n",
    "COUNTRY = 'AT'\n",
    "\n",
    "# Filtra país\n",
    "u_country = unified[unified['country'] == COUNTRY].copy()\n",
    "# Renderiza tabela completa ordenada por datetime (crescente)\n",
    "from IPython.display import display\n",
    "if u_country.empty:\n",
    "    print(f\"Sem dados no unificado para {COUNTRY}.\")\n",
    "else:\n",
    "    if 'datetime' in u_country.columns:\n",
    "        u_country_sorted = u_country.sort_values('datetime', ascending=True).reset_index(drop=True)\n",
    "    else:\n",
    "        u_country_sorted = u_country.copy()\n",
    "    # Ajusta opções para mostrar todas as linhas/colunas durante a exibição\n",
    "    try:\n",
    "        with pandas.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "            print(f\"\\n[AT] Tabela ordenada por datetime (crescente) — {len(u_country_sorted):,} linhas\")\n",
    "            display(u_country_sorted)\n",
    "    except Exception:\n",
    "        # Fallback simples\n",
    "        print(u_country_sorted.head(200).to_string())\n",
    "\n",
    "    # Plots abaixo (mantém comportamento anterior)\n",
    "    # Plot carga\n",
    "    if 'quantity_MW' in u_country.columns:\n",
    "        plt.figure(figsize=(14,4))\n",
    "        plt.plot(u_country['datetime'], u_country['quantity_MW'], label='Load (MW)')\n",
    "        plt.title(f\"Carga realizada – {COUNTRY}\")\n",
    "        plt.xlabel('Tempo (UTC)'); plt.ylabel('MW'); plt.tight_layout(); plt.show()\n",
    "    else:\n",
    "        print(\"Coluna quantity_MW ausente no unificado.\")\n",
    "    # Plot preço\n",
    "    if 'price_EUR_MWh' in u_country.columns:\n",
    "        plt.figure(figsize=(14,4))\n",
    "        plt.plot(u_country['datetime'], u_country['price_EUR_MWh'], color='orange', label='Price (EUR/MWh)')\n",
    "        plt.title(f\"Preços Day-Ahead – {COUNTRY}\")\n",
    "        plt.xlabel('Tempo (UTC)'); plt.ylabel('EUR/MWh'); plt.tight_layout(); plt.show()\n",
    "    else:\n",
    "        print(\"Coluna price_EUR_MWh ausente no unificado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c51d7a5",
   "metadata": {},
   "source": [
    "## Capitulo 2 — Definição dos Problemas (Carga e Mercado)\n",
    "\n",
    "Nesta etapa definimos três problemas, agora baseados em carga (load) e mercado (preço de energia). Cada problema possui variantes A/B/C para complexidade crescente.\n",
    "\n",
    "- Nível 1 — Preço univariado (A44)\n",
    "  - A: 1 país (ex.: AT), lookback curto (7 dias), horizonte 1 dia (96 passos de 15 min)\n",
    "  - B: 1 país, lookback longo (180 dias), horizonte 1 dia\n",
    "  - C: Multi-país (ex.: AT/DE/FR) com lookbacks máximo (365 dias)\n",
    "\n",
    "- Nível 2 — Preço com exógena de carga (A44 + A65)\n",
    "  - A/B/C como acima, adicionando a série de carga como exógena\n",
    "\n",
    "- Nível 3 — Multitarefa (A44 + A65)\n",
    "  - A/B/C como acima, mas prevendo simultaneamente preço e carga\n",
    "\n",
    "Abaixo, criamos construtores de datasets (builders) que leem os Parquets salvos em `data/` e montam janelas de treino com passo de 15 minutos, iniciando sempre à meia‑noite do dia. Os builders retornam tuplas (X, Y, feat_cols, target_cols, country)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8e3d5d",
   "metadata": {},
   "source": [
    "## Capitulo 3: Definição de melhores formatos para cada modelo\n",
    "\n",
    "### Formatos recomendados por modelo (15 min, início à meia-noite, H=96)\n",
    "\n",
    "Premissas gerais:\n",
    "- Cada linha dos dados brutos representa 15 minutos, começando às 00:00 do dia.\n",
    "- Horizonte de previsão padrão: H=96 passos (1 dia de 15 min).\n",
    "- Lookback L (janela de histórico) depende da variante A/B/C dos problemas, mas pode ser limitado por modelo.\n",
    "\n",
    "| Modelo | Entrada X (tensor) | Alvo Y (tensor) | Horizon (H) | Lead (gap) | Frequência | Armazenamento recomendado | Observações |\n",
    "|---|---|---|---:|---:|---|---|---|\n",
    "| Linear (Ridge/Lasso ou Regressor Linear Multi‑Saída) | X: [N, L·F] float32 (janela L de F features, achatada) — alternativa: [N, L, F] e achatar no loader | Y: [N, H] float32 (multissaída direta) | 96 | 0 | 15 min | .npz com arrays: X, Y, feat_names; opcional Parquet “windowed” | Simples e rápido. Útil usar lags (1, 96, 672) e médias móveis na engenharia de features. Para multialvo (preço+carga), Y: [N, H·T]. |\n",
    "| LSTM (seq2seq) | X_enc: [N, L, F_past] float32; X_future(opcional): [N, H, F_known] (features conhecidas no futuro, ex.: calendário); static(opc.): [N, S] | Y: [N, H, T] float32 (T=1 preço; T=2 preço+carga) | 96 | 0 | 15 min | .npz: {X_enc, X_future, Y, static, feat_lists.json}; salvar metadados (L, H, T, nomes) | Encoder‑decoder com teacher forcing opcional. Recomenda‑se normalização por série e clipping de outliers. L usual: 7–28 dias (limitar mesmo se o raw tiver 365d). |\n",
    "| Microsoft TFT (Temporal Fusion Transformer) | Formato “long” (tabela) em Parquet: colunas [id, time, target(s), known_future_*, observed_past_*, static_*]. O dataloader recorta janelas [L,H]. | Derivado do target nas janelas do dataloader (não precisa salvar Y em disco pré‑janelado) | 96 | 0 | 15 min | Parquet “long” + JSON/YAML de metadados de papéis dos atributos | Separar papéis: static (ex.: país), observed_past (lags, rolling), known_future (calendário, feriados). Definir encoder_length=L e prediction_length=H no treino. |\n",
    "| TimesFM | Contexto (histórico): [N, L, T] float32; opcional: known_future [N, H, F_known] se suportado pela lib usada; focar inicialmente em T=1 (preço) | Y: [N, H, T] para fine‑tuning; para inferência pura, só contexto | 96 | 0 | 15 min | .npz: {context, target, feat_lists.json} ou Parquet “long” minimalista (id, time, target) | TimesFM favorece entrada limpa e normalizada por série. Comece univariado (preço). Use L razoável (7–28 dias) para custo/latência; H=96. |\n",
    "\n",
    "Notas práticas:\n",
    "- Normalização: padronize por série (z‑score por país) ou robust scaling; persistir estatísticas (mean/std por país) em JSON.\n",
    "- Alinhamento temporal: garantir que as janelas [L] iniciem em timestamps válidos e que H comece imediatamente após o fim do lookback (lead=0). Se desejar “pular” um período, ajustar lead>0 e deslocar Y.\n",
    "- Engenharia de features: usar lags (1, 96, 672) e rolling (96, 672) para Linear e como observed_past para LSTM/TFT.\n",
    "- Multitarefa (preço+carga): definir T=2; para Linear, Y pode ser [N, H·2] (concatenado) ou duas cabeças separadas; para LSTM/TFT, Y: [N, H, 2].\n",
    "- Tamanhos: N é o número de janelas; L é lookback em passos de 15 min; F/F_past/F_known são contagens de features; S número de atributos estáticos. Guardar feat_lists.json descrevendo cada papel ajuda a reusar os conjuntos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7d2e84",
   "metadata": {},
   "source": [
    "### Notas de Otimização e Uso\n",
    "\n",
    "Esta versão do fluxo de coleta recebeu melhorias focadas em desempenho e reprodutibilidade e foi ajustada para:\n",
    "\n",
    "1. Sessão HTTP reutilizada com pool e retries para reduzir overhead de conexão e lidar com erros transitórios (429 / 5xx).\n",
    "2. Chunking automático de janelas longas (`CHUNK_DAYS`) evitando limites práticos da API e permitindo montar períodos maiores via concatenação.\n",
    "3. Parsing XML mais rápido via `lxml.iterparse` com fallback para `pandas.read_xml`.\n",
    "4. Dtypes otimizados (`float32`, `Int32`, `category`) diminuindo memória e tamanho de arquivos Parquet.\n",
    "5. Salvamento Parquet com compressão configurável (`PARQUET_COMPRESSION` = `zstd` por padrão). Ajuste para `snappy` se priorizar velocidade de leitura.\n",
    "6. Overwrite forçado (`ALWAYS_OVERWRITE=True`) — sempre reescreve os Parquet.\n",
    "7. Leitura \"lazy\" nas visualizações (seleção de colunas) para diminuir tempo e uso de RAM.\n",
    "8. Construção de parâmetros usando apenas valores não nulos; evita requisições inválidas.\n",
    "9. Frequência padronizada em 15 minutos, iniciando à meia‑noite do dia (96 pontos/dia) para carga e preço.\n",
    "\n",
    "Variáveis importantes\n",
    "- `MAX_WORKERS`: ajuste se receber muitos 429; valores entre 4 e 8 costumam ser seguros.\n",
    "- `CHUNK_DAYS`: use 30 para segurança. Reduza se notar timeouts; aumente apenas se a API aceitar.\n",
    "- `PARQUET_COMPRESSION`: `zstd` (boa razão de compressão) ou `snappy` (mais rápido, maior tamanho).\n",
    "- `DATE_RANGES`: fixado em 365 dias (até ontem) conforme solicitado.\n",
    "\n",
    "Reexecuções\n",
    "1. Execute as células na ordem: dependências → imports → funções utilitárias → configuração → coleta → visualização → builders (Etapa 2).\n",
    "2. Verifique se o token está definido.\n",
    "\n",
    "Token ENTSO‑E\n",
    "Crie um arquivo `.env` com:\n",
    "```\n",
    "ENTSOE_SECURITY_TOKEN=SEU_TOKEN_AQUI\n",
    "```\n",
    "\n",
    "Próximos passos sugeridos\n",
    "- Adicionar sanity checks: validar 96 pontos por dia por arquivo.\n",
    "- Acrescentar stubs de treino/avaliação para Linear/LSTM/TFT/TimesFM usando os builders L1/L2/L3.\n",
    "- Implementar cache opcional do XML bruto para debug."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5b47ef",
   "metadata": {},
   "source": [
    "# Etapa 2: Préprocessamento de dados\n",
    "\n",
    "Etapa de contrução da pipelines de pre-processamento de dados\n",
    "\n",
    "Cada modelo terá uma classe específica criada especificamente para poder préprocessar seus dados para cada um dos problemas a serem resulvidos em suas respectivas etapas\n",
    "\n",
    "As classe vai incluir métodos de:\n",
    "\n",
    "- Encodding\n",
    "- Decodding\n",
    "- Normalization\n",
    "- Denormalization\n",
    "\n",
    "Ao final do processamento de dados os arquivos será salvos na pasta:\n",
    "\n",
    "_./data/processed/{model_name}/_\n",
    "\n",
    "\n",
    "Essa etapa será dividida em 4 capitulos:\n",
    "\n",
    "1. Linear\n",
    "2. LSTM\n",
    "3. TFT\n",
    "4. Times FM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f852c4",
   "metadata": {},
   "source": [
    "## Capitulo 0: Classe geral de preprocessamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302b0d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional, List\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class Preprocessor:\n",
    "    \"\"\"Pré-processador base.\n",
    "\n",
    "    - lag/lead como inteiros são expandidos para ranges [1..N] quando apropriado.\n",
    "    - feature_cols/target_cols definem bases permitidas e servem como seleção no export.\n",
    "    - Nenhuma coluna é removida dos dados; seleção ocorre apenas na exportação.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        lag: int,\n",
    "        lead: int,\n",
    "        country_list: Optional[List[str]] = None,\n",
    "        *,\n",
    "        model_name: str = \"linear\",\n",
    "        data_dir: str = \"data/processed\",\n",
    "        feature_cols: Optional[List[str]] = None,\n",
    "        target_cols: Optional[List[str]] = None,\n",
    "    ):\n",
    "        self.lag = lag\n",
    "        self.lead = lead\n",
    "        self.country_list = country_list\n",
    "        self.model_name = model_name\n",
    "        self.data_dir = data_dir\n",
    "        self.save_dir = os.path.join(self.data_dir, self.model_name)\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "        self.feature_cols: List[str] = list(feature_cols) if feature_cols else []\n",
    "        self.target_cols: List[str] = list(target_cols) if target_cols else []\n",
    "\n",
    "        self.norm_objects = {}\n",
    "        self.encod_objects = {}\n",
    "        self.df_base = pd.DataFrame()\n",
    "\n",
    "    def _expand_steps(self, steps, default_max: Optional[int]) -> List[int]:\n",
    "        \"\"\"Normaliza passos: int→[1..N], None→[1..default_max], lista→como está.\"\"\"\n",
    "        if isinstance(steps, int):\n",
    "            return list(range(1, steps + 1)) if steps > 0 else [1]\n",
    "        if steps is None and isinstance(default_max, int) and default_max > 0:\n",
    "            return list(range(1, default_max + 1))\n",
    "        if isinstance(steps, (list, tuple)):\n",
    "            return list(steps)\n",
    "        return [1]\n",
    "\n",
    "    def load_data(self, raw_dir: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"Carrega Parquet unificado em data/raw (ou raw_dir) e atualiza self.df_base.\"\"\"\n",
    "        start_date, end_date = CONFIG_ARRAY[0][\"period\"]\n",
    "        start_tag = start_date.strftime('%Y%m%d')\n",
    "        end_tag = end_date.strftime('%Y%m%d')\n",
    "        base_raw = raw_dir or os.path.join('data', 'raw')\n",
    "        unified_path = os.path.join(base_raw, f'entsoe_raw_{start_tag}_{end_tag}.parquet')\n",
    "        if not os.path.exists(unified_path):\n",
    "            raise FileNotFoundError(f\"Arquivo unificado não encontrado: {unified_path}. Execute a coleta primeiro.\")\n",
    "        df = pd.read_parquet(unified_path, engine='pyarrow')\n",
    "        if 'datetime' in df.columns:\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "        if self.country_list and 'country' in df.columns:\n",
    "            df = df[df['country'].isin(self.country_list)].copy()\n",
    "        sort_cols = [c for c in ['country', 'datetime'] if c in df.columns]\n",
    "        if sort_cols:\n",
    "            df = df.sort_values(sort_cols).reset_index(drop=True)\n",
    "        self.df_base = df\n",
    "        return self.df_base\n",
    "\n",
    "    def encode(self, encode_cols: str = 'datetime', encode_method: str = 'label') -> pd.DataFrame:\n",
    "        \"\"\"Codifica de forma não destrutiva e atualiza self.df_base.\n",
    "\n",
    "        - label: usa LabelEncoder com suporte a NaN via placeholder interno que é revertido no decode.\n",
    "        - time_cycle: adiciona features de calendário e cíclicas sem remover datetime.\n",
    "        \"\"\"\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            print(\"df_base vazio. Chame load_data() primeiro.\")\n",
    "            return self.df_base\n",
    "        df = self.df_base.copy()\n",
    "        if encode_method == 'label':\n",
    "            le = LabelEncoder()\n",
    "            placeholder = \"__NA__\"\n",
    "            s = df[encode_cols].astype(object)\n",
    "            s_filled = s.fillna(placeholder)\n",
    "            le.fit(s_filled)\n",
    "            df[encode_cols] = le.transform(s_filled)\n",
    "            # salva metadados incluindo o code do NaN\n",
    "            na_code = int(le.transform([placeholder])[0])\n",
    "            self.encod_objects['label'] = {\n",
    "                'encode_cols': encode_cols,\n",
    "                'label_encoder': le,\n",
    "                'na_placeholder': placeholder,\n",
    "                'na_code': na_code,\n",
    "            }\n",
    "        elif encode_method == 'time_cycle':\n",
    "            if encode_cols not in df.columns:\n",
    "                print(f\"Coluna {encode_cols} não encontrada para time_cycle.\")\n",
    "                self.df_base = df\n",
    "                return df\n",
    "            dt = pd.to_datetime(df[encode_cols], utc=True)\n",
    "            # Mantém a coluna original e adiciona componentes discretos e cíclicos\n",
    "            df['year'] = dt.dt.year\n",
    "            df['month'] = dt.dt.month\n",
    "            df['day'] = dt.dt.day\n",
    "            df['hour'] = dt.dt.hour\n",
    "            df['minute'] = dt.dt.minute\n",
    "            current_year = time.localtime().tm_year\n",
    "            df['year_sin'] = np.sin(2 * np.pi * df['year'] / max(current_year, 1))\n",
    "            df['year_cos'] = np.cos(2 * np.pi * df['year'] / max(current_year, 1))\n",
    "            df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "            df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "            df['day_sin'] = np.sin(2 * np.pi * df['day'] / 31)\n",
    "            df['day_cos'] = np.cos(2 * np.pi * df['day'] / 31)\n",
    "            df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "            df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "            df['minute_sin'] = np.sin(2 * np.pi * df['minute'] / 60)\n",
    "            df['minute_cos'] = np.cos(2 * np.pi * df['minute'] / 60)\n",
    "            self.encod_objects['time_cycle'] = {'encode_cols': encode_cols}\n",
    "        else:\n",
    "            print(f\"encode_method '{encode_method}' não suportado.\")\n",
    "        self.df_base = df\n",
    "        return self.df_base\n",
    "\n",
    "    def decode(self, encode_method: str = 'label', target_col: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"Reverte codificações suportadas (label, time_cycle).\"\"\"\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            print(\"df_base vazio. Nada para decodificar.\")\n",
    "            return self.df_base\n",
    "        df = self.df_base.copy()\n",
    "        if encode_method == 'label':\n",
    "            info = self.encod_objects.get('label')\n",
    "            if not info:\n",
    "                print(\"Nenhuma informação de label encoding salva.\")\n",
    "                return self.df_base\n",
    "            col = info['encode_cols']\n",
    "            le: LabelEncoder = info['label_encoder']\n",
    "            placeholder = info.get('na_placeholder', '__NA__')\n",
    "            try:\n",
    "                inv = le.inverse_transform(df[col].astype(int))\n",
    "                # mapeia placeholder de volta para NaN\n",
    "                inv = pd.Series(inv).replace(placeholder, np.nan).values\n",
    "                df[col] = inv\n",
    "            except Exception as e:\n",
    "                print(f\"Falha ao decodificar label para coluna {col}: {e}\")\n",
    "        elif encode_method == 'time_cycle':\n",
    "            if 'year' not in df.columns:\n",
    "                print(\"Componentes de tempo ausentes para reconstrução.\")\n",
    "                return self.df_base\n",
    "            tgt = target_col or 'decoded_datetime'\n",
    "            def _recover_component(sin_col, cos_col, period, offset):\n",
    "                if sin_col not in df.columns or cos_col not in df.columns:\n",
    "                    return pd.Series([np.nan] * len(df))\n",
    "                ang = np.arctan2(df[sin_col], df[cos_col])\n",
    "                ang = (ang + 2 * np.pi) % (2 * np.pi)\n",
    "                idx = np.round((ang / (2 * np.pi)) * period).astype('Int64') % period\n",
    "                return idx + offset\n",
    "            month = _recover_component('month_sin', 'month_cos', 12, 1)\n",
    "            day = _recover_component('day_sin', 'day_cos', 31, 1)\n",
    "            hour = _recover_component('hour_sin', 'hour_cos', 24, 0)\n",
    "            minute = _recover_component('minute_sin', 'minute_cos', 60, 0)\n",
    "            year = df['year'] if 'year' in df.columns else pd.Series([np.nan] * len(df))\n",
    "            dt = pd.to_datetime({\n",
    "                'year': year.astype('Int64'),\n",
    "                'month': month.astype('Int64'),\n",
    "                'day': day.astype('Int64'),\n",
    "                'hour': hour.astype('Int64'),\n",
    "                'minute': minute.astype('Int64'),\n",
    "            }, errors='coerce', utc=True)\n",
    "            df[tgt] = dt\n",
    "        else:\n",
    "            print(f\"encode_method '{encode_method}' não suportado para decode.\")\n",
    "        self.df_base = df\n",
    "        return self.df_base\n",
    "\n",
    "    def normalize(self, value_cols: List[str], normalization_method: str = 'minmax') -> pd.DataFrame:\n",
    "        \"\"\"Normaliza colunas e atualiza self.df_base.\"\"\"\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            print(\"df_base vazio. Chame load_data() primeiro.\")\n",
    "            return self.df_base\n",
    "        df = self.df_base.copy()\n",
    "        scaler = MinMaxScaler() if normalization_method == 'minmax' else (\n",
    "            StandardScaler() if normalization_method == 'standard' else None)\n",
    "        if scaler is None:\n",
    "            raise ValueError(\"normalization_method deve ser 'minmax' ou 'standard'\")\n",
    "        df[value_cols] = scaler.fit_transform(df[value_cols])\n",
    "        self.norm_objects[normalization_method] = {'value_cols': value_cols, 'scaler': scaler}\n",
    "        self.df_base = df\n",
    "        return self.df_base\n",
    "\n",
    "    def denormalize(self, normalization_method: str = 'minmax') -> pd.DataFrame:\n",
    "        \"\"\"Reverte normalização usando metadados salvos.\"\"\"\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            print(\"df_base vazio. Nada para denormalizar.\")\n",
    "            return self.df_base\n",
    "        info = self.norm_objects.get(normalization_method)\n",
    "        if not info:\n",
    "            print(f\"Nenhum scaler salvo para o método '{normalization_method}'.\")\n",
    "            return self.df_base\n",
    "        cols: List[str] = info['value_cols']\n",
    "        scaler = info['scaler']\n",
    "        df = self.df_base.copy()\n",
    "        try:\n",
    "            df[cols] = scaler.inverse_transform(df[cols])\n",
    "        except Exception as e:\n",
    "            print(f\"Falha ao denormalizar colunas {cols}: {e}\")\n",
    "            return self.df_base\n",
    "        self.df_base = df\n",
    "        return self.df_base\n",
    "\n",
    "    def save_df_base(self, filename: Optional[str] = None, compression: Optional[str] = None, partition_by: Optional[List[str]] = None) -> Optional[str]:\n",
    "        \"\"\"Salva self.df_base em Parquet dentro de data_dir/{model_name}.\"\"\"\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            print(\"df_base vazio. Nada para salvar.\")\n",
    "            return None\n",
    "        comp = compression\n",
    "        if comp is None:\n",
    "            try:\n",
    "                comp = PARQUET_COMPRESSION\n",
    "            except NameError:\n",
    "                comp = 'zstd'\n",
    "        if filename is None:\n",
    "            try:\n",
    "                start_date, end_date = CONFIG_ARRAY[0][\"period\"]\n",
    "                filename = f\"df_base_{start_date:%Y%m%d}_{end_date:%Y%m%d}.parquet\"\n",
    "            except Exception:\n",
    "                filename = \"df_base.parquet\"\n",
    "        out_path = os.path.join(self.save_dir, filename)\n",
    "        df = self.df_base.copy()\n",
    "        if 'datetime' in df.columns:\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "        try:\n",
    "            if partition_by:\n",
    "                df.to_parquet(out_path, engine='pyarrow', compression=comp, index=False, partition_cols=partition_by)\n",
    "            else:\n",
    "                df.to_parquet(out_path, engine='pyarrow', compression=comp, index=False)\n",
    "            print(f\"[SALVO] df_base: {len(df):,} linhas → {out_path}\")\n",
    "            return out_path\n",
    "        except Exception as e:\n",
    "            print(f\"Falha ao salvar df_base em {out_path}: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa7c121",
   "metadata": {},
   "source": [
    "## Capítulo 1: Preprocessamento do Modelo Linear\n",
    "\n",
    "Esse modelo deve será contruido a partir de lags e leads passados como parâmetros na função, resultando na contrução de novas colunas lead lag, assim gerando uma flat matrix 2D que será usada no modelo linear\n",
    "\n",
    "Observação importante: lag e lead são inteiros e representam o máximo de passos; o pipeline expande para intervalos 1..N automaticamente. Por exemplo, lag=96 gera features com defasagens de 1 a 96; lead=96 gera alvos de 1 a 96.\n",
    "\n",
    "Os arquivos do modelo serão salvos em TFrecords já que o modelo linear será contruído usando tensor flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dd3ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearPreprocessor(Preprocessor):\n",
    "    \"\"\"Pré-processador linear: gera matriz flat (lags/leads) e exporta TFRecords.\n",
    "\n",
    "    Princípios:\n",
    "    - Não remove colunas do df_base, apenas adiciona as derivadas.\n",
    "    - Usa feature_cols/target_cols definidos no construtor do Preprocessor.\n",
    "    - Seleção é estrita apenas na exportação (TFRecords).\n",
    "    \"\"\"\n",
    "\n",
    "    def build_flat_matrix(\n",
    "        self,\n",
    "        value_cols: Optional[List[str]] = None,\n",
    "        target: Optional[str] = None,\n",
    "        lags: Optional[int] = None,\n",
    "        leads: Optional[int] = None,\n",
    "        dropna: bool = True,\n",
    "        group_cols: Optional[List[str]] = None,\n",
    "        time_col: str = 'datetime',\n",
    "    ) -> pd.DataFrame:\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            print(\"df_base vazio. Chame load_data() primeiro.\")\n",
    "            return self.df_base\n",
    "\n",
    "        df = self.df_base.copy()\n",
    "        feats = value_cols or self.feature_cols\n",
    "        tgt = target or (self.target_cols[0] if self.target_cols else None)\n",
    "        if not feats:\n",
    "            print(\"Nenhuma coluna de feature informada.\")\n",
    "            return self.df_base\n",
    "        if tgt is None:\n",
    "            print(\"Nenhum target informado.\")\n",
    "            return self.df_base\n",
    "\n",
    "        group_cols = group_cols or [c for c in ['country'] if c in df.columns]\n",
    "        lag_steps = self._expand_steps(lags if lags is not None else self.lag, None)\n",
    "        lead_steps = self._expand_steps(leads if leads is not None else self.lead, None)\n",
    "\n",
    "        if group_cols:\n",
    "            sort_by = group_cols + ([time_col] if time_col in df.columns else [])\n",
    "            if sort_by:\n",
    "                df = df.sort_values(sort_by).reset_index(drop=True)\n",
    "\n",
    "        new_cols = []\n",
    "        # Lags para features\n",
    "        for col in feats:\n",
    "            if col not in df.columns:\n",
    "                continue\n",
    "            for k in lag_steps:\n",
    "                cname = f\"{col}_lag{k}\"\n",
    "                if group_cols:\n",
    "                    df[cname] = df.groupby(group_cols, sort=False)[col].shift(k)\n",
    "                else:\n",
    "                    df[cname] = df[col].shift(k)\n",
    "                new_cols.append(cname)\n",
    "        # Leads para target\n",
    "        if tgt in df.columns:\n",
    "            for k in lead_steps:\n",
    "                cname = f\"{tgt}_lead{k}\"\n",
    "                if group_cols:\n",
    "                    df[cname] = df.groupby(group_cols, sort=False)[tgt].shift(-k)\n",
    "                else:\n",
    "                    df[cname] = df[tgt].shift(-k)\n",
    "                new_cols.append(cname)\n",
    "        else:\n",
    "            print(f\"Target '{tgt}' não encontrado. Ignorando leads.\")\n",
    "\n",
    "        if dropna and new_cols:\n",
    "            df = df.dropna(subset=new_cols).reset_index(drop=True)\n",
    "\n",
    "        self.df_base = df\n",
    "        return self.df_base\n",
    "\n",
    "    def save_tfrecords(\n",
    "        self,\n",
    "        output_basename: str = 'dataset',\n",
    "        shard_size: int = 100_000,\n",
    "        compression: Optional[str] = None,\n",
    "    ) -> Optional[List[str]]:\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            print(\"df_base vazio. Nada para salvar em TFRecords.\")\n",
    "            return None\n",
    "        try:\n",
    "            import tensorflow as tf\n",
    "        except Exception as e:\n",
    "            print(f\"TensorFlow não disponível: {e}\")\n",
    "            return None\n",
    "\n",
    "        # Seleção estrita: apenas colunas configuradas e existentes\n",
    "        present_feats = [c for c in self.feature_cols if c in self.df_base.columns]\n",
    "        present_tgts = [c for c in self.target_cols if c in self.df_base.columns]\n",
    "        if not present_feats:\n",
    "            print(\"Nenhuma feature presente no df_base para export.\")\n",
    "            return None\n",
    "        if not present_tgts:\n",
    "            print(\"Nenhum target presente no df_base para export.\")\n",
    "            return None\n",
    "\n",
    "        missing_feats = [c for c in self.feature_cols if c not in present_feats]\n",
    "        missing_tgts = [c for c in self.target_cols if c not in present_tgts]\n",
    "        if missing_feats:\n",
    "            print(f\"[WARN] Features ausentes: {missing_feats}\")\n",
    "        if missing_tgts:\n",
    "            print(f\"[WARN] Targets ausentes: {missing_tgts}\")\n",
    "\n",
    "        df = self.df_base.reset_index(drop=True)\n",
    "        X = df[present_feats].astype('float32').to_numpy(copy=False)\n",
    "        y = df[present_tgts].astype('float32').to_numpy(copy=False)\n",
    "\n",
    "        x_dim = X.shape[1]\n",
    "        y_dim = y.shape[1]\n",
    "        n = len(df)\n",
    "\n",
    "        def _float_feature(v):\n",
    "            return tf.train.Feature(float_list=tf.train.FloatList(value=v))\n",
    "\n",
    "        def _serialize_row(i):\n",
    "            example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                'x': _float_feature(X[i]),\n",
    "                'y': _float_feature(y[i]),\n",
    "            }))\n",
    "            return example.SerializeToString()\n",
    "\n",
    "        comp = compression or 'GZIP'\n",
    "        options = tf.io.TFRecordOptions(compression_type=comp) if comp else None\n",
    "\n",
    "        paths: List[str] = []\n",
    "        for shard_idx, start in enumerate(range(0, n, shard_size)):\n",
    "            end = min(start + shard_size, n)\n",
    "            shard_path = os.path.join(self.save_dir, f\"{output_basename}_{shard_idx:05d}.tfrecord\")\n",
    "            with tf.io.TFRecordWriter(shard_path, options=options) as w:\n",
    "                for i in range(start, end):\n",
    "                    w.write(_serialize_row(i))\n",
    "            paths.append(shard_path)\n",
    "\n",
    "        # Metadados\n",
    "        meta = {\n",
    "            'x_dim': int(x_dim),\n",
    "            'y_dim': int(y_dim),\n",
    "            'feature_cols': present_feats,\n",
    "            'target_cols': present_tgts,\n",
    "            'count': int(n),\n",
    "            'compression': comp or 'NONE',\n",
    "            'basename': output_basename,\n",
    "        }\n",
    "        try:\n",
    "            import json\n",
    "            with open(os.path.join(self.save_dir, f\"{output_basename}.meta.json\"), 'w', encoding='utf-8') as f:\n",
    "                json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Falha ao salvar metadados: {e}\")\n",
    "\n",
    "        print(f\"[SALVO] TFRecords: {len(paths)} shard(s) em {self.save_dir}\")\n",
    "        return paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b561da11",
   "metadata": {},
   "source": [
    "## Capítulo 2 — LSTM: Pré-processamento\n",
    "\n",
    "Este capítulo prepara janelas sequenciais para modelos LSTM a partir do `df_base` do Preprocessor.\n",
    "\n",
    "- Entrada: janelas com `lookback` passos de `feature_cols`\n",
    "- Saída: janelas com `horizon` passos de `target_cols`\n",
    "- Agrupamento por país (ou outra chave) garante que sequências não cruzem fronteiras de séries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07624afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "class LSTMPreprocessor(Preprocessor):\n",
    "    \"\"\"Gera janelas sequenciais para LSTM a partir de df_base.\n",
    "\n",
    "    - X: [num_windows, lookback, n_features]\n",
    "    - y: [num_windows, horizon, n_targets]\n",
    "    - Respeita separação por grupos (p.ex., país) e ordena por datetime quando disponível.\n",
    "    \"\"\"\n",
    "\n",
    "    def build_sequences(\n",
    "        self,\n",
    "        lookback: int,\n",
    "        horizon: int,\n",
    "        group_cols: Optional[List[str]] = None,\n",
    "        time_col: str = 'datetime',\n",
    "        feature_cols: Optional[List[str]] = None,\n",
    "        target_cols: Optional[List[str]] = None,\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            raise ValueError(\"df_base vazio. Carregue dados com load_data().\")\n",
    "        feats = feature_cols or self.feature_cols\n",
    "        tgts = target_cols or self.target_cols\n",
    "        if not feats or not tgts:\n",
    "            raise ValueError(\"feature_cols e target_cols devem estar definidos.\")\n",
    "        df = self.df_base.copy()\n",
    "        # Ordenação por grupo e tempo\n",
    "        group_cols = group_cols or [c for c in ['country'] if c in df.columns]\n",
    "        if group_cols:\n",
    "            sort_by = group_cols + ([time_col] if time_col in df.columns else [])\n",
    "            if sort_by:\n",
    "                df = df.sort_values(sort_by).reset_index(drop=True)\n",
    "        else:\n",
    "            if time_col in df.columns:\n",
    "                df = df.sort_values(time_col).reset_index(drop=True)\n",
    "\n",
    "        X_list, y_list = [], []\n",
    "        if group_cols:\n",
    "            for _, g in df.groupby(group_cols, sort=False):\n",
    "                Xg, yg = self._windows_from_frame(g, feats, tgts, lookback, horizon)\n",
    "                if Xg.size and yg.size:\n",
    "                    X_list.append(Xg)\n",
    "                    y_list.append(yg)\n",
    "        else:\n",
    "            Xg, yg = self._windows_from_frame(df, feats, tgts, lookback, horizon)\n",
    "            if Xg.size and yg.size:\n",
    "                X_list.append(Xg)\n",
    "                y_list.append(yg)\n",
    "        if not X_list:\n",
    "            return np.empty((0, lookback, len(feats)), dtype=np.float32), np.empty((0, horizon, len(tgts)), dtype=np.float32)\n",
    "        X = np.concatenate(X_list, axis=0)\n",
    "        y = np.concatenate(y_list, axis=0)\n",
    "        return X.astype(np.float32, copy=False), y.astype(np.float32, copy=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def _windows_from_frame(g: pd.DataFrame, feats: List[str], tgts: List[str], lookback: int, horizon: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        f = g[feats].to_numpy(dtype=np.float32, copy=False)\n",
    "        t = g[tgts].to_numpy(dtype=np.float32, copy=False)\n",
    "        n = len(g)\n",
    "        win = lookback + horizon\n",
    "        if n < win:\n",
    "            return np.empty((0, lookback, len(feats)), dtype=np.float32), np.empty((0, horizon, len(tgts)), dtype=np.float32)\n",
    "        X, Y = [], []\n",
    "        for i in range(n - win + 1):\n",
    "            X.append(f[i:i+lookback])\n",
    "            Y.append(t[i+lookback:i+win])\n",
    "        return np.array(X, dtype=np.float32), np.array(Y, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b69a226",
   "metadata": {},
   "source": [
    "## Capítulo 3 — TFT: Pré-processamento\n",
    "\n",
    "Este capítulo prepara tensores para o Temporal Fusion Transformer (TFT).\n",
    "\n",
    "- Suporta variáveis estáticas por série (ex.: país) e variáveis conhecidas do futuro (ex.: calendário)\n",
    "- Saídas esperadas: triplo (encoder, decoder, targets) com shapes padronizados\n",
    "- Mantém alocação eficiente e seleção baseada em feature_cols/target_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00410d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "class TFTPreprocessor(Preprocessor):\n",
    "    \"\"\"Prepara tensores para o Temporal Fusion Transformer (TFT).\n",
    "\n",
    "    Produz um dicionário com:\n",
    "    - encoder: [N, enc_len, n_feat]\n",
    "    - decoder: [N, dec_len, n_feat_known_future]\n",
    "    - target:  [N, dec_len, n_tgt]\n",
    "    \"\"\"\n",
    "\n",
    "    def build_tensors(\n",
    "        self,\n",
    "        enc_len: int,\n",
    "        dec_len: int,\n",
    "        group_cols: Optional[List[str]] = None,\n",
    "        time_col: str = 'datetime',\n",
    "        feature_cols: Optional[List[str]] = None,\n",
    "        known_future_cols: Optional[List[str]] = None,\n",
    "        target_cols: Optional[List[str]] = None,\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            raise ValueError(\"df_base vazio. Carregue dados com load_data().\")\n",
    "        feats = feature_cols or self.feature_cols\n",
    "        tgts = target_cols or self.target_cols\n",
    "        known = known_future_cols or [c for c in feats if c.endswith(('_sin', '_cos')) or c in ('month', 'day', 'hour', 'minute')]\n",
    "        if not feats or not tgts:\n",
    "            raise ValueError(\"feature_cols e target_cols devem estar definidos.\")\n",
    "        df = self.df_base.copy()\n",
    "        # Ordenação por grupo e tempo\n",
    "        group_cols = group_cols or [c for c in ['country'] if c in df.columns]\n",
    "        if group_cols:\n",
    "            sort_by = group_cols + ([time_col] if time_col in df.columns else [])\n",
    "            if sort_by:\n",
    "                df = df.sort_values(sort_by).reset_index(drop=True)\n",
    "        else:\n",
    "            if time_col in df.columns:\n",
    "                df = df.sort_values(time_col).reset_index(drop=True)\n",
    "\n",
    "        def _split_windows(g: pd.DataFrame):\n",
    "            F = g[feats].to_numpy(dtype=np.float32, copy=False)\n",
    "            K = g[known].to_numpy(dtype=np.float32, copy=False) if known else np.empty((len(g), 0), dtype=np.float32)\n",
    "            T = g[tgts].to_numpy(dtype=np.float32, copy=False)\n",
    "            n = len(g)\n",
    "            win = enc_len + dec_len\n",
    "            enc_list, dec_list, tgt_list = [], [], []\n",
    "            for i in range(n - win + 1):\n",
    "                enc_list.append(F[i:i+enc_len])\n",
    "                # no decoder, geralmente usamos apenas variáveis conhecidas do futuro\n",
    "                dec_list.append(K[i+enc_len:i+win] if K.shape[1] > 0 else np.zeros((dec_len, 0), dtype=np.float32))\n",
    "                tgt_list.append(T[i+enc_len:i+win])\n",
    "            if not enc_list:\n",
    "                return np.empty((0, enc_len, len(feats)), dtype=np.float32), \\\n",
    "                       np.empty((0, dec_len, K.shape[1] if K.size else 0), dtype=np.float32), \\\n",
    "                       np.empty((0, dec_len, len(tgts)), dtype=np.float32)\n",
    "            return (\n",
    "                np.array(enc_list, dtype=np.float32),\n",
    "                np.array(dec_list, dtype=np.float32),\n",
    "                np.array(tgt_list, dtype=np.float32),\n",
    "            )\n",
    "\n",
    "        enc_all, dec_all, tgt_all = [], [], []\n",
    "        if group_cols:\n",
    "            for _, g in df.groupby(group_cols, sort=False):\n",
    "                e, d, t = _split_windows(g)\n",
    "                if e.size:\n",
    "                    enc_all.append(e); dec_all.append(d); tgt_all.append(t)\n",
    "        else:\n",
    "            e, d, t = _split_windows(df)\n",
    "            if e.size:\n",
    "                enc_all.append(e); dec_all.append(d); tgt_all.append(t)\n",
    "\n",
    "        if not enc_all:\n",
    "            return {'encoder': np.empty((0, enc_len, len(feats)), dtype=np.float32),\n",
    "                    'decoder': np.empty((0, dec_len, len(known)), dtype=np.float32),\n",
    "                    'target':  np.empty((0, dec_len, len(tgts)), dtype=np.float32)}\n",
    "\n",
    "        encoder = np.concatenate(enc_all, axis=0)\n",
    "        decoder = np.concatenate(dec_all, axis=0)\n",
    "        target  = np.concatenate(tgt_all, axis=0)\n",
    "        return {'encoder': encoder, 'decoder': decoder, 'target': target}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5749a96",
   "metadata": {},
   "source": [
    "## Capítulo 4 — TimesFM: Pré-processamento\n",
    "\n",
    "Este capítulo prepara a série e metadados no formato típico de modelos foundation de séries temporais (ex.: TimesFM).\n",
    "\n",
    "- Saída: listas/dicionários prontos para consumo por APIs de inferência\n",
    "- Seleciona colunas alvo e contextuais e organiza por série (país) mantendo a cadência de 15 minutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fb7f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimesFMPreprocessor(Preprocessor):\n",
    "    \"\"\"Prepara dados no formato simplificado para TimesFM-like modelos foundation.\n",
    "\n",
    "    Produz um dicionário:\n",
    "    - series: lista de arrays 1D (targets) por série (ex.: país)\n",
    "    - context: lista de dataframes/arrays com features contextuais alinhadas\n",
    "    - index: lista de índices de tempo para referência externa\n",
    "    \"\"\"\n",
    "\n",
    "    def build_payload(\n",
    "        self,\n",
    "        group_cols: Optional[List[str]] = None,\n",
    "        time_col: str = 'datetime',\n",
    "        feature_cols: Optional[List[str]] = None,\n",
    "        target_cols: Optional[List[str]] = None,\n",
    "    ) -> Dict[str, List]:\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            raise ValueError(\"df_base vazio. Carregue dados com load_data().\")\n",
    "        feats = feature_cols or self.feature_cols\n",
    "        tgts = target_cols or self.target_cols\n",
    "        if not tgts:\n",
    "            raise ValueError(\"target_cols deve estar definido para gerar a série principal.\")\n",
    "        df = self.df_base.copy()\n",
    "        group_cols = group_cols or [c for c in ['country'] if c in df.columns]\n",
    "        # Ordenação\n",
    "        sort_by = (group_cols or []) + ([time_col] if time_col in df.columns else [])\n",
    "        if sort_by:\n",
    "            df = df.sort_values(sort_by).reset_index(drop=True)\n",
    "        # Preparação por série\n",
    "        series_list, context_list, index_list = [], [], []\n",
    "        if group_cols:\n",
    "            for _, g in df.groupby(group_cols, sort=False):\n",
    "                idx = g[time_col].tolist() if time_col in g.columns else list(range(len(g)))\n",
    "                y = g[tgts].to_numpy(dtype=np.float32, copy=False)\n",
    "                # Se múltiplos targets, concatena/seleciona o primeiro como série principal\n",
    "                y1d = y[:, 0] if y.ndim == 2 and y.shape[1] >= 1 else y.ravel()\n",
    "                series_list.append(y1d)\n",
    "                ctx = g[feats].to_numpy(dtype=np.float32, copy=False) if feats else np.empty((len(g), 0), dtype=np.float32)\n",
    "                context_list.append(ctx)\n",
    "                index_list.append(idx)\n",
    "        else:\n",
    "            g = df\n",
    "            idx = g[time_col].tolist() if time_col in g.columns else list(range(len(g)))\n",
    "            y = g[tgts].to_numpy(dtype=np.float32, copy=False)\n",
    "            y1d = y[:, 0] if y.ndim == 2 and y.shape[1] >= 1 else y.ravel()\n",
    "            series_list.append(y1d)\n",
    "            ctx = g[feats].to_numpy(dtype=np.float32, copy=False) if feats else np.empty((len(g), 0), dtype=np.float32)\n",
    "            context_list.append(ctx)\n",
    "            index_list.append(idx)\n",
    "        return {'series': series_list, 'context': context_list, 'index': index_list}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8ece38",
   "metadata": {},
   "source": [
    "# Etapa 3 — Construção dos Modelos\n",
    "\n",
    "A seguir, definimos construtores simples e eficientes para cada modelo (Linear, LSTM, TFT e TimesFM),\n",
    "prontos para uso em rotinas de otimização de hiperparâmetros (por exemplo, Optuna). Cada construtor\n",
    "recebe um dicionário de parâmetros (`params`) e retorna um modelo compilado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556d5525",
   "metadata": {},
   "source": [
    "## Capítulo 1 — Linear: Construção do Modelo\n",
    "\n",
    "Objetivo: um regressor denso simples (MLP) para prever `target_cols` a partir de `feature_cols`.\n",
    "\n",
    "Contrato rápido:\n",
    "- Entrada: vetor de tamanho `x_dim` (número de features)\n",
    "- Saída: vetor de tamanho `y_dim` (número de targets)\n",
    "- Parâmetros (exemplos): hidden_units, activation, dropout, lr, l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727619dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "def build_linear_model(x_dim: int, y_dim: int, params: Dict[str, Any]) -> keras.Model:\n",
    "    \"\"\"Constrói um MLP simples para regressão multivariada.\n",
    "\n",
    "    params:\n",
    "      - hidden_units: List[int]\n",
    "      - activation: str (ex.: 'relu')\n",
    "      - dropout: float (0..1)\n",
    "      - l2: float (regularização L2)\n",
    "      - lr: float (learning rate)\n",
    "    \"\"\"\n",
    "    hidden_units = params.get('hidden_units', [128, 64])\n",
    "    activation = params.get('activation', 'relu')\n",
    "    dropout = float(params.get('dropout', 0.0))\n",
    "    l2 = float(params.get('l2', 0.0))\n",
    "    lr = float(params.get('lr', 1e-3))\n",
    "\n",
    "    inputs = keras.Input(shape=(x_dim,), name='features')\n",
    "    x = inputs\n",
    "    for i, units in enumerate(hidden_units):\n",
    "        x = layers.Dense(units, activation=activation,\n",
    "                         kernel_regularizer=keras.regularizers.l2(l2),\n",
    "                         name=f'dense_{i}')(x)\n",
    "        if dropout > 0:\n",
    "            x = layers.Dropout(dropout, name=f'dropout_{i}')(x)\n",
    "    outputs = layers.Dense(y_dim, name='targets')(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs, name='linear_mlp')\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "                  loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Uso (exemplo)\n",
    "# model = build_linear_model(x_dim=len(feature_cols), y_dim=len(target_cols),\n",
    "#                            params={'hidden_units':[128,64], 'activation':'relu', 'dropout':0.1, 'l2':1e-6, 'lr':1e-3})\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f0eb15",
   "metadata": {},
   "source": [
    "## Capítulo 2 — LSTM: Construção do Modelo\n",
    "\n",
    "Objetivo: um seq2seq LSTM para prever `horizon` passos à frente (multi-step) usando `lookback` passos de entrada.\n",
    "\n",
    "Contrato rápido:\n",
    "- Entrada: [lookback, x_dim]\n",
    "- Saída: [horizon, y_dim]\n",
    "- Parâmetros: units, dropout, lr, l2, bidirectional (bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53dede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def build_lstm_model(lookback: int, x_dim: int, horizon: int, y_dim: int, params: Dict[str, Any]) -> keras.Model:\n",
    "    \"\"\"Seq2seq LSTM simples com RepeatVector + TimeDistributed.\n",
    "\n",
    "    params:\n",
    "      - units: int (tamanho das camadas LSTM)\n",
    "      - layers: int (nº de camadas LSTM no encoder/decoder)\n",
    "      - bidirectional: bool\n",
    "      - dropout: float (0..1)\n",
    "      - lr: float\n",
    "      - l2: float\n",
    "    \"\"\"\n",
    "    units = int(params.get('units', 128))\n",
    "    layers_n = int(params.get('layers', 1))\n",
    "    bidir = bool(params.get('bidirectional', False))\n",
    "    dropout = float(params.get('dropout', 0.0))\n",
    "    lr = float(params.get('lr', 1e-3))\n",
    "    l2 = float(params.get('l2', 0.0))\n",
    "\n",
    "    inputs = keras.Input(shape=(lookback, x_dim), name='seq_in')\n",
    "    x = inputs\n",
    "    for i in range(layers_n):\n",
    "        lstm = layers.LSTM(units, return_sequences=(i < layers_n - 1),\n",
    "                           kernel_regularizer=keras.regularizers.l2(l2), name=f'enc_lstm_{i}')\n",
    "        x = (layers.Bidirectional(lstm, name=f'enc_bi_{i}')(x) if bidir else lstm(x))\n",
    "        if dropout > 0:\n",
    "            x = layers.Dropout(dropout, name=f'enc_drop_{i}')(x)\n",
    "\n",
    "    x = layers.RepeatVector(horizon, name='repeat')(x)\n",
    "\n",
    "    for i in range(layers_n):\n",
    "        x = layers.LSTM(units, return_sequences=True,\n",
    "                        kernel_regularizer=keras.regularizers.l2(l2), name=f'dec_lstm_{i}')(x)\n",
    "        if dropout > 0:\n",
    "            x = layers.Dropout(dropout, name=f'dec_drop_{i}')(x)\n",
    "\n",
    "    outputs = layers.TimeDistributed(layers.Dense(y_dim), name='td_out')(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs, name='lstm_seq2seq')\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "                  loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Uso (exemplo)\n",
    "# model = build_lstm_model(lookback=96, x_dim=len(feature_cols), horizon=4, y_dim=len(target_cols),\n",
    "#                          params={'units':128, 'layers':1, 'bidirectional':False, 'dropout':0.1, 'lr':1e-3, 'l2':1e-6})\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8e7a3d",
   "metadata": {},
   "source": [
    "## Capítulo 3 — TFT: Construção do Modelo\n",
    "\n",
    "Objetivo: um modelo inspirado no Temporal Fusion Transformer (versão simplificada) com encoder/decoder e atenção multihead.\n",
    "\n",
    "Contrato rápido:\n",
    "- Entradas: encoder [enc_len, x_dim], decoder_known [dec_len, k_dim]\n",
    "- Saída: [dec_len, y_dim]\n",
    "- Parâmetros: num_heads, key_dim, ff_dim, layers, dropout, lr, l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe36577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementação correta (oficial) do TFT via PyTorch Forecasting\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def build_tft_model(\n",
    "    df_long: pd.DataFrame,\n",
    "    *,\n",
    "    time_idx: str,\n",
    "    group_ids: List[str],\n",
    "    target: str,\n",
    "    max_encoder_length: int,\n",
    "    max_prediction_length: int,\n",
    "    time_varying_known_reals: Optional[List[str]] = None,\n",
    "    time_varying_unknown_reals: Optional[List[str]] = None,\n",
    "    static_categoricals: Optional[List[str]] = None,\n",
    "    static_reals: Optional[List[str]] = None,\n",
    "    params: Optional[Dict[str, Any]] = None,\n",
    ") -> Tuple[\"TemporalFusionTransformer\", \"TimeSeriesDataSet\"]:\n",
    "    \"\"\"Constroi um TemporalFusionTransformer oficial com PyTorch Forecasting.\n",
    "\n",
    "    Requer dataframe no formato \"long\" com colunas de papel bem definidas.\n",
    "    Retorna (model, training_dataset) pronto para treinamento e HPO.\n",
    "\n",
    "    Parâmetros esperados em params:\n",
    "      - hidden_size: int (tamanho embeddings/hidden)\n",
    "      - attention_head_size: int\n",
    "      - dropout: float\n",
    "      - learning_rate: float\n",
    "      - lstm_layers: int\n",
    "      - output_quantiles: List[float] (padrão [0.1,0.2,...,0.9])\n",
    "      - hidden_continuous_size: int\n",
    "      - embedding_sizes: Optional[dict] (normalmente inferido do dataset)\n",
    "      - batch_size: int (para dataloader externo)\n",
    "    \"\"\"\n",
    "    params = params or {}\n",
    "    try:\n",
    "        import pytorch_forecasting as ptf\n",
    "        from pytorch_forecasting import TimeSeriesDataSet\n",
    "        from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "        from pytorch_forecasting.metrics import QuantileLoss\n",
    "    except Exception as e:\n",
    "        raise ImportError(\n",
    "            \"PyTorch Forecasting não está instalado. Instale com: pip install pytorch-forecasting pytorch-lightning torch\"\n",
    "        ) from e\n",
    "\n",
    "    # Defaults\n",
    "    hidden_size = int(params.get(\"hidden_size\", 64))\n",
    "    attention_head_size = int(params.get(\"attention_head_size\", 4))\n",
    "    dropout = float(params.get(\"dropout\", 0.1))\n",
    "    learning_rate = float(params.get(\"learning_rate\", params.get(\"lr\", 1e-3)))\n",
    "    lstm_layers = int(params.get(\"lstm_layers\", 1))\n",
    "    hidden_continuous_size = int(params.get(\"hidden_continuous_size\", 16))\n",
    "    quantiles = params.get(\"output_quantiles\", [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "\n",
    "    # Campos de papel\n",
    "    time_varying_known_reals = list(time_varying_known_reals or [])\n",
    "    time_varying_unknown_reals = list(time_varying_unknown_reals or [target])\n",
    "    static_categoricals = list(static_categoricals or [])\n",
    "    static_reals = list(static_reals or [])\n",
    "\n",
    "    # Cria dataset com features auxiliares\n",
    "    training = TimeSeriesDataSet(\n",
    "        df_long,\n",
    "        time_idx=time_idx,\n",
    "        target=target,\n",
    "        group_ids=group_ids,\n",
    "        max_encoder_length=max_encoder_length,\n",
    "        max_prediction_length=max_prediction_length,\n",
    "        time_varying_known_reals=time_varying_known_reals,\n",
    "        time_varying_unknown_reals=time_varying_unknown_reals,\n",
    "        static_categoricals=static_categoricals,\n",
    "        static_reals=static_reals,\n",
    "        add_relative_time_idx=True,\n",
    "        add_target_scales=True,\n",
    "        add_encoder_length=True,\n",
    "    )\n",
    "\n",
    "    # Constrói modelo a partir do dataset\n",
    "    loss = QuantileLoss(quantiles=quantiles)\n",
    "    model = TemporalFusionTransformer.from_dataset(\n",
    "        training,\n",
    "        hidden_size=hidden_size,\n",
    "        attention_head_size=attention_head_size,\n",
    "        dropout=dropout,\n",
    "        learning_rate=learning_rate,\n",
    "        loss=loss,\n",
    "        lstm_layers=lstm_layers,\n",
    "        hidden_continuous_size=hidden_continuous_size,\n",
    "        output_size=len(quantiles),  # corresponde aos quantis\n",
    "    )\n",
    "\n",
    "    return model, training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f545eb6",
   "metadata": {},
   "source": [
    "## Capítulo 4 — TimesFM: Construção do Modelo\n",
    "\n",
    "Objetivo: um transformer de previsão simplificado (TimesFM-like) para experimentar com janela de contexto e horizonte.\n",
    "\n",
    "Contrato rápido:\n",
    "- Entradas: contexto [ctx_len, x_dim], futuro_conhecido [horizon, k_dim]\n",
    "- Saída: [horizon, y_dim]\n",
    "- Parâmetros: num_heads, key_dim, ff_dim, layers, dropout, lr, l2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afbf736",
   "metadata": {},
   "source": [
    "# TESTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e97fb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_linear_pipeline():\n",
    "    # Criando Pipeline para modelo linear\n",
    "    preproc = LinearPreprocessor(\n",
    "        data_dir='data',\n",
    "        model_name='linear_model',\n",
    "        feature_cols=[\"country\",\"datetime\",\"quantity_MW\",\"price_EUR_MWh\",\"currency\"],\n",
    "        target_cols=[\"price_EUR_MWh\"],\n",
    "        lag=3,\n",
    "        lead=1,\n",
    "        country_list=['US', 'BR']\n",
    "    )\n",
    "    preproc.load_data()\n",
    "    preproc.encode(encode_cols='datetime', encode_method='time_cycle')\n",
    "    preproc.encode(encode_cols='currency', encode_method='label')\n",
    "    preproc.normalize(value_cols=['quantity_MW','price_EUR_MWh'], normalization_method='minmax')\n",
    "    preproc.build_flat_matrix(value_cols=[\"quantity_MW\",\"price_EUR_MWh\"], target=\"price_EUR_MWh\", lags=7*96, leads=96, dropna=True, group_cols=['country'], time_col='datetime')\n",
    "    preproc.save_tfrecords(output_basename='linear_dataset', shard_size=50000, compression='GZIP')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d8e826",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_lin = test_linear_pipeline()\n",
    "# r_lstm = test_lstm_pipeline()\n",
    "# r_tft = test_tft_pipeline()\n",
    "print(r_lin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfc_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

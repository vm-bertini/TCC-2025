{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6db5d9d0",
   "metadata": {},
   "source": [
    "# UtilitÃ¡rios Unificados â€” Modelos, Datasets e RuÃ­do\n",
    "\n",
    "Este arquivo reÃºne trÃªs funÃ§Ãµes principais, com interface unificada e documentaÃ§Ã£o clara:\n",
    "\n",
    "1. `load_model_unificado(modelo, caminho, ...)`\n",
    "   - Carrega qualquer modelo: `linear`, `mlp`, `lstm` (Keras) ou `tft` (PyTorch Forecasting).\n",
    "   - Aceita arquivo/diretÃ³rio EXATO ou uma pasta raiz para descoberta recursiva.\n",
    "\n",
    "2. Carregamento de dados (Parquet)\n",
    "   - `linear/mlp/lstm` â†’ Parquet com `*.meta.json` contendo `x_dim`, `y_dim` (e `seq_len`, `lead` no LSTM).\n",
    "   - `tft` â†’ Parquet (padrÃ£o: retorna DataFrame; opcional: cria `TimeSeriesDataSet`).\n",
    "\n",
    "3. `add_noise_features(obj, sigma, tipo, ...)`\n",
    "   - Adiciona ruÃ­do GAUSSIANO somente nas FEATURES.\n",
    "   - `tipo='tfdata'` â†’ aplica em `tf.data.Dataset` (x,y).\n",
    "   - `tipo='tft'` â†’ aplica em batches de `TimeSeriesDataSet`/`DataLoader` (chaves `encoder_cont`/`decoder_cont`).\n",
    "\n",
    "> ObservaÃ§Ã£o: O notebook foi simplificado para Parquet apenas (sem TFRecords)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd8ef53",
   "metadata": {},
   "source": [
    "## 1) Carregamento Unificado de Modelos\n",
    "\n",
    "Contrato rÃ¡pido:\n",
    "- Entradas:\n",
    "  - `modelo`: `linear` | `mlp` | `lstm` | `tft`\n",
    "  - `caminho`: arquivo/diretÃ³rio exato OU uma pasta para varredura recursiva\n",
    "  - `prefer_exts` (opcional): lista de extensÃµes a priorizar (ex.: `[\".cpfg\", \".ckpt\"]` para TFT)\n",
    "  - `allow_unsafe` (bool): permite desserializaÃ§Ã£o insegura apenas para artefatos LOCAIS (Lambda em Keras)\n",
    "- SaÃ­das: `(obj_modelo, info)`\n",
    "  - `obj_modelo`: instancia do modelo carregado (Keras ou TemporalFusionTransformer)\n",
    "  - `info`: dicionÃ¡rio com metadados Ãºteis (`path`, `backend`, `kind`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca4ba9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 07:47:58.978429: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1763030881.161941   17053 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9975 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:06:00.0, compute capability: 8.6\n",
      "/home/victor-bertini/Documentos/tcc_2025/TCC-2025/tfc_venv_keras/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'adam', because it has 6 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TensorFlow model loaded.\n",
      "âœ… TensorFlow model loaded.\n",
      "âœ… TensorFlow model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Carregamento dos modelos treinados\n",
    "from tensorflow import keras\n",
    "import json, os\n",
    "import tensorflow as tf\n",
    "from keras.saving import register_keras_serializable  \n",
    "\n",
    "@register_keras_serializable()\n",
    "def repeat_to_seq_len(inputs):\n",
    "    emb, feats = inputs\n",
    "    seq_len = tf.shape(feats)[1]\n",
    "    emb_tiled = tf.tile(emb, [1, seq_len, 1])  # (batch, seq_len, emb_dim)\n",
    "    return emb_tiled\n",
    "\n",
    "def load_model(path: str):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"âŒ File not found: {path}\")\n",
    "\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "\n",
    "    # === Load model ===\n",
    "    if ext == \".keras\":\n",
    "        # âœ… repeat dynamically with tf.tile (preserves shape)\n",
    "        model = keras.models.load_model(path)\n",
    "        print(\"âœ… TensorFlow model loaded.\")\n",
    "    else:\n",
    "        raise ValueError(f\"âŒ Unsupported file extension: {ext}\")\n",
    "\n",
    "    # === Load optional JSON config ===\n",
    "    json_path = f\"{os.path.splitext(path)[0]}.model.json\"\n",
    "    config = None\n",
    "    if os.path.exists(json_path):\n",
    "        with open(json_path, \"r\") as f:\n",
    "            config = json.load(f)\n",
    "        print(f\"ðŸ§© Loaded config: {json_path}\")\n",
    "\n",
    "    return model, config\n",
    "\n",
    "\n",
    "## Carregando modelo linear\n",
    "linear, info_linear = load_model('./modelos/treinamento/linear.keras')\n",
    "\n",
    "## Carregando modelo MLP\n",
    "mlp, info_mlp = load_model('./modelos/treinamento/mlp.keras')\n",
    "\n",
    "## Carregando modelo LSTM\n",
    "lstm, info_lstm = load_model('./modelos/treinamento/lstm.keras')\n",
    "\n",
    "model_list = [\n",
    "    (linear, info_linear),\n",
    "    (mlp, info_mlp),\n",
    "    (lstm, info_lstm)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b698b7",
   "metadata": {},
   "source": [
    "## 2) Carregando preprocessadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fd49a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Todos os preprocessadores carregados com sucesso.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "# === LINEAR ===\n",
    "linear_path = \"./data/treinamento/preprocessor/linear_preproc.pkl\"\n",
    "with open(linear_path, \"rb\") as f:\n",
    "    linear_preproc = pickle.load(f)\n",
    "\n",
    "\n",
    "# === LSTM ===\n",
    "lstm_path = \"./data/treinamento/preprocessor/lstm_preproc.pkl\"\n",
    "with open(lstm_path, \"rb\") as f:\n",
    "    lstm_preproc = pickle.load(f)\n",
    "\n",
    "# === PREPROCESSORS DICT ===\n",
    "preprocessors = {\n",
    "    \"linear\": linear_preproc,\n",
    "    \"mlp\": linear_preproc,  # MLP usa o mesmo prÃ©-processador do Linear\n",
    "    \"lstm\": lstm_preproc\n",
    "}\n",
    "\n",
    "print(\"âœ… Todos os preprocessadores carregados com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0afe84",
   "metadata": {},
   "source": [
    "# FunÃ§Ãµes helper para anÃ¡lise dos modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e44a40",
   "metadata": {},
   "source": [
    "## FunÃ§Ãµes de coleta de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d85f6b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from preprocessor_keras import LinearPreprocessor, LSTMPreprocessor\n",
    "import numpy as np\n",
    "\n",
    "def load_dataset_info(model_type: str, dataset_type: str, problem_name: str) -> Dict[str, Any]:\n",
    "    info_path = f'./data/{problem_name}/{model_type}_dataset_{dataset_type}.meta.json'\n",
    "    if not os.path.exists(info_path):\n",
    "        raise FileNotFoundError(f\"âŒ Dataset info file not found: {info_path}\")\n",
    "    with open(info_path, 'r') as f:\n",
    "        info = json.load(f)\n",
    "    return info\n",
    "\n",
    "\n",
    "\n",
    "def get_problem_df(model_type: str, lag, lead, country_list, problem_name) -> pd.DataFrame:\n",
    "    # Instanciando preprocessadores\n",
    "    dataset_info = load_dataset_info(model_type, \"test\", problem_name)\n",
    "    destino_dir = f'./data/{problem_name}'\n",
    "\n",
    "    if model_type == 'linear':\n",
    "        df, dataset_info = LinearPreprocessor.load_linear_parquet_dataset(\n",
    "        data_dir=destino_dir,\n",
    "        split='test',\n",
    "        batch_size=256,\n",
    "        shuffle=True\n",
    "        )\n",
    "    elif model_type == 'lstm':\n",
    "        df,dataset_info = LSTMPreprocessor.load_lstm_parquet_dataset(\n",
    "        data_dir=destino_dir,\n",
    "        split='test',\n",
    "        batch_size=256,\n",
    "        shuffle=True\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Modelo desconhecido: {model_type}\")\n",
    "    return df, dataset_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011a1ff4",
   "metadata": {},
   "source": [
    "## FunÃ§Ãµes de avaliaÃ§Ã£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fde0d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "def avaliar_modelo_keras(\n",
    "    model,\n",
    "    dataset,\n",
    "    titulo=\"AvaliaÃ§Ã£o do Modelo\",\n",
    "    problem_name=\"problema\",\n",
    "    max_samples=None,\n",
    "    n_leads=1,\n",
    "    save_dir=\"./resultados/graficos\",\n",
    "    show_plots=True,\n",
    "    df_info={},\n",
    "    preproc=None\n",
    "):\n",
    "    print(f\"ðŸš€ Avaliando modelo '{model.name}'...\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # 1) EVALUATE\n",
    "    # ==========================================================\n",
    "    try:\n",
    "        eval_out = model.evaluate(dataset, verbose=0, return_dict=True)\n",
    "        resultados = {k: float(v) for k, v in eval_out.items()}\n",
    "    except Exception:\n",
    "        try:\n",
    "            raw = model.evaluate(dataset, verbose=0)\n",
    "            raw = raw if isinstance(raw, (list, tuple)) else [raw]\n",
    "            names = model.metrics_names or [f\"metric_{i}\" for i in range(len(raw))]\n",
    "            resultados = {n: float(v) for n, v in zip(names, raw)}\n",
    "        except:\n",
    "            resultados = {}\n",
    "\n",
    "    # ==========================================================\n",
    "    # 2) PREDICT\n",
    "    # ==========================================================\n",
    "    # Detect model type once\n",
    "    is_linear = \"linear\" in df_info[\"basename\"]\n",
    "    is_lstm   = \"lstm\"   in df_info[\"basename\"]\n",
    "\n",
    "    X_parts = []\n",
    "    preds = []\n",
    "    trues = []\n",
    "\n",
    "    for Xb, Yb in dataset:\n",
    "        preds.append(model.predict(Xb, verbose=0))\n",
    "        trues.append(Yb)\n",
    "\n",
    "        if is_linear:\n",
    "            # Xb is already a tensor (batch, features)\n",
    "            X_parts.append(Xb.numpy())\n",
    "\n",
    "        elif is_lstm:\n",
    "            # LSTM/X uses dict inputs\n",
    "            X_parts.append(Xb[\"num_feats\"].numpy())\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model basename: {df_info['basename']}\")\n",
    "\n",
    "    # === Now safe to concatenate ===\n",
    "    Y_pred = np.concatenate(preds, axis=0)\n",
    "    Y_real = np.concatenate(trues, axis=0)\n",
    "    X_vals = np.concatenate(X_parts, axis=0)\n",
    "\n",
    "    # ==========================================================\n",
    "    # 3) LIMIT SAMPLE SIZE\n",
    "    # ==========================================================\n",
    "    max_samples = len(Y_real) if not max_samples or max_samples <= 0 else min(max_samples, len(Y_real))\n",
    "    Y_real, Y_pred, X_vals = Y_real[:max_samples], Y_pred[:max_samples], X_vals[:max_samples]\n",
    "\n",
    "    # ==========================================================\n",
    "    # 4) CREATE DF FROM MODEL INPUT\n",
    "    # ==========================================================\n",
    "    df_x = pd.DataFrame()\n",
    "    df_y_real = pd.DataFrame()\n",
    "    df_y_pred = pd.DataFrame()\n",
    "\n",
    "    if is_lstm:\n",
    "        # Un sqe2sqe reshape\n",
    "        Y_real = Y_real.squeeze(-1)\n",
    "        Y_pred = Y_pred.squeeze(-1)\n",
    "        X_vals = X_vals[:, -1, :]\n",
    "        tmp = []\n",
    "        for col in df_info[\"target_cols\"]:\n",
    "            for lead in range(1, df_info[\"lead\"]+1):\n",
    "                tmp.append(f\"{col}_lead{lead}\")\n",
    "        df_info[\"target_cols\"] = tmp\n",
    "            \n",
    "    df_x = pd.DataFrame(X_vals, columns=df_info['feature_cols'])\n",
    "    df_y_real = pd.DataFrame(Y_real, columns=df_info['target_cols'])\n",
    "    df_y_pred = pd.DataFrame(Y_pred, columns=df_info['target_cols'])\n",
    "\n",
    "\n",
    "    # ==========================================================\n",
    "    # 5) DENORMALIZE + DECODE\n",
    "    # ==========================================================\n",
    "    if preproc:\n",
    "        df_x = preproc.denormalize('standard', df=df_x)\n",
    "        df_y_pred = preproc.denormalize('standard', df=df_y_pred)\n",
    "        df_y_real = preproc.denormalize('standard', df=df_y_real)\n",
    "\n",
    "        df_x = preproc.decode('label', df=df_x, target_col=\"country\")\n",
    "        df_x = preproc.decode('time_cycle', df=df_x)\n",
    "\n",
    "    # ==========================================================\n",
    "    # 6) COMBINE EVERYTHING INTO A SINGLE DF\n",
    "    # ==========================================================\n",
    "    df_full = df_x.copy()\n",
    "\n",
    "    # Attach real + predicted columns\n",
    "    for col in df_y_real.columns:\n",
    "        df_full[f\"{col}_real\"] = df_y_real[col].values\n",
    "    for col in df_y_pred.columns:\n",
    "        df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
    "\n",
    "    # Detect country columns\n",
    "    country_cols = [c for c in df_full.columns if \"country\" in c.lower()]\n",
    "\n",
    "    # Sorting order\n",
    "    order_cols = []\n",
    "    if \"datetime\" in df_full.columns:\n",
    "        order_cols.append(\"datetime\")\n",
    "    order_cols.extend(country_cols)\n",
    "\n",
    "    if order_cols:\n",
    "        df_full = df_full.sort_values(order_cols).reset_index(drop=True)\n",
    "    else:\n",
    "        print(\"âš ï¸ Nenhuma coluna para ordenar. Mantendo ordem original.\")\n",
    "\n",
    "    df_combined = df_full  # keep this for later use\n",
    "\n",
    "    # ==========================================================\n",
    "    # 7) OUTPUT DIR\n",
    "    # ==========================================================\n",
    "    output_dir = os.path.join(save_dir, problem_name, model.name)\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # ==========================================================\n",
    "    # 8) GLOBAL METRICS\n",
    "    # ==========================================================\n",
    "    yt_all = df_y_real.values.reshape(-1)\n",
    "    yp_all = df_y_pred.values.reshape(-1)\n",
    "    diff = yp_all - yt_all\n",
    "\n",
    "    mae = float(np.mean(np.abs(diff)))\n",
    "    mse = float(np.mean(diff ** 2))\n",
    "    rmse = float(np.sqrt(mse))\n",
    "\n",
    "    corr = float(np.corrcoef(yt_all, yp_all)[0, 1]) \\\n",
    "        if np.std(yt_all) > 0 and np.std(yp_all) > 0 else float(\"nan\")\n",
    "\n",
    "    ss_res = np.sum((yt_all - yp_all) ** 2)\n",
    "    ss_tot = np.sum((yt_all - np.mean(yt_all)) ** 2)\n",
    "    r2 = float(1 - ss_res / ss_tot) if ss_tot > 0 else float(\"nan\")\n",
    "\n",
    "    resultados.update(dict(mae=mae, mse=mse, rmse=rmse, correlacao_pearson=corr, r2=r2))\n",
    "\n",
    "    # ==========================================================\n",
    "    # 9) PER-LEAD METRICS\n",
    "    # ==========================================================\n",
    "    per_lead = {}\n",
    "    lead_cols = df_y_real.columns\n",
    "\n",
    "    for col in lead_cols:\n",
    "        t = df_y_real[col].values\n",
    "        p = df_y_pred[col].values\n",
    "        d = p - t\n",
    "\n",
    "        mae_i = float(np.mean(np.abs(d)))\n",
    "        mse_i = float(np.mean(d ** 2))\n",
    "        rmse_i = float(np.sqrt(mse_i))\n",
    "\n",
    "        corr_i = float(np.corrcoef(t, p)[0, 1]) \\\n",
    "            if np.std(t) > 0 and np.std(p) > 0 else float(\"nan\")\n",
    "\n",
    "        ss_res = np.sum((t - p) ** 2)\n",
    "        ss_tot = np.sum((t - np.mean(t)) ** 2)\n",
    "        r2_i = float(1 - ss_res / ss_tot) if ss_tot > 0 else float(\"nan\")\n",
    "\n",
    "        for key, val in zip([\"mae\", \"mse\", \"rmse\", \"correlacao_pearson\", \"r2\"],\n",
    "                            [mae_i, mse_i, rmse_i, corr_i, r2_i]):\n",
    "            per_lead.setdefault(key, []).append(val)\n",
    "\n",
    "    # ==========================================================\n",
    "    # 10) SAVE OVERALL METRIC TABLE\n",
    "    # ==========================================================\n",
    "    num_items = [(k, v) for k, v in resultados.items() if isinstance(v, (int, float))]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 0.4 * len(num_items) + 1))\n",
    "    ax.axis(\"off\")\n",
    "    ax.table(\n",
    "        cellText=[[k, f\"{v:.6f}\"] for k, v in num_items],\n",
    "        colLabels=[\"MÃ©trica\", \"Valor\"],\n",
    "        loc=\"center\",\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(os.path.join(output_dir, \"overall_metrics.png\"), dpi=150)\n",
    "    plt.show() if show_plots else plt.close()\n",
    "\n",
    "    # ==========================================================\n",
    "    # 11) PLOT METRIC PER LEAD\n",
    "    # ==========================================================\n",
    "    for metric, vals in per_lead.items():\n",
    "        xs = np.arange(1, len(vals) + 1)\n",
    "        plt.figure(figsize=(9, 4))\n",
    "        plt.plot(xs, vals, marker=\"o\")\n",
    "        plt.title(f\"{titulo} â€” {metric.upper()} por Lead\")\n",
    "        plt.xlabel(\"Lead\")\n",
    "        plt.ylabel(metric.upper())\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "        plt.tight_layout()\n",
    "        fp = os.path.join(output_dir, f\"metric_{metric}.png\")\n",
    "        plt.savefig(fp, dpi=150)\n",
    "        plt.show() if show_plots else plt.close()\n",
    "\n",
    "    # ==========================================================\n",
    "    # 12) PLOTS POR PAÃS E LEAD\n",
    "    # ==========================================================\n",
    "    datetime_col = \"datetime\"\n",
    "    if datetime_col not in df_combined.columns:\n",
    "        raise ValueError(\"Coluna 'datetime' nÃ£o encontrada em df_combined.\")\n",
    "\n",
    "    if len(country_cols) >= 1:\n",
    "        main_country_col = country_cols[0]\n",
    "        unique_countries = df_combined[main_country_col].unique()\n",
    "    else:\n",
    "        unique_countries = [None]\n",
    "\n",
    "    for country in unique_countries:\n",
    "        if country is not None:\n",
    "            df_plot = df_combined[df_combined[main_country_col] == country]\n",
    "            tag = f\"{main_country_col}_{country}\"\n",
    "        else:\n",
    "            df_plot = df_combined\n",
    "            tag = \"all\"\n",
    "\n",
    "        time_axis = pd.to_datetime(df_plot[datetime_col])\n",
    "\n",
    "        for col in lead_cols:\n",
    "            real_col = f\"{col}_real\"\n",
    "            pred_col = f\"{col}_pred\"\n",
    "\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            plt.plot(time_axis, df_plot[real_col], label=f\"Real ({col})\", linewidth=2)\n",
    "            plt.plot(time_axis, df_plot[pred_col],\n",
    "                     label=f\"Pred ({col})\", linestyle=\"--\", linewidth=2)\n",
    "\n",
    "            plt.title(f\"{titulo} â€” {col} â€” {tag}\")\n",
    "            plt.xlabel(\"Tempo\")\n",
    "            plt.ylabel(\"Valor\")\n",
    "            plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "            plt.legend()\n",
    "\n",
    "            fp = os.path.join(output_dir, f\"{tag}_lead_{col}.png\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(fp, dpi=150)\n",
    "            plt.show() if show_plots else plt.close()\n",
    "\n",
    "    print(\"âœ… AvaliaÃ§Ã£o concluÃ­da.\")\n",
    "\n",
    "    return dict(\n",
    "        resultados,\n",
    "        y_true=Y_real,\n",
    "        y_pred=Y_pred,\n",
    "        per_lead_metrics=per_lead,\n",
    "        df=df_combined\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e135642",
   "metadata": {},
   "source": [
    "# N1A â€” SÃ©rie Univariada (seq_len=72, lead=72)\n",
    "\n",
    "\n",
    "Objetivo\n",
    "- Prever 24 horas de carga Ã  frente com janelas de 48 horas de histÃ³rico para um Ãºnico paÃ­s.\n",
    "\n",
    "\n",
    "Artefatos esperados\n",
    "- Parquet (Linear/MLP): `data/N1A/linear_dataset_{split}.parquet` + `linear_dataset_{split}.meta.json` â†’ { x_dim, y_dim }\n",
    "- Parquet (LSTM): `data/N1A/lstm_dataset_{split}.parquet` + `lstm_dataset_{split}.meta.json` â†’ { seq_len=240, lead=72, x_dim, y_dim }\n",
    "\n",
    "\n",
    "Modelos a comparar\n",
    "- Linear, MLP, LSTM (Keras) e, opcionalmente, TFT.\n",
    "\n",
    "\n",
    "MÃ©tricas e checks\n",
    "- MAE, RMSE, MAPE.\n",
    "- Checar: shapes conforme meta.json; ausÃªncia de NaNs; nÃºmero de amostras > 0.\n",
    "\n",
    "\n",
    "VisualizaÃ§Ãµes sugeridas\n",
    "- Boxplot de erro por horizonte; barras de MAE por modelo; curva MAE vs horizonte.\n",
    "\n",
    "\n",
    "Notas\n",
    "- As variantes A/B sÃ£o obtidas reduzindo a janela/horizonte efetivos na avaliaÃ§Ã£o a partir do dataset base (240/72).\n",
    "- Padding (se houver) deve usar sentinela fixo para permitir mascaramento e ruÃ­do seletivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73153dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Avaliando modelo 'linear_model'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 07:48:03.808493: I external/local_xla/xla/service/service.cc:163] XLA service 0x18713540 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-11-13 07:48:03.808504: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2025-11-13 07:48:03.816563: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-11-13 07:48:03.831921: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91501\n",
      "I0000 00:00:1763030884.270663   17137 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2025-11-13 07:48:04.291774: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-13 07:48:04.960047: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_17', 36 bytes spill stores, 36 bytes spill loads\n",
      "\n",
      "2025-11-13 07:48:05.208740: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-13 07:48:05.914265: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8', 164 bytes spill stores, 164 bytes spill loads\n",
      "\n",
      "2025-11-13 07:48:06.794911: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "/home/victor-bertini/Documentos/tcc_2025/TCC-2025/preprocessor.py:302: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[tgt] = dt\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/home/victor-bertini/Documentos/tcc_2025/TCC-2025/tfc_venv_keras/lib/python3.12/site-packages/numpy/_core/_methods.py:170: RuntimeWarning: overflow encountered in reduce\n",
      "  arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… AvaliaÃ§Ã£o concluÃ­da.\n",
      "ðŸš€ Avaliando modelo 'mlp_model'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 07:48:23.246058: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-13 07:48:23.246092: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-13 07:48:24.180425: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 116 bytes spill stores, 116 bytes spill loads\n",
      "\n",
      "2025-11-13 07:48:24.521937: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-13 07:48:24.521957: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-13 07:48:24.727363: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_48', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-11-13 07:48:25.236680: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_48', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-11-13 07:48:25.429016: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-11-13 07:48:25.494260: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 296 bytes spill stores, 296 bytes spill loads\n",
      "\n",
      "2025-11-13 07:48:25.805747: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-13 07:48:25.805765: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-13 07:48:26.528748: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-11-13 07:48:26.781163: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9', 464 bytes spill stores, 464 bytes spill loads\n",
      "\n",
      "2025-11-13 07:48:26.903948: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_20', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "2025-11-13 07:48:28.188434: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "/home/victor-bertini/Documentos/tcc_2025/TCC-2025/preprocessor.py:302: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[tgt] = dt\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n",
      "/tmp/ipykernel_17053/3228294489.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_full[f\"{col}_pred\"] = df_y_pred[col].values\n"
     ]
    }
   ],
   "source": [
    "# Carregamento dos dataset N1A (Parquet)\n",
    "\n",
    "# linear/mlp (iguais)\n",
    "ds_linear, linear_info = get_problem_df(\n",
    "    model_type='linear',\n",
    "    lag=72,\n",
    "    lead=72,\n",
    "    country_list=['ES'],\n",
    "    problem_name='N1A'\n",
    ")\n",
    "\n",
    "# lstm\n",
    "ds_lstm, lstm_info = get_problem_df(\n",
    "    model_type='lstm',\n",
    "    lag=72,\n",
    "    lead=72,\n",
    "    country_list=['ES'],\n",
    "    problem_name='N1A'\n",
    ")\n",
    "\n",
    "\n",
    "## Avaliando dados\n",
    "\n",
    "### Avaliando modelo Linear\n",
    "avaliar_modelo_keras(\n",
    "    model=linear,\n",
    "    dataset=ds_linear,\n",
    "    titulo=\"Modelo Linear - Problema N1A\",\n",
    "    problem_name=\"N1A\",\n",
    "    n_leads=72,\n",
    "    max_samples=5000,\n",
    "    show_plots=False,\n",
    "    preproc=preprocessors['linear'],   \n",
    "    df_info=linear_info\n",
    ")\n",
    "\n",
    "### Avaliando modelo MLP\n",
    "avaliar_modelo_keras(\n",
    "    model=mlp,\n",
    "    dataset=ds_linear,\n",
    "    titulo=\"Modelo MLP - Problema N1A\",\n",
    "    problem_name=\"N1A\",\n",
    "    n_leads=72,\n",
    "    max_samples=5000,\n",
    "    show_plots=False,\n",
    "    preproc=preprocessors['mlp'],\n",
    "               \n",
    "            \n",
    "                \n",
    "    df_info=linear_info\n",
    ")\n",
    "\n",
    "### Avaliando modelo LSTM\n",
    "avaliar_modelo_keras(\n",
    "    model=lstm,\n",
    "    dataset=ds_lstm,\n",
    "    titulo=\"Modelo lstm - Problema N1A\",\n",
    "    problem_name=\"N1A\",\n",
    "    n_leads=72,\n",
    "    max_samples=5000,\n",
    "    show_plots=False,\n",
    "    preproc=preprocessors['lstm'],\n",
    "               \n",
    "            \n",
    "                \n",
    "    df_info=lstm_info\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb06705",
   "metadata": {},
   "source": [
    "# N1B â€” SÃ©rie Univariada (seq_len=168, lead=48)\n",
    "\n",
    "\n",
    "Objetivo\n",
    "- Prever 48 horas de carga Ã  frente com janelas de 168 horas de histÃ³rico para um Ãºnico paÃ­s.\n",
    "\n",
    "\n",
    "Artefatos esperados\n",
    "- Parquet (Linear/MLP): `data/N1B/linear_dataset_{split}.parquet` + meta { x_dim, y_dim }\n",
    "- Parquet (LSTM): `data/N1B/lstm_dataset_{split}.parquet` + meta { seq_len=240, lead=72, x_dim, y_dim }\n",
    "- TFT (opcional): `data/treinamento/tft_dataset_{split}.parquet`\n",
    "\n",
    "\n",
    "Modelos a comparar\n",
    "- Linear, MLP, LSTM, TFT (opcional).\n",
    "\n",
    "\n",
    "MÃ©tricas e checks\n",
    "- MAE, RMSE, MAPE; validaÃ§Ã£o de shapes e ausÃªncia de NaNs.\n",
    "\n",
    "\n",
    "VisualizaÃ§Ãµes sugeridas\n",
    "- Barras de MAE mÃ©dio por modelo; curva de erro por horizonte.\n",
    "\n",
    "\n",
    "Notas\n",
    "- As variantes A/B sÃ£o derivadas do dataset base (240/72) reduzindo janela/horizonte na avaliaÃ§Ã£o, sem retreinar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb1b4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento dos dataset N1B (Parquet)\n",
    "\n",
    "# linear/mlp (iguais)\n",
    "ds_linear, linear_info = get_problem_df(\n",
    "    model_type='linear',\n",
    "    lag=168,\n",
    "    lead=72,\n",
    "    country_list=['ES'],\n",
    "    problem_name='N1B'\n",
    ")\n",
    "\n",
    "# lstm\n",
    "ds_lstm, lstm_info = get_problem_df(\n",
    "    model_type='lstm',\n",
    "    lag=168,\n",
    "    lead=72,\n",
    "    country_list=['ES'],\n",
    "    problem_name='N1B'\n",
    ")\n",
    "\n",
    "\n",
    "## Avaliando dados\n",
    "\n",
    "## Avaliando modelo Linear\n",
    "avaliar_modelo_keras(\n",
    "    model=linear,\n",
    "    dataset=ds_linear,\n",
    "    titulo=\"Modelo Linear - Problema N1B\",\n",
    "    problem_name=\"N1B\",\n",
    "    n_leads=72,\n",
    "    max_samples=5000,\n",
    "    show_plots=False,\n",
    "    preproc=preprocessors['linear'],\n",
    "               \n",
    "            \n",
    "                \n",
    "    df_info=linear_info\n",
    ")\n",
    "\n",
    "### Avaliando modelo MLP\n",
    "avaliar_modelo_keras(\n",
    "    model=mlp,\n",
    "    dataset=ds_linear,\n",
    "    titulo=\"Modelo MLP - Problema N1B\",\n",
    "    problem_name=\"N1B\",\n",
    "    n_leads=72,\n",
    "    max_samples=5000,\n",
    "    show_plots=False,\n",
    "    preproc=preprocessors['mlp'],\n",
    "               \n",
    "            \n",
    "                \n",
    "    df_info=linear_info\n",
    ")\n",
    "\n",
    "### Avaliando modelo LSTM\n",
    "avaliar_modelo_keras(\n",
    "    model=lstm,\n",
    "    dataset=ds_lstm,\n",
    "    titulo=\"Modelo lstm - Problema N1B\",\n",
    "    problem_name=\"N1B\",\n",
    "    n_leads=72,\n",
    "    max_samples=5000,\n",
    "    show_plots=False,\n",
    "    preproc=preprocessors['lstm'],\n",
    "               \n",
    "            \n",
    "                \n",
    "    df_info=lstm_info\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ca18c5",
   "metadata": {},
   "source": [
    "# N1C â€” SÃ©rie Univariada (seq_len=240, lead=72)\n",
    "\n",
    "\n",
    "Objetivo\n",
    "- PrÃ©-treino/treino com a janela de 240 horas e avaliar horizonte de 72 horas.\n",
    "\n",
    "\n",
    "Artefatos esperados\n",
    "- Parquet (Linear/MLP): `data/N1C/linear_dataset_{split}.parquet` + meta { x_dim, y_dim }\n",
    "- Parquet (LSTM): `data/N1C/lstm_dataset_{split}.parquet` + meta { seq_len=240, lead=72, x_dim, y_dim }\n",
    "- TFT (opcional): `data/treinamento/tft_dataset_{split}.parquet`\n",
    "\n",
    "\n",
    "Modelos a comparar\n",
    "- Linear, MLP, LSTM, TFT (opcional).\n",
    "\n",
    "\n",
    "MÃ©tricas e checks\n",
    "- MAE, RMSE, MAPE; nÃºmero de amostras por split; coerÃªncia entre seq_len/lead do meta e shapes efetivos.\n",
    "\n",
    "\n",
    "VisualizaÃ§Ãµes sugeridas\n",
    "- Curva comparativa de MAE vs horizonte; topâ€‘k modelos por MAE.\n",
    "\n",
    "\n",
    "Notas\n",
    "- Esta variante (C) Ã© a base mÃ¡xima de lookback e horizonte; A/B sÃ£o obtidas por reduÃ§Ã£o na avaliaÃ§Ã£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e50ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento dos dataset N1C (Parquet)\n",
    "\n",
    "# linear/mlp (iguais)\n",
    "ds_linear, linear_info = get_problem_df(\n",
    "    model_type='linear',\n",
    "    lag=240,\n",
    "    lead=72,\n",
    "    country_list=['ES'],\n",
    "    problem_name='N1C'\n",
    ")\n",
    "\n",
    "# lstm\n",
    "ds_lstm, lstm_info = get_problem_df(\n",
    "    model_type='lstm',\n",
    "    lag=240,\n",
    "    lead=72,\n",
    "    country_list=['ES'],\n",
    "    problem_name='N1C'\n",
    ")\n",
    "\n",
    "\n",
    "## Avaliando dados\n",
    "\n",
    "## Avaliando modelo Linear\n",
    "avaliar_modelo_keras(\n",
    "    model=linear,\n",
    "    dataset=ds_linear,\n",
    "    titulo=\"Modelo Linear - Problema N1C\",\n",
    "    problem_name=\"N1C\",\n",
    "    n_leads=72,\n",
    "    max_samples=5000,\n",
    "    show_plots=False,\n",
    "    preproc=preprocessors['linear'],\n",
    "               \n",
    "            \n",
    "                \n",
    "    df_info=linear_info\n",
    ")\n",
    "\n",
    "### Avaliando modelo MLP\n",
    "avaliar_modelo_keras(\n",
    "    model=mlp,\n",
    "    dataset=ds_linear,\n",
    "    titulo=\"Modelo MLP - Problema N1C\",\n",
    "    problem_name=\"N1C\",\n",
    "    n_leads=72,\n",
    "    max_samples=5000,\n",
    "    show_plots=False,\n",
    "    preproc=preprocessors['mlp'],\n",
    "               \n",
    "            \n",
    "                \n",
    "    df_info=linear_info\n",
    ")\n",
    "\n",
    "### Avaliando modelo LSTM\n",
    "avaliar_modelo_keras(\n",
    "    model=lstm,\n",
    "    dataset=ds_lstm,\n",
    "    titulo=\"Modelo lstm - Problema N1C\",\n",
    "    problem_name=\"N1C\",\n",
    "    n_leads=72,\n",
    "    max_samples=5000,\n",
    "    show_plots=False,\n",
    "    preproc=preprocessors['lstm'],\n",
    "               \n",
    "            \n",
    "                \n",
    "    df_info=lstm_info\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264ad282",
   "metadata": {},
   "source": [
    "# N2A â€” MÃºltiplos PaÃ­ses (seq_len=72, lead=24)\n",
    "\n",
    "\n",
    "Objetivo\n",
    "- Prever 24 horas com 72 horas de histÃ³rico, agrupando por paÃ­s.\n",
    "\n",
    "\n",
    "Artefatos esperados\n",
    "- Parquet (Linear/MLP): `data/N2A/linear_dataset_{split}.parquet` + meta\n",
    "- Parquet (LSTM): `data/N2A/lstm_dataset_{split}.parquet` + meta { seq_len=240, lead=72, x_dim, y_dim }\n",
    "- TFT (recomendado): `data/treinamento/tft_dataset_{split}.parquet` (colunas: _group_id=country, time_idx crescente por grupo, quantity_MW)\n",
    "\n",
    "\n",
    "Modelos a comparar\n",
    "- Linear, MLP, LSTM (podem exigir codificaÃ§Ã£o/flatten por grupo);\n",
    "- TFT (nativamente multiâ€‘grupo).\n",
    "\n",
    "\n",
    "MÃ©tricas e checks\n",
    "- MAE/RMSE por paÃ­s e globais; nÃºmero de grupos; equilÃ­brio de amostras por grupo.\n",
    "\n",
    "\n",
    "VisualizaÃ§Ãµes sugeridas\n",
    "- Barras de MAE por modelo; facetas por paÃ­s; curva MAE vs horizonte.\n",
    "\n",
    "\n",
    "Notas\n",
    "- As variantes A/B/C partem do dataset base (240/72), aplicando janelas/horizontes reduzidos na avaliaÃ§Ã£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d186b896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento dos dataset N2A (Parquet)\n",
    "\n",
    "# linear/mlp (iguais)\n",
    "ds_linear, linear_info = get_problem_df(\n",
    "    model_type='linear',\n",
    "    lag=72,\n",
    "    lead=72,\n",
    "    country_list=['ES'],\n",
    "    problem_name='N2A'\n",
    ")\n",
    "\n",
    "# lstm\n",
    "ds_lstm, lstm_info = get_problem_df(\n",
    "    model_type='lstm',\n",
    "    lag=72,\n",
    "    lead=72,\n",
    "    country_list=['ES'],\n",
    "    problem_name='N2A'\n",
    ")\n",
    "\n",
    "\n",
    "## Avaliando dados\n",
    "\n",
    "## Avaliando modelo Linear\n",
    "avaliar_modelo_keras(\n",
    "    model=linear,\n",
    "    dataset=ds_linear,\n",
    "    titulo=\"Modelo Linear - Problema N2A\",\n",
    "    problem_name=\"N2A\",\n",
    "    n_leads=72,\n",
    "    max_samples=5000,\n",
    "    show_plots=False,\n",
    "    preproc=preprocessors['linear'],\n",
    "               \n",
    "            \n",
    "                \n",
    "    df_info=linear_info\n",
    ")\n",
    "\n",
    "### Avaliando modelo MLP\n",
    "avaliar_modelo_keras(\n",
    "    model=mlp,\n",
    "    dataset=ds_linear,\n",
    "    titulo=\"Modelo MLP - Problema N2A\",\n",
    "    problem_name=\"N2A\",\n",
    "    n_leads=72,\n",
    "    max_samples=5000,\n",
    "    show_plots=False,\n",
    "    preproc=preprocessors['mlp'],\n",
    "               \n",
    "            \n",
    "                \n",
    "    df_info=linear_info\n",
    ")\n",
    "\n",
    "### Avaliando modelo LSTM\n",
    "avaliar_modelo_keras(\n",
    "    model=lstm,\n",
    "    dataset=ds_lstm,\n",
    "    titulo=\"Modelo lstm - Problema N2A\",\n",
    "    problem_name=\"N2A\",\n",
    "    n_leads=72,\n",
    "    max_samples=5000,\n",
    "    show_plots=False,\n",
    "    preproc=preprocessors['lstm'],\n",
    "               \n",
    "            \n",
    "                \n",
    "    df_info=lstm_info\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b4db9d",
   "metadata": {},
   "source": [
    "# N2B â€” MÃºltiplos PaÃ­ses (seq_len=168, lead=48)\n",
    "\n",
    "\n",
    "Objetivo\n",
    "- Prever 48 horas com 168 horas de histÃ³rico, agrupado por paÃ­s.\n",
    "\n",
    "\n",
    "Artefatos esperados\n",
    "- Parquet (Linear/MLP): `data/N2B/linear_dataset_{split}.parquet` + meta\n",
    "- Parquet (LSTM): `data/N2B/lstm_dataset_{split}.parquet` + meta { seq_len=240, lead=72, x_dim, y_dim }\n",
    "- TFT (recomendado): `data/treinamento/tft_dataset_{split}.parquet` com `_group_id`, `time_idx`, target.\n",
    "\n",
    "\n",
    "Modelos a comparar\n",
    "- Linear, MLP, LSTM; TFT.\n",
    "\n",
    "\n",
    "MÃ©tricas e checks\n",
    "- MAE/RMSE por paÃ­s e agregadas; distribuiÃ§Ã£o de amostras por grupo.\n",
    "\n",
    "\n",
    "VisualizaÃ§Ãµes sugeridas\n",
    "- Barras de MAE por modelo; linhas por horizonte; painel por paÃ­s.\n",
    "\n",
    "\n",
    "Notas\n",
    "- Variantes A/B/C usam janelas/horizontes efetivos na avaliaÃ§Ã£o; base: 240/72."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72a93ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento dos dataset N2B (Parquet)\n",
    "\n",
    "# linear/mlp (iguais)\n",
    "ds_linear, linear_info = get_problem_df(\n",
    "    model_type='linear',\n",
    "    lag=168,\n",
    "    lead=72,\n",
    "    country_list=['ES'],\n",
    "    problem_name='N2B'\n",
    ")\n",
    "\n",
    "# lstm\n",
    "ds_lstm, lstm_info = get_problem_df(\n",
    "    model_type='lstm',\n",
    "    lag=168,\n",
    "    lead=72,\n",
    "    country_list=['ES'],\n",
    "    problem_name='N2B'\n",
    ")\n",
    "\n",
    "\n",
    "## Avaliando dados\n",
    "\n",
    "## Avaliando modelo Linear\n",
    "avaliar_modelo_keras(\n",
    "    model=linear,\n",
    "    dataset=ds_linear,\n",
    "    titulo=\"Modelo Linear - Problema N2B\",\n",
    "    problem_name=\"N2B\",\n",
    "    n_leads=72,\n",
    "    max_samples=5000,\n",
    "    show_plots=False,\n",
    "    preproc=preprocessors['linear'],\n",
    "               \n",
    "            \n",
    "                \n",
    "    df_info=linear_info\n",
    ")\n",
    "\n",
    "### Avaliando modelo MLP\n",
    "avaliar_modelo_keras(\n",
    "    model=mlp,\n",
    "    dataset=ds_linear,\n",
    "    titulo=\"Modelo MLP - Problema N2B\",\n",
    "    problem_name=\"N2B\",\n",
    "    n_leads=72,\n",
    "    max_samples=5000,\n",
    "    show_plots=False,\n",
    "    preproc=preprocessors['mlp'],\n",
    "               \n",
    "            \n",
    "                \n",
    "    df_info=linear_info\n",
    ")\n",
    "\n",
    "### Avaliando modelo LSTM\n",
    "avaliar_modelo_keras(\n",
    "    model=lstm,\n",
    "    dataset=ds_lstm,\n",
    "    titulo=\"Modelo lstm - Problema N2B\",\n",
    "    problem_name=\"N2B\",\n",
    "    n_leads=72,\n",
    "    max_samples=5000,\n",
    "    show_plots=False,\n",
    "    preproc=preprocessors['lstm'],\n",
    "               \n",
    "            \n",
    "                \n",
    "    df_info=lstm_info\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1278bb",
   "metadata": {},
   "source": [
    "# N2C â€” MÃºltiplos PaÃ­ses (seq_len=240, lead=72)\n",
    "\n",
    "\n",
    "Objetivo\n",
    "- Prever 72 horas com 240 horas de histÃ³rico, agrupado por paÃ­s. Esta variante Ã© a base para reuso em A/B.\n",
    "\n",
    "\n",
    "Artefatos esperados\n",
    "- Parquet (Linear/MLP): `data/N2C/linear_dataset_{split}.parquet` + meta\n",
    "- Parquet (LSTM): `data/N2C/lstm_dataset_{split}.parquet` + meta { seq_len=240, lead=72, x_dim, y_dim }\n",
    "- TFT (recomendado): `data/treinamento/tft_dataset_{split}.parquet` (agrupado por `_group_id`).\n",
    "\n",
    "\n",
    "Modelos a comparar\n",
    "- Linear, MLP, LSTM, TFT.\n",
    "\n",
    "\n",
    "MÃ©tricas e checks\n",
    "- MAE, RMSE, MAPE; comparaÃ§Ã£o por paÃ­s; checagem de time_idx e integridade por grupo.\n",
    "\n",
    "\n",
    "VisualizaÃ§Ãµes sugeridas\n",
    "- Curva MAE vs horizonte; ranking de modelos por paÃ­s e global.\n",
    "\n",
    "\n",
    "Notas\n",
    "- Base usa seq_len=240 e lead=72; variaÃ§Ãµes A/B podem ser avaliadas reduzindo janela no dataset sem retreino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfac332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento dos dataset N2C (Parquet)\n",
    "\n",
    "# linear/mlp (iguais)\n",
    "ds_linear, linear_info = get_problem_df(\n",
    "    model_type='linear',\n",
    "    lag=240,\n",
    "    lead=72,\n",
    "    country_list=['ES'],\n",
    "    problem_name='N2C'\n",
    ")\n",
    "\n",
    "# lstm\n",
    "ds_lstm, lstm_info = get_problem_df(\n",
    "    model_type='lstm',\n",
    "    lag=240,\n",
    "    lead=72,\n",
    "    country_list=['ES'],\n",
    "    problem_name='N2C'\n",
    ")\n",
    "\n",
    "\n",
    "## Avaliando dados\n",
    "\n",
    "## Avaliando modelo Linear\n",
    "avaliar_modelo_keras(\n",
    "    model=linear,\n",
    "    dataset=ds_linear,\n",
    "    titulo=\"Modelo Linear - Problema N2C\",\n",
    "    problem_name=\"N2C\",\n",
    "    n_leads=72,\n",
    "    max_samples=5000,\n",
    "    show_plots=False,\n",
    "    preproc=preprocessors['linear'],\n",
    "               \n",
    "            \n",
    "                \n",
    "    df_info=linear_info\n",
    ")\n",
    "\n",
    "### Avaliando modelo MLP\n",
    "avaliar_modelo_keras(\n",
    "    model=mlp,\n",
    "    dataset=ds_linear,\n",
    "    titulo=\"Modelo MLP - Problema N2C\",\n",
    "    problem_name=\"N2C\",\n",
    "    n_leads=72,\n",
    "    max_samples=5000,\n",
    "    show_plots=False,\n",
    "    preproc=preprocessors['mlp'],\n",
    "               \n",
    "            \n",
    "                \n",
    "    df_info=linear_info\n",
    ")\n",
    "\n",
    "### Avaliando modelo LSTM\n",
    "avaliar_modelo_keras(\n",
    "    model=lstm,\n",
    "    dataset=ds_lstm,\n",
    "    titulo=\"Modelo lstm - Problema N2C\",\n",
    "    problem_name=\"N2C\",\n",
    "    n_leads=72,\n",
    "    max_samples=5000,\n",
    "    show_plots=False,\n",
    "    preproc=preprocessors['lstm'],\n",
    "               \n",
    "            \n",
    "                \n",
    "    df_info=lstm_info\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dffd95",
   "metadata": {},
   "source": [
    "# N3 â€” Robustez a RuÃ­do (sobre N2)\n",
    "\n",
    "Objetivo\n",
    "- Medir degradaÃ§Ã£o de desempenho sob ruÃ­do gaussiano nas FEATURES (teste), mantendo rÃ³tulos intactos.\n",
    "\n",
    "ConfiguraÃ§Ã£o\n",
    "- Conjuntos: use os datasets do N2 (A/B/C).\n",
    "- Intensidades: Ïƒ âˆˆ {0.00, 0.01, 0.03, 0.05, 0.10}.\n",
    "- AplicaÃ§Ã£o:\n",
    "  - Keras/tf.data: `add_noise_features(ds, sigma, tipo='tfdata', pad_sentinel=-999.0)`.\n",
    "  - TFT: `add_noise_features(tft_ds ou dataloader, sigma, tipo='tft', batch_size=..., train=False)`.\n",
    "\n",
    "MÃ©tricas e checks\n",
    "- MAE/RMSE por sigma; checar preservaÃ§Ã£o de sentinela (TF) e invariÃ¢ncia de Y.\n",
    "\n",
    "VisualizaÃ§Ãµes sugeridas\n",
    "- Curvas MAE vs Ïƒ por modelo; heatmap de degradaÃ§Ã£o por horizonte e sigma.\n",
    "\n",
    "Notas\n",
    "- Aplique ruÃ­do apÃ³s normalizaÃ§Ã£o das features.\n",
    "- NÃ£o altere o treino; apenas avaliaÃ§Ã£o/benchmark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244d4ae4",
   "metadata": {},
   "source": [
    "## N3A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7943fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento dos dataset N3A (Parquet)\n",
    "\n",
    "# linear/mlp (iguais)\n",
    "ds_linear, linear_info = get_problem_df(\n",
    "    model_type='linear',\n",
    "    lag=72,\n",
    "    lead=72,\n",
    "    country_list=['ES'],\n",
    "    problem_name='N3A'\n",
    ")\n",
    "\n",
    "# lstm\n",
    "ds_lstm, lstm_info = get_problem_df(\n",
    "    model_type='lstm',\n",
    "    lag=72,\n",
    "    lead=72,\n",
    "    country_list=['ES'],\n",
    "    problem_name='N3A'\n",
    ")\n",
    "\n",
    "\n",
    "## Avaliando dados\n",
    "\n",
    "## Avaliando modelo Linear\n",
    "avaliar_modelo_keras(\n",
    "    model=linear,\n",
    "    dataset=ds_linear,\n",
    "    titulo=\"Modelo Linear - Problema N3A\",\n",
    "    problem_name=\"N3A\",\n",
    "    n_leads=72,\n",
    "    max_samples=5000,\n",
    "    show_plots=False,\n",
    "    preproc=preprocessors['linear'],\n",
    "               \n",
    "            \n",
    "                \n",
    "    df_info=linear_info\n",
    ")\n",
    "\n",
    "### Avaliando modelo MLP\n",
    "avaliar_modelo_keras(\n",
    "    model=mlp,\n",
    "    dataset=ds_linear,\n",
    "    titulo=\"Modelo MLP - Problema N3A\",\n",
    "    problem_name=\"N3A\",\n",
    "    n_leads=72,\n",
    "    max_samples=5000,\n",
    "    show_plots=False,\n",
    "    preproc=preprocessors['mlp'],\n",
    "               \n",
    "            \n",
    "                \n",
    "    df_info=linear_info\n",
    ")\n",
    "\n",
    "### Avaliando modelo LSTM\n",
    "avaliar_modelo_keras(\n",
    "    model=lstm,\n",
    "    dataset=ds_lstm,\n",
    "    titulo=\"Modelo lstm - Problema N3A\",\n",
    "    problem_name=\"N3A\",\n",
    "    n_leads=72,\n",
    "    max_samples=5000,\n",
    "    show_plots=False,\n",
    "    preproc=preprocessors['lstm'],\n",
    "               \n",
    "            \n",
    "                \n",
    "    df_info=lstm_info\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb89698",
   "metadata": {},
   "source": [
    "## N3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048418a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento dos dataset N3B (Parquet)\n",
    "\n",
    "# linear/mlp (iguais)\n",
    "ds_linear, linear_info = get_problem_df(\n",
    "    model_type='linear',\n",
    "    lag=168,\n",
    "    lead=72,\n",
    "    country_list=['ES'],\n",
    "    problem_name='N3B'\n",
    ")\n",
    "\n",
    "# lstm\n",
    "ds_lstm, lstm_info = get_problem_df(\n",
    "    model_type='lstm',\n",
    "    lag=168,\n",
    "    lead=72,\n",
    "    country_list=['ES'],\n",
    "    problem_name='N3B'\n",
    ")\n",
    "\n",
    "\n",
    "## Avaliando dados\n",
    "\n",
    "## Avaliando modelo Linear\n",
    "avaliar_modelo_keras(\n",
    "    model=linear,\n",
    "    dataset=ds_linear,\n",
    "    titulo=\"Modelo Linear - Problema N3B\",\n",
    "    problem_name=\"N3B\",\n",
    "    n_leads=72,\n",
    "    max_samples=5000,\n",
    "    show_plots=False,\n",
    "    preproc=preprocessors['linear'],\n",
    "               \n",
    "            \n",
    "                \n",
    "    df_info=linear_info\n",
    ")\n",
    "\n",
    "### Avaliando modelo MLP\n",
    "avaliar_modelo_keras(\n",
    "    model=mlp,\n",
    "    dataset=ds_linear,\n",
    "    titulo=\"Modelo MLP - Problema N3B\",\n",
    "    problem_name=\"N3B\",\n",
    "    n_leads=72,\n",
    "    max_samples=5000,\n",
    "    show_plots=False,\n",
    "    preproc=preprocessors['mlp'],\n",
    "               \n",
    "            \n",
    "                \n",
    "    df_info=linear_info\n",
    ")\n",
    "\n",
    "### Avaliando modelo LSTM\n",
    "avaliar_modelo_keras(\n",
    "    model=lstm,\n",
    "    dataset=ds_lstm,\n",
    "    titulo=\"Modelo lstm - Problema N3B\",\n",
    "    problem_name=\"N3B\",\n",
    "    n_leads=72,\n",
    "    max_samples=5000,\n",
    "    show_plots=False,\n",
    "    preproc=preprocessors['lstm'],\n",
    "               \n",
    "            \n",
    "                \n",
    "    df_info=lstm_info\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5154d8f1",
   "metadata": {},
   "source": [
    "## N3C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252e5cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento dos dataset N3C (Parquet)\n",
    "\n",
    "# linear/mlp (iguais)\n",
    "ds_linear, linear_info = get_problem_df(\n",
    "    model_type='linear',\n",
    "    lag=240,\n",
    "    lead=72,\n",
    "    country_list=['ES'],\n",
    "    problem_name='N3C'\n",
    ")\n",
    "\n",
    "# lstm\n",
    "ds_lstm, lstm_info = get_problem_df(\n",
    "    model_type='lstm',\n",
    "    lag=240,\n",
    "    lead=72,\n",
    "    country_list=['ES'],\n",
    "    problem_name='N3C'\n",
    ")\n",
    "\n",
    "\n",
    "## Avaliando dados\n",
    "\n",
    "## Avaliando modelo Linear\n",
    "avaliar_modelo_keras(\n",
    "    model=linear,\n",
    "    dataset=ds_linear,\n",
    "    titulo=\"Modelo Linear - Problema N3C\",\n",
    "    problem_name=\"N3C\",\n",
    "    n_leads=72,\n",
    "    max_samples=5000,\n",
    "    show_plots=False,\n",
    "    preproc=preprocessors['linear'],\n",
    "               \n",
    "            \n",
    "                \n",
    "    df_info=linear_info\n",
    ")\n",
    "\n",
    "### Avaliando modelo MLP\n",
    "avaliar_modelo_keras(\n",
    "    model=mlp,\n",
    "    dataset=ds_linear,\n",
    "    titulo=\"Modelo MLP - Problema N3C\",\n",
    "    problem_name=\"N3C\",\n",
    "    n_leads=72,\n",
    "    max_samples=5000,\n",
    "    show_plots=False,\n",
    "    preproc=preprocessors['mlp'],\n",
    "               \n",
    "            \n",
    "                \n",
    "    df_info=linear_info\n",
    ")\n",
    "\n",
    "### Avaliando modelo LSTM\n",
    "avaliar_modelo_keras(\n",
    "    model=lstm,\n",
    "    dataset=ds_lstm,\n",
    "    titulo=\"Modelo lstm - Problema N3C\",\n",
    "    problem_name=\"N3C\",\n",
    "    n_leads=72,\n",
    "    max_samples=5000,\n",
    "    show_plots=False,\n",
    "    preproc=preprocessors['lstm'],\n",
    "               \n",
    "            \n",
    "                \n",
    "    df_info=lstm_info\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfc_venv_keras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

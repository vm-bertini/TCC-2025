{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1149ce94",
   "metadata": {},
   "source": [
    "## Depend√™ncias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce23fd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando depend√™ncias (pyarrow para Parquet)...\n",
      "PyArrow dispon√≠vel: 21.0.0\n",
      "Instalando python-dotenv...\n",
      "Instalando scikit-learn...\n",
      "Instalando scikit-learn...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depend√™ncias prontas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Checagem/instala√ß√£o leve de depend√™ncias\n",
    "print(\"Verificando depend√™ncias (pyarrow para Parquet)...\")\n",
    "\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    print(f\"PyArrow dispon√≠vel: {pa.__version__}\")\n",
    "except Exception:\n",
    "    print(\"Instalando pyarrow...\")\n",
    "    !pip install --upgrade \"pyarrow>=18\" --quiet\n",
    "    import importlib\n",
    "    importlib.invalidate_caches()\n",
    "    import pyarrow as pa\n",
    "    print(f\"PyArrow instalado: {pa.__version__}\")\n",
    "\n",
    "# fastparquet √© opcional\n",
    "try:\n",
    "    import fastparquet  # noqa: F401\n",
    "    print(\"fastparquet dispon√≠vel (opcional)\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Outras bibliotecas sob demanda\n",
    "for lib in [\n",
    "    \"numpy\", \"python-dotenv\", \"pandas\", \"matplotlib\", \"seaborn\",\n",
    "    \"scikit-learn\", \"tensorflow\", \"keras\", \"lxml\", \"pytz\", \"requests\", \"optuna\"\n",
    "]:\n",
    "    try:\n",
    "        __import__(lib)\n",
    "    except ImportError:\n",
    "        print(f\"Instalando {lib}...\")\n",
    "        !pip install {lib} --quiet\n",
    "\n",
    "print(\"Depend√™ncias prontas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cc7e78",
   "metadata": {},
   "source": [
    "## VARI√ÅVEIS NECESS√ÅRIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d89d36dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è No GPU detected, running on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Imports para a API e utilidades\n",
    "import os\n",
    "import requests\n",
    "import pandas\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta, date\n",
    "import pytz\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import mixed_precision\n",
    "import tensorflow as tf\n",
    "\n",
    "# ==============================================\n",
    "# GPU CONFIGURATION\n",
    "# ==============================================\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    mixed_precision.set_global_policy('mixed_float16')\n",
    "    print(f\"‚úÖ GPU detected ({gpus[0].name}) - using mixed precision.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected, running on CPU.\")\n",
    "\n",
    "# Carregar vari√°veis de ambiente do .env\n",
    "load_dotenv()\n",
    "# ---------------- CONFIG ---------------- #\n",
    "COUNTRY_DOMAINS = {\n",
    "    \"FR\": {\"domain\": \"10YFR-RTE------C\"},\n",
    "    \"ES\": {\"domain\": \"10YES-REE------0\"},\n",
    "    \"PT\": {\"domain\": \"10YPT-REN------W\"}\n",
    "}\n",
    "\n",
    "DATA_ITEMS = [\n",
    "    {'key': 'load_total', 'documentType': 'A65', 'processType': 'A16', 'domainParam': 'outBiddingZone_Domain', 'parser': 'load'},\n",
    "    {'key': 'market_prices', 'documentType': 'A44', 'processType': 'A07', 'domainParamIn': 'in_Domain', 'domainParamOut': 'out_Domain', 'parser': 'price'}\n",
    "]\n",
    "\n",
    "ENTSOE_TOKEN = os.environ.get(\"ENTSOE_SECURITY_TOKEN\")\n",
    "BASE_URL = \"https://web-api.tp.entsoe.eu/api\"\n",
    "MAX_WORKERS = 100\n",
    "RAW_DIR = os.path.join(\"data\", \"raw\")\n",
    "PARQUET_COMPRESSION = \"zstd\"\n",
    "os.makedirs(RAW_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5b47ef",
   "metadata": {},
   "source": [
    "# Cap√≠tulo 1: Pr√©processamento de dados\n",
    "\n",
    "Etapa de contru√ß√£o da pipelines de pre-processamento de dados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f852c4",
   "metadata": {},
   "source": [
    "## Classe geral de preprocessamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "302b0d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional, List, Tuple\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class Preprocessor:\n",
    "    \"\"\"Pr√©-processador base.\n",
    "\n",
    "    - lag/lead como inteiros s√£o expandidos para ranges [1..N] quando apropriado.\n",
    "    - feature_cols/target_cols definem bases permitidas e servem como sele√ß√£o no export.\n",
    "    - Nenhuma coluna √© removida dos dados; sele√ß√£o ocorre apenas na exporta√ß√£o.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        lag: int,\n",
    "        lead: int,\n",
    "        country_list: Optional[List[str]] = None,\n",
    "        *,\n",
    "        model_name: str = \"linear\",\n",
    "        data_dir: str = \"data/processed\",\n",
    "        feature_cols: Optional[List[str]] = None,\n",
    "        target_cols: Optional[List[str]] = None,\n",
    "    ):\n",
    "        self.lag = lag\n",
    "        self.lead = lead\n",
    "        self.country_list = country_list\n",
    "        self.model_name = model_name\n",
    "        self.data_dir = data_dir\n",
    "        self.save_dir = os.path.join(self.data_dir, self.model_name)\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "        self.feature_cols: List[str] = list(feature_cols) if feature_cols else []\n",
    "        self.target_cols: List[str] = list(target_cols) if target_cols else []\n",
    "\n",
    "        self.norm_objects = {}\n",
    "        self.encod_objects = {}\n",
    "        self.df_base = pd.DataFrame()\n",
    "\n",
    "    def _expand_steps(self, steps, default_max: Optional[int]) -> List[int]:\n",
    "        \"\"\"Normaliza passos: int‚Üí[1..N], None‚Üí[1..default_max], lista‚Üícomo est√°.\"\"\"\n",
    "        if isinstance(steps, int):\n",
    "            return list(range(1, steps + 1)) if steps > 0 else [1]\n",
    "        if steps is None and isinstance(default_max, int) and default_max > 0:\n",
    "            return list(range(1, default_max + 1))\n",
    "        if isinstance(steps, (list, tuple)):\n",
    "            return list(steps)\n",
    "        return [1]\n",
    "\n",
    "    def load_data(self, raw_dir: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"Carrega Parquet unificado em data/raw (ou raw_dir) e atualiza self.df_base.\"\"\"\n",
    "        base_raw = raw_dir or os.path.join('data', 'raw')\n",
    "        unified_path = os.path.join(base_raw, f'raw_dataset.parquet')\n",
    "        if not os.path.exists(unified_path):\n",
    "            raise FileNotFoundError(f\"Arquivo unificado n√£o encontrado: {unified_path}. Execute a coleta primeiro.\")\n",
    "        df = pd.read_parquet(unified_path, engine='pyarrow')\n",
    "        if 'datetime' in df.columns:\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "        if self.country_list and 'country' in df.columns:\n",
    "            df = df[df['country'].isin(self.country_list)].copy()\n",
    "        sort_cols = [c for c in ['country', 'datetime'] if c in df.columns]\n",
    "        if sort_cols:\n",
    "            df = df.sort_values(sort_cols).reset_index(drop=True)\n",
    "            \n",
    "        # Filtrando Colunas apenas para as necess√°rias\n",
    "        cols = list(set([c for c in self.feature_cols + self.target_cols if c in df.columns]))\n",
    "        df = df.loc[:, ~df.columns.duplicated()]  # optional: remove duplicates\n",
    "        df = df[cols]\n",
    "\n",
    "        self.df_base = df\n",
    "        return self.df_base\n",
    "\n",
    "    def encode(self, encode_cols: str = 'datetime', encode_method: str = 'label') -> pd.DataFrame:\n",
    "        \"\"\"Codifica de forma n√£o destrutiva e atualiza self.df_base.\n",
    "\n",
    "        - label: usa LabelEncoder com suporte a NaN via placeholder interno que √© revertido no decode.\n",
    "        - time_cycle: adiciona features de calend√°rio e c√≠clicas sem remover datetime.\n",
    "        \"\"\"\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            print(\"df_base vazio. Chame load_data() primeiro.\")\n",
    "            return self.df_base\n",
    "        df = self.df_base.copy()\n",
    "        if encode_method == 'label':\n",
    "            le = LabelEncoder()\n",
    "            s = df[encode_cols].astype(object)\n",
    "            le.fit(s)\n",
    "            df[encode_cols] = le.transform(s)\n",
    "            # salva metadados incluindo o code do NaN\n",
    "            self.encod_objects['label'] = {\n",
    "                'encode_cols': encode_cols,\n",
    "                'label_encoder': le,\n",
    "            }\n",
    "        elif encode_method == 'time_cycle':\n",
    "            if encode_cols not in df.columns:\n",
    "                print(f\"Coluna {encode_cols} n√£o encontrada para time_cycle.\")\n",
    "                self.df_base = df\n",
    "                return df\n",
    "            dt = pd.to_datetime(df[encode_cols], utc=True)\n",
    "            # Mant√©m a coluna original e adiciona componentes discretos e c√≠clicos\n",
    "            df['year'] = dt.dt.year\n",
    "            df['month'] = dt.dt.month\n",
    "            df['day'] = dt.dt.day\n",
    "            df['hour'] = dt.dt.hour\n",
    "            df['minute'] = dt.dt.minute\n",
    "            current_year = time.localtime().tm_year\n",
    "            df['year_sin'] = np.sin(2 * np.pi * df['year'] / max(current_year, 1))\n",
    "            df['year_cos'] = np.cos(2 * np.pi * df['year'] / max(current_year, 1))\n",
    "            df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "            df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "            df['day_sin'] = np.sin(2 * np.pi * df['day'] / 31)\n",
    "            df['day_cos'] = np.cos(2 * np.pi * df['day'] / 31)\n",
    "            df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "            df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "            df['minute_sin'] = np.sin(2 * np.pi * df['minute'] / 60)\n",
    "            df['minute_cos'] = np.cos(2 * np.pi * df['minute'] / 60)\n",
    "            self.encod_objects['time_cycle'] = {'encode_cols': encode_cols}\n",
    "            self.feature_cols.extend([\"year_sin\", \"year_cos\",\n",
    "                                                     \"month_sin\", \"month_cos\",\n",
    "                                                     \"day_sin\", \"day_cos\",\n",
    "                                                     \"hour_sin\", \"hour_cos\",\n",
    "                                                     \"minute_sin\", \"minute_cos\"])\n",
    "        else:\n",
    "            print(f\"encode_method '{encode_method}' n√£o suportado.\")\n",
    "        self.df_base = df\n",
    "        return self.df_base\n",
    "\n",
    "    def decode(self, encode_method: str = 'label', target_col: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"Reverte codifica√ß√µes suportadas (label, time_cycle).\"\"\"\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            print(\"df_base vazio. Nada para decodificar.\")\n",
    "            return self.df_base\n",
    "        df = self.df_base.copy()\n",
    "        if encode_method == 'label':\n",
    "            info = self.encod_objects.get('label')\n",
    "            if not info:\n",
    "                print(\"Nenhuma informa√ß√£o de label encoding salva.\")\n",
    "                return self.df_base\n",
    "            col = info['encode_cols']\n",
    "            le: LabelEncoder = info['label_encoder']\n",
    "            placeholder = info.get('na_placeholder', '__NA__')\n",
    "            try:\n",
    "                inv = le.inverse_transform(df[col].astype(int))\n",
    "                # mapeia placeholder de volta para NaN\n",
    "                inv = pd.Series(inv).replace(placeholder, np.nan).values\n",
    "                df[col] = inv\n",
    "            except Exception as e:\n",
    "                print(f\"Falha ao decodificar label para coluna {col}: {e}\")\n",
    "        elif encode_method == 'time_cycle':\n",
    "            if 'year' not in df.columns:\n",
    "                print(\"Componentes de tempo ausentes para reconstru√ß√£o.\")\n",
    "                return self.df_base\n",
    "            tgt = target_col or 'decoded_datetime'\n",
    "            def _recover_component(sin_col, cos_col, period, offset):\n",
    "                if sin_col not in df.columns or cos_col not in df.columns:\n",
    "                    return pd.Series([np.nan] * len(df))\n",
    "                ang = np.arctan2(df[sin_col], df[cos_col])\n",
    "                ang = (ang + 2 * np.pi) % (2 * np.pi)\n",
    "                idx = np.round((ang / (2 * np.pi)) * period).astype('Int64') % period\n",
    "                return idx + offset\n",
    "            month = _recover_component('month_sin', 'month_cos', 12, 1)\n",
    "            day = _recover_component('day_sin', 'day_cos', 31, 1)\n",
    "            hour = _recover_component('hour_sin', 'hour_cos', 24, 0)\n",
    "            minute = _recover_component('minute_sin', 'minute_cos', 60, 0)\n",
    "            year = df['year'] if 'year' in df.columns else pd.Series([np.nan] * len(df))\n",
    "            dt = pd.to_datetime({\n",
    "                'year': year.astype('Int64'),\n",
    "                'month': month.astype('Int64'),\n",
    "                'day': day.astype('Int64'),\n",
    "                'hour': hour.astype('Int64'),\n",
    "                'minute': minute.astype('Int64'),\n",
    "            }, errors='coerce', utc=True)\n",
    "            df[tgt] = dt\n",
    "        else:\n",
    "            print(f\"encode_method '{encode_method}' n√£o suportado para decode.\")\n",
    "        self.df_base = df\n",
    "        return self.df_base\n",
    "\n",
    "    def normalize(self, value_cols: List[str], normalization_method: str = 'minmax') -> pd.DataFrame:\n",
    "        \"\"\"Normaliza colunas e atualiza self.df_base.\"\"\"\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            print(\"df_base vazio. Chame load_data() primeiro.\")\n",
    "            return self.df_base\n",
    "        df = self.df_base.copy()\n",
    "        scaler = MinMaxScaler() if normalization_method == 'minmax' else (\n",
    "            StandardScaler() if normalization_method == 'standard' else None)\n",
    "        if scaler is None:\n",
    "            raise ValueError(\"normalization_method deve ser 'minmax' ou 'standard'\")\n",
    "        df[value_cols] = scaler.fit_transform(df[value_cols])\n",
    "        self.norm_objects[normalization_method] = {'value_cols': value_cols, 'scaler': scaler}\n",
    "        self.df_base = df\n",
    "        return self.df_base\n",
    "\n",
    "    def normalize_splits(self, value_cols: List[str], normalization_method: str = 'minmax') -> dict:\n",
    "        \"\"\"Normaliza os conjuntos de treino, valida√ß√£o e teste.\"\"\"\n",
    "        if not self.splits:\n",
    "            print(\"Nenhum conjunto dividido encontrado.\")\n",
    "            return {}\n",
    "        normalized_splits = {}\n",
    "        for split_name, split_df in self.splits.items():\n",
    "            self.df_base = split_df\n",
    "            normalized_df = self.normalize(value_cols=value_cols, normalization_method=normalization_method)\n",
    "            normalized_splits[split_name] = normalized_df\n",
    "        self.splits = normalized_splits\n",
    "        return normalized_splits\n",
    "\n",
    "    def denormalize(self, normalization_method: str = 'minmax') -> pd.DataFrame:\n",
    "        \"\"\"Reverte normaliza√ß√£o usando metadados salvos.\"\"\"\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            print(\"df_base vazio. Nada para denormalizar.\")\n",
    "            return self.df_base\n",
    "        info = self.norm_objects.get(normalization_method)\n",
    "        if not info:\n",
    "            print(f\"Nenhum scaler salvo para o m√©todo '{normalization_method}'.\")\n",
    "            return self.df_base\n",
    "        cols: List[str] = info['value_cols']\n",
    "        scaler = info['scaler']\n",
    "        df = self.df_base.copy()\n",
    "        try:\n",
    "            df[cols] = scaler.inverse_transform(df[cols])\n",
    "        except Exception as e:\n",
    "            print(f\"Falha ao denormalizar colunas {cols}: {e}\")\n",
    "            return self.df_base\n",
    "        self.df_base = df\n",
    "        return self.df_base\n",
    "\n",
    "    def save_df_base(self, filename: Optional[str] = None, compression: Optional[str] = None, partition_by: Optional[List[str]] = None) -> Optional[str]:\n",
    "        \"\"\"Salva self.df_base em Parquet dentro de data_dir/{model_name}.\"\"\"\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            print(\"df_base vazio. Nada para salvar.\")\n",
    "            return None\n",
    "        comp = compression\n",
    "        if comp is None:\n",
    "            try:\n",
    "                comp = PARQUET_COMPRESSION\n",
    "            except NameError:\n",
    "                comp = 'zstd'\n",
    "        filename = \"raw_dataset.parquet\"\n",
    "        out_path = os.path.join(self.save_dir, filename)\n",
    "        df = self.df_base.copy()\n",
    "        if 'datetime' in df.columns:\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "        try:\n",
    "            if partition_by:\n",
    "                df.to_parquet(out_path, engine='pyarrow', compression=comp, index=False, partition_cols=partition_by)\n",
    "            else:\n",
    "                df.to_parquet(out_path, engine='pyarrow', compression=comp, index=False)\n",
    "            print(f\"[SALVO] df_base: {len(df):,} linhas ‚Üí {out_path}\")\n",
    "            return out_path\n",
    "        except Exception as e:\n",
    "            print(f\"Falha ao salvar df_base em {out_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def split_train_val_test(self, train_size: float = 0.7, val_size: float = 0.15, test_size: float = 0.15, time_col: str = 'datetime') -> Optional[dict]:\n",
    "        \"\"\"Divide df_base em conjuntos de treino, valida√ß√£o e teste com base em time_col.\"\"\"\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            print(\"df_base vazio. Nada para dividir.\")\n",
    "            return None\n",
    "        if not np.isclose(train_size + val_size + test_size, 1.0):\n",
    "            print(\"train_size, val_size e test_size devem somar 1.0\")\n",
    "            return None\n",
    "        df = self.df_base.copy()\n",
    "        if time_col not in df.columns:\n",
    "            print(f\"Coluna de tempo '{time_col}' n√£o encontrada em df_base.\")\n",
    "            return None\n",
    "        df = df.sort_values(time_col).reset_index(drop=True)\n",
    "        n = len(df)\n",
    "        train_end = int(n * train_size)\n",
    "        val_end = train_end + int(n * val_size)\n",
    "        splits = {\n",
    "            'train': df.iloc[:train_end].reset_index(drop=True),\n",
    "            'val': df.iloc[train_end:val_end].reset_index(drop=True),\n",
    "            'test': df.iloc[val_end:].reset_index(drop=True),\n",
    "        }\n",
    "        for split_name, split_df in splits.items():\n",
    "            print(f\"[DIVIDIDO] {split_name}: {len(split_df):,} linhas\")\n",
    "        self.splits = splits\n",
    "        return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa7c121",
   "metadata": {},
   "source": [
    "## Preprocessamento do Modelo Linear\n",
    "\n",
    "Esse modelo deve ser√° contruido a partir de lags e leads passados como par√¢metros na fun√ß√£o, resultando na contru√ß√£o de novas colunas lead lag, assim gerando uma flat matrix 2D que ser√° usada no modelo linear\n",
    "\n",
    "Observa√ß√£o importante: lag e lead s√£o inteiros e representam o m√°ximo de passos; o pipeline expande para intervalos 1..N automaticamente. Por exemplo, lag=96 gera features com defasagens de 1 a 96; lead=96 gera alvos de 1 a 96.\n",
    "\n",
    "Os arquivos do modelo ser√£o salvos em TFrecords j√° que o modelo linear ser√° contru√≠do usando tensor flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75dd3ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "class LSTMPreprocessor(Preprocessor):\n",
    "    \"\"\"Pr√©-processador sequencial para LSTM: gera janelas 3D (N, seq_len, features).\"\"\"\n",
    "\n",
    "    def build_sequence_matrix(\n",
    "        self,\n",
    "        value_cols: Optional[List[str]] = None,\n",
    "        target_cols: Optional[List[str]] = None,\n",
    "        seq_len: Optional[int] = None,\n",
    "        lead: Optional[int] = None,\n",
    "        group_cols: Optional[List[str]] = None,\n",
    "        time_col: str = \"datetime\",\n",
    "        drop_last_incomplete: bool = True,\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Constr√≥i tensores X (entradas) e Y (alvos) para modelo LSTM.\n",
    "        Cada grupo (ex.: pa√≠s) √© processado separadamente e concatenado.\n",
    "        \"\"\"\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            print(\"df_base vazio. Chame load_data() primeiro.\")\n",
    "            return {}\n",
    "\n",
    "        df = self.df_base.copy()\n",
    "        feats = value_cols or self.feature_cols\n",
    "        tgts = target_cols or self.target_cols\n",
    "        if not feats:\n",
    "            raise ValueError(\"Nenhuma coluna de feature informada.\")\n",
    "        if not tgts:\n",
    "            raise ValueError(\"Nenhum target informado.\")\n",
    "\n",
    "        if time_col not in df.columns:\n",
    "            raise ValueError(f\"Coluna temporal '{time_col}' n√£o encontrada.\")\n",
    "\n",
    "        group_cols = group_cols or [c for c in [\"country\"] if c in df.columns]\n",
    "        sort_cols = (group_cols or []) + [time_col]\n",
    "        df = df.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "        if group_cols:\n",
    "            df[\"_group_id\"] = df[group_cols].astype(str).agg(\"_\".join, axis=1)\n",
    "        else:\n",
    "            df[\"_group_id\"] = \"global\"\n",
    "\n",
    "        seq_len = seq_len or getattr(self, \"seq_len\", 24)\n",
    "        lead = lead or getattr(self, \"lead\", 1)\n",
    "\n",
    "        X_list, Y_list = [], []\n",
    "        for gid, g in df.groupby(\"_group_id\", sort=False):\n",
    "            g = g.reset_index(drop=True)\n",
    "            if len(g) < seq_len + lead:\n",
    "                continue\n",
    "            X_src = g[feats].to_numpy(np.float32)\n",
    "            Y_src = g[tgts].to_numpy(np.float32)\n",
    "            for i in range(len(g) - seq_len - lead + 1):\n",
    "                x_win = X_src[i:i+seq_len]\n",
    "                y_val = Y_src[i+seq_len+lead-1]\n",
    "                X_list.append(x_win)\n",
    "                Y_list.append(y_val)\n",
    "\n",
    "        if not X_list:\n",
    "            print(\"[WARN] Nenhuma janela gerada.\")\n",
    "            return {}\n",
    "\n",
    "        X = np.stack(X_list)\n",
    "        Y = np.stack(Y_list)\n",
    "        print(f\"[JANELAS] X={X.shape}, Y={Y.shape}\")\n",
    "        self._seq_data = dict(X=X, Y=Y, seq_len=seq_len, x_dim=X.shape[-1], y_dim=Y.shape[-1])\n",
    "        return self._seq_data\n",
    "\n",
    "    def save_sequence_tfrecords(\n",
    "        self,\n",
    "        output_basename: str = 'lstm_dataset',\n",
    "        shard_size: int = 50_000,\n",
    "        compression: str = 'GZIP',\n",
    "    ) -> Optional[List[str]]:\n",
    "        \"\"\"Salva janelas (X,Y) como TFRecords comprimidos.\"\"\"\n",
    "        if not hasattr(self, \"_seq_data\"):\n",
    "            print(\"Nenhum dado sequencial encontrado. Execute build_sequence_matrix() antes.\")\n",
    "            return None\n",
    "\n",
    "        X, Y = self._seq_data[\"X\"], self._seq_data[\"Y\"]\n",
    "        seq_len, x_dim, y_dim = self._seq_data[\"seq_len\"], self._seq_data[\"x_dim\"], self._seq_data[\"y_dim\"]\n",
    "        n = len(X)\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "        options = tf.io.TFRecordOptions(compression_type=compression)\n",
    "        paths = []\n",
    "\n",
    "        def _bytes_feature(arr: np.ndarray) -> tf.train.Feature:\n",
    "            return tf.train.Feature(bytes_list=tf.train.BytesList(value=[arr.tobytes()]))\n",
    "\n",
    "        for shard_idx, start in enumerate(range(0, n, shard_size)):\n",
    "            end = min(start + shard_size, n)\n",
    "            shard_path = os.path.join(self.save_dir, f\"{output_basename}_{shard_idx}.tfrecord\")\n",
    "            with tf.io.TFRecordWriter(shard_path, options=options) as w:\n",
    "                for i in range(start, end):\n",
    "                    ex = tf.train.Example(features=tf.train.Features(feature={\n",
    "                        'x_raw': _bytes_feature(X[i]),\n",
    "                        'y_raw': _bytes_feature(Y[i]),\n",
    "                    }))\n",
    "                    w.write(ex.SerializeToString())\n",
    "            paths.append(shard_path)\n",
    "\n",
    "        meta = {\n",
    "            'seq_len': seq_len,\n",
    "            'x_dim': x_dim,\n",
    "            'y_dim': y_dim,\n",
    "            'compression': compression,\n",
    "            'count': int(n),\n",
    "            'basename': output_basename,\n",
    "        }\n",
    "        with open(os.path.join(self.save_dir, f\"{output_basename}.meta.json\"), 'w') as f:\n",
    "            json.dump(meta, f, indent=2)\n",
    "\n",
    "        print(f\"[‚úÖ] TFRecords salvos ({len(paths)} shards) em {self.save_dir}\")\n",
    "        return paths\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_tfrecord(example_proto, seq_len:int, x_dim:int, y_dim:int):\n",
    "        \"\"\"Fun√ß√£o para leitura dos TFRecords salvos.\"\"\"\n",
    "        features = {\n",
    "            'x_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "            'y_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "        parsed = tf.io.parse_single_example(example_proto, features)\n",
    "        x = tf.io.decode_raw(parsed['x_raw'], tf.float32)\n",
    "        y = tf.io.decode_raw(parsed['y_raw'], tf.float32)\n",
    "        x = tf.reshape(x, [seq_len, x_dim])\n",
    "        y = tf.reshape(y, [y_dim])\n",
    "        return x, y\n",
    "\n",
    "    @staticmethod\n",
    "    def load_sequence_dataset(\n",
    "        path_pattern: str,\n",
    "        seq_len: int,\n",
    "        x_dim: int,\n",
    "        y_dim: int,\n",
    "        batch_size: int = 256,\n",
    "        compression: str = 'GZIP'\n",
    "    ) -> tf.data.Dataset:\n",
    "        \"\"\"Carrega os TFRecords como dataset pronto para treino.\"\"\"\n",
    "        files = tf.io.gfile.glob(path_pattern)\n",
    "        ds = tf.data.TFRecordDataset(files, compression_type=compression)\n",
    "        ds = ds.map(lambda ex: LSTMPreprocessor.parse_tfrecord(ex, seq_len, x_dim, y_dim),\n",
    "                    num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        print(f\"[DATASET] {len(files)} shards carregados ‚Üí batch_size={batch_size}\")\n",
    "        return ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8ece38",
   "metadata": {},
   "source": [
    "# Cap√≠tulo 2 ‚Äî Constru√ß√£o dos Modelos\n",
    "\n",
    "A seguir, definimos construtores simples e eficientes para cada modelo (Linear, LSTM, TFT e TimesFM),\n",
    "prontos para uso em rotinas de otimiza√ß√£o de hiperpar√¢metros (por exemplo, Optuna). Cada construtor\n",
    "recebe um dicion√°rio de par√¢metros (`params`) e retorna um modelo compilado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556d5525",
   "metadata": {},
   "source": [
    "## Constru√ß√£o do Modelo Linear\n",
    "\n",
    "Objetivo: um regressor denso simples (MLP) para prever `target_cols` a partir de `feature_cols`.\n",
    "\n",
    "Contrato r√°pido:\n",
    "- Entrada: vetor de tamanho `x_dim` (n√∫mero de features)\n",
    "- Sa√≠da: vetor de tamanho `y_dim` (n√∫mero de targets)\n",
    "- Par√¢metros (exemplos): hidden_units, activation, dropout, lr, l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "727619dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "def build_lstm_model(seq_len: int, x_dim: int, y_dim: int, params: Dict[str, Any]) -> keras.Model:\n",
    "    \"\"\"\n",
    "    LSTM para regress√£o multivariada temporal com suporte a m√°scara para valores nulos/padding.\n",
    "\n",
    "    - As entradas devem conter `NaN` ou um valor sentinel (ex.: 0.0) para timesteps a mascarar.\n",
    "    - Camadas LSTM automaticamente ignoram esses timesteps durante o treinamento.\n",
    "    \"\"\"\n",
    "\n",
    "    lstm_units = params.get('lstm_units', [128, 64])\n",
    "    dense_units = params.get('dense_units', [128])\n",
    "    dropout = float(params.get('dropout', 0.1))\n",
    "    rec_dropout = float(params.get('rec_dropout', 0.0))\n",
    "    act = params.get('act', 'relu')\n",
    "    lr = float(params.get('lr', 1e-3))\n",
    "    l2 = float(params.get('l2', 0.0))\n",
    "    layer_norm = bool(params.get('layer_norm', True))\n",
    "    mask_value = float(params.get('mask_value', 0.0))  # sentinel for masking\n",
    "\n",
    "    # --- Inputs & Mask ---\n",
    "    inputs = keras.Input(shape=(seq_len, x_dim), name='sequence_input')\n",
    "    # Replace NaNs with mask_value before masking\n",
    "    x = layers.Lambda(lambda v: tf.where(tf.math.is_nan(v), tf.fill(tf.shape(v), mask_value), v))(inputs)\n",
    "    x = layers.Masking(mask_value=mask_value, name='masking')(x)\n",
    "\n",
    "    # --- LSTM stack ---\n",
    "    for i, units in enumerate(lstm_units):\n",
    "        return_seq = i < len(lstm_units) - 1\n",
    "        x = layers.LSTM(\n",
    "            units,\n",
    "            return_sequences=return_seq,\n",
    "            dropout=dropout,\n",
    "            recurrent_dropout=rec_dropout,\n",
    "            kernel_regularizer=keras.regularizers.l2(l2),\n",
    "            name=f'lstm_{i}'\n",
    "        )(x)\n",
    "        if layer_norm:\n",
    "            x = layers.LayerNormalization(name=f'ln_{i}')(x)\n",
    "\n",
    "    # --- Dense layers ---\n",
    "    for i, units in enumerate(dense_units):\n",
    "        x = layers.Dense(units, activation=act, name=f'dense_{i}')(x)\n",
    "        if dropout > 0:\n",
    "            x = layers.Dropout(dropout, name=f'dropout_{i}')(x)\n",
    "\n",
    "    outputs = layers.Dense(y_dim, name='output')(x)\n",
    "\n",
    "    # --- Compile ---\n",
    "    model = keras.Model(inputs, outputs, name='lstm_regressor')\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=lr, clipnorm=1.0)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "    return model\n",
    "def parse_seq_tfrecord(example_proto, seq_len, x_dim, y_dim):\n",
    "    \"\"\"\n",
    "    Faz o parsing de TFRecords com dados 3D salvos em bytes.\n",
    "    Espera features:\n",
    "        'x_raw': sequ√™ncia de entrada (float32 bytes)\n",
    "        'y_raw': target (float32 bytes)\n",
    "    \"\"\"\n",
    "    feature_description = {\n",
    "        'x_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "        'y_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    parsed = tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "    x = tf.io.decode_raw(parsed['x_raw'], tf.float32)\n",
    "    y = tf.io.decode_raw(parsed['y_raw'], tf.float32)\n",
    "\n",
    "    x = tf.reshape(x, [seq_len, x_dim])\n",
    "    y = tf.reshape(y, [y_dim])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def load_seq_tfrecord_dataset(path_pattern, seq_len, x_dim, y_dim, batch_size=64, compression='GZIP'):\n",
    "    \"\"\"\n",
    "    Carrega TFRecords sequenciais e retorna um tf.data.Dataset pronto para treino.\n",
    "\n",
    "    Cada exemplo cont√©m:\n",
    "        X.shape = (seq_len, x_dim)\n",
    "        Y.shape = (y_dim,)\n",
    "    \"\"\"\n",
    "    files = tf.io.gfile.glob(path_pattern)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"Nenhum arquivo TFRecord encontrado em {path_pattern}\")\n",
    "\n",
    "    ds = tf.data.TFRecordDataset(files, compression_type=compression)\n",
    "    ds = ds.map(\n",
    "        lambda ex: parse_seq_tfrecord(ex, seq_len, x_dim, y_dim),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    print(f\"[DATASET] {len(files)} shards carregados | batch_size={batch_size}\")\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afbf736",
   "metadata": {},
   "source": [
    "# Cap√≠tulo 3 - Contru√ß√£o da Pipelines de dados dos modelos\n",
    "\n",
    "Contruir o fluxo de dados, incluindo a o preprocessamento e treinamento dos modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d03c22",
   "metadata": {},
   "source": [
    "## Pipeline dos Modelos Lineares\n",
    "\n",
    "Ser√° gerada uma pipeline completa para cada n√≠vel de pergunta\n",
    "\n",
    "Cada fun√ß√£o ir√° processar os dados para cad problema e fazer o treinamento do modelo\n",
    "\n",
    "Seus outputs ser√£o os modelos treinados, onde os valores ser√£o comparados ao final do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e97fb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "def lstm_pipeline(\n",
    "    data_dir: str,\n",
    "    country_list: List[str],\n",
    "    feature_cols: List[str],\n",
    "    target_cols: List[str],\n",
    "    seq_len: int,\n",
    "    lead: int,\n",
    "    value_cols: List[str],\n",
    "    use_optuna: bool = True,\n",
    "    n_trials: int = 20,\n",
    "    batch_size: int = 128\n",
    ") -> Tuple[LSTMPreprocessor, keras.Model]:\n",
    "    \"\"\"\n",
    "    Pipeline completa de pr√©-processamento, tuning e treinamento de um modelo LSTM.\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------\n",
    "    # Pr√©-processamento\n",
    "    # ----------------------------\n",
    "    preproc = LSTMPreprocessor(\n",
    "        data_dir=data_dir,\n",
    "        model_name='lstm_model',\n",
    "        feature_cols=feature_cols,\n",
    "        target_cols=target_cols,\n",
    "        country_list=country_list,\n",
    "        lag=seq_len,\n",
    "        lead=lead\n",
    "    )\n",
    "\n",
    "    preproc.load_data()\n",
    "    preproc.encode(encode_cols='datetime', encode_method='time_cycle')\n",
    "    preproc.encode(encode_cols='country', encode_method='label')\n",
    "    preproc.split_train_val_test(train_size=0.6, val_size=0.2, test_size=0.2, time_col='datetime')\n",
    "    preproc.normalize_splits(value_cols=value_cols, normalization_method='minmax')\n",
    "\n",
    "    # Constr√≥i janelas e salva TFRecords para cada split\n",
    "    for split_name, split_df in preproc.splits.items():\n",
    "        preproc.df_base = split_df\n",
    "        preproc.build_sequence_matrix(\n",
    "            value_cols=value_cols,\n",
    "            target_cols=target_cols,\n",
    "            seq_len=seq_len,\n",
    "            lead=lead,\n",
    "            group_cols=['country'],\n",
    "            time_col='datetime'\n",
    "        )\n",
    "        preproc.save_sequence_tfrecords(output_basename=f'lstm_dataset_{split_name}', shard_size=1000, compression='GZIP')\n",
    "    print(\"‚úÖ Pr√©-processamento sequencial conclu√≠do.\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # TFRecord datasets\n",
    "    # ----------------------------\n",
    "    meta = preproc._seq_data\n",
    "    x_dim, y_dim = meta['x_dim'], meta['y_dim']\n",
    "\n",
    "    dataset_train = LSTMPreprocessor.load_sequence_dataset(\n",
    "        path_pattern=os.path.join(preproc.save_dir, 'lstm_dataset_train*.tfrecord'),\n",
    "        seq_len=seq_len, x_dim=x_dim, y_dim=y_dim, batch_size=batch_size\n",
    "    )\n",
    "    dataset_val = LSTMPreprocessor.load_sequence_dataset(\n",
    "        path_pattern=os.path.join(preproc.save_dir, 'lstm_dataset_val*.tfrecord'),\n",
    "        seq_len=seq_len, x_dim=x_dim, y_dim=y_dim, batch_size=batch_size\n",
    "    )\n",
    "    print(\"üì¶ Dataset TFRecord carregado para treinamento.\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Optuna hyperparameter search\n",
    "    # ----------------------------\n",
    "    if use_optuna:\n",
    "        def objective(trial):\n",
    "            lstm_layers = trial.suggest_int(\"n_lstm_layers\", 1, 2)\n",
    "            lstm_units = [trial.suggest_int(f\"lstm_u{i}\", 64, 256, step=64) for i in range(lstm_layers)]\n",
    "            dense_units = [trial.suggest_int(\"dense_u\", 64, 256, step=64)]\n",
    "\n",
    "            params = {\n",
    "                \"lstm_units\": lstm_units,\n",
    "                \"dense_units\": dense_units,\n",
    "                \"dropout\": trial.suggest_float(\"dropout\", 0.0, 0.3),\n",
    "                \"rec_dropout\": trial.suggest_float(\"rec_dropout\", 0.0, 0.2),\n",
    "                \"act\": trial.suggest_categorical(\"act\", [\"relu\", \"tanh\", \"gelu\"]),\n",
    "                \"lr\": trial.suggest_float(\"lr\", 1e-4, 3e-3, log=True),\n",
    "                \"l2\": trial.suggest_float(\"l2\", 1e-7, 1e-4, log=True),\n",
    "                \"layer_norm\": trial.suggest_categorical(\"layer_norm\", [True, False])\n",
    "            }\n",
    "\n",
    "            model = build_lstm_model(seq_len=seq_len, x_dim=x_dim, y_dim=y_dim, params=params)\n",
    "            es = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=0)\n",
    "\n",
    "            hist = model.fit(dataset_train, validation_data=dataset_val,\n",
    "                             epochs=60, callbacks=[es], verbose=0)\n",
    "            return min(hist.history['val_loss'])\n",
    "\n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(objective, n_trials=n_trials, n_jobs=-1)\n",
    "        print(\"üéØ Melhores hiperpar√¢metros Optuna:\", study.best_params)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        lstm_layers = best_params.pop(\"n_lstm_layers\")\n",
    "        best_params[\"lstm_units\"] = [best_params.pop(f\"lstm_u{i}\") for i in range(lstm_layers)]\n",
    "        best_params[\"dense_units\"] = [best_params.pop(\"dense_u\")]\n",
    "    else:\n",
    "        best_params = {\n",
    "            \"lstm_units\": [128, 64],\n",
    "            \"dense_units\": [128],\n",
    "            \"dropout\": 0.1,\n",
    "            \"rec_dropout\": 0.0,\n",
    "            \"act\": \"relu\",\n",
    "            \"lr\": 1e-3,\n",
    "            \"l2\": 1e-6,\n",
    "            \"layer_norm\": True\n",
    "        }\n",
    "\n",
    "    # ----------------------------\n",
    "    # Constru√ß√£o e Treinamento Final\n",
    "    # ----------------------------\n",
    "    model = build_lstm_model(seq_len=seq_len, x_dim=x_dim, y_dim=y_dim, params=best_params)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=0)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=6, min_lr=1e-6, verbose=0)\n",
    "\n",
    "    hist = model.fit(\n",
    "        dataset_train,\n",
    "        validation_data=dataset_val,\n",
    "        epochs=100,\n",
    "        callbacks=[early_stopping, reduce_lr]\n",
    "    )\n",
    "\n",
    "    # ----------------------------\n",
    "    # Plot hist√≥rico\n",
    "    # ----------------------------\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(hist.history['loss'], label='train_loss')\n",
    "    plt.plot(hist.history['val_loss'], label='val_loss')\n",
    "    plt.title(\"Treinamento LSTM\")\n",
    "    plt.xlabel(\"√âpoca\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return preproc, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d8e826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Running LSTM pipeline Nivel 1 - A ...\n",
      "[DIVIDIDO] train: 10,357 linhas\n",
      "[DIVIDIDO] val: 3,452 linhas\n",
      "[DIVIDIDO] test: 3,454 linhas\n",
      "[JANELAS] X=(9590, 672, 1), Y=(9590, 1)\n",
      "[‚úÖ] TFRecords salvos (10 shards) em data/N1A\\lstm_model\n",
      "[JANELAS] X=(2685, 672, 1), Y=(2685, 1)\n",
      "[‚úÖ] TFRecords salvos (3 shards) em data/N1A\\lstm_model\n",
      "[JANELAS] X=(2687, 672, 1), Y=(2687, 1)\n",
      "[‚úÖ] TFRecords salvos (3 shards) em data/N1A\\lstm_model\n",
      "‚úÖ Pr√©-processamento sequencial conclu√≠do.\n",
      "[DATASET] 10 shards carregados ‚Üí batch_size=256\n",
      "[DATASET] 3 shards carregados ‚Üí batch_size=256\n",
      "üì¶ Dataset TFRecord carregado para treinamento.\n",
      "WARNING:tensorflow:From c:\\Users\\victo\\OneDrive\\Documentos\\TCC\\TCC-2025\\tfc_venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Epoch 1/100\n",
      "     38/Unknown \u001b[1m154s\u001b[0m 4s/step - loss: 0.2737 - mae: 0.3703"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\OneDrive\\Documentos\\TCC\\TCC-2025\\tfc_venv\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:164: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 4s/step - loss: 0.1130 - mae: 0.2222 - val_loss: 0.1601 - val_mae: 0.3492 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m12/38\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m1:41\u001b[0m 4s/step - loss: 0.0308 - mae: 0.1391"
     ]
    }
   ],
   "source": [
    "import os, gc, tensorflow as tf\n",
    "\n",
    "lstm_pipelines = [\n",
    "    dict(name=\"Nivel 1 - A\", data_dir=\"data/N1A\", feats=[\"country\",\"datetime\",\"quantity_MW\"], tgts=[\"quantity_MW\"], seq_len=7*96, lead=96, vals=[\"quantity_MW\"], countries=[\"ES\"]),\n",
    "    dict(name=\"Nivel 1 - B\", data_dir=\"data/N1B\", feats=[\"country\",\"datetime\",\"quantity_MW\"], tgts=[\"quantity_MW\"], seq_len=15*96, lead=3*96, vals=[\"quantity_MW\"], countries=COUNTRY_DOMAINS.keys()),\n",
    "    dict(name=\"Nivel 1 - C\", data_dir=\"data/N1C\", feats=[\"country\",\"datetime\",\"quantity_MW\"], tgts=[\"quantity_MW\"], seq_len=30*96, lead=7*96, vals=[\"quantity_MW\"], countries=COUNTRY_DOMAINS.keys()),\n",
    "    dict(name=\"Nivel 2 - A\", data_dir=\"data/N2A\", feats=[\"country\",\"datetime\",\"quantity_MW\",\"price_EUR_MWh\"], tgts=[\"quantity_MW\",\"price_EUR_MWh\"], seq_len=7*96, lead=96, vals=[\"quantity_MW\",\"price_EUR_MWh\"], countries=[\"ES\"]),\n",
    "    dict(name=\"Nivel 2 - B\", data_dir=\"data/N2B\", feats=[\"country\",\"datetime\",\"quantity_MW\",\"price_EUR_MWh\"], tgts=[\"quantity_MW\",\"price_EUR_MWh\"], seq_len=15*96, lead=3*96, vals=[\"quantity_MW\",\"price_EUR_MWh\"], countries=COUNTRY_DOMAINS.keys()),\n",
    "    dict(name=\"Nivel 2 - C\", data_dir=\"data/N2C\", feats=[\"country\",\"datetime\",\"quantity_MW\",\"price_EUR_MWh\"], tgts=[\"quantity_MW\",\"price_EUR_MWh\"], seq_len=30*96, lead=7*96, vals=[\"quantity_MW\",\"price_EUR_MWh\"], countries=COUNTRY_DOMAINS.keys())\n",
    "]\n",
    "\n",
    "for cfg in lstm_pipelines:\n",
    "    name = cfg[\"name\"]\n",
    "    print(f\"\\nüöÄ Running LSTM pipeline {name} ...\")\n",
    "\n",
    "    preproc, _ = lstm_pipeline(\n",
    "        data_dir=cfg[\"data_dir\"],\n",
    "        feature_cols=cfg[\"feats\"],\n",
    "        target_cols=cfg[\"tgts\"],\n",
    "        seq_len=cfg[\"seq_len\"],\n",
    "        lead=cfg[\"lead\"],\n",
    "        value_cols=cfg[\"vals\"],\n",
    "        country_list=cfg[\"countries\"],\n",
    "        batch_size=256,\n",
    "        use_optuna=False  # se quiser tuning, mude para True\n",
    "    )\n",
    "\n",
    "    del preproc\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"‚úÖ Finished {name} - memory cleared\\n{'-'*60}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfc_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

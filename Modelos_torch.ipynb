{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c49e9f73",
   "metadata": {},
   "source": [
    "# Introdu√ß√£o\n",
    "\n",
    "Esse Notebook ser√° respons√°vel pelo preprocessamento dos dados contidos em ./data/raw para formatos compat√≠veis e otimizados para o treinamento de cada modelo\n",
    "As defini√ß√µes dos par√¢metros do treinamento aos quais os modelos dever√£o solucionar j√° foram definidas no notebook \"Coleta de Dados\"\n",
    "\n",
    "Modelos a serem Criados:\n",
    "\n",
    "1. Modelo Linear: MLP sem fun√ß√µes de ativa√ß√£o, composta apenas de somas lineares\n",
    "2. MLP: rede neural - efetivamente identica ao modelo linear, no entanto, apresenta fun√ß√£o de ativa√ß√£o ao final do somat√≥rio de fun√ß√µes lineares\n",
    "3. LSTM: Um modelo de rede neural recorrente, com capacidade de diferencia√ß√£o de informa√ß√£o de curto e longo prazo\n",
    "4. TFT: Modelo baseado em LLMs desenvolvido pela microsoft - servir√° como um comparativo mais moderno\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1149ce94",
   "metadata": {},
   "source": [
    "## Depend√™ncias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce23fd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib\n",
    "\n",
    "print(\"üîß Ambiente: PyTorch + Lightning + Forecasting (GPU)\")\n",
    "\n",
    "# ==============================================\n",
    "# 1) INSTALAR PYTORCH (GPU se dispon√≠vel)\n",
    "# ==============================================\n",
    "has_gpu = os.system(\"nvidia-smi > /dev/null 2>&1\") == 0\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"torch={torch.__version__} | cuda={torch.cuda.is_available()}\")\n",
    "except Exception:\n",
    "    print(\"‚öôÔ∏è Instalando PyTorch (CUDA 12.1)...\")\n",
    "\n",
    "    # remove qualquer torch quebrado\n",
    "    os.system(\"pip uninstall -y torch torchvision torchaudio nvidia-*\")\n",
    "\n",
    "    if has_gpu:\n",
    "        os.system(\n",
    "            \"pip install torch torchvision torchaudio \"\n",
    "            \"--index-url https://download.pytorch.org/whl/cu121 --quiet\"\n",
    "        )\n",
    "    else:\n",
    "        os.system(\n",
    "            \"pip install torch torchvision torchaudio \"\n",
    "            \"--index-url https://download.pytorch.org/whl/cpu --quiet\"\n",
    "        )\n",
    "\n",
    "    importlib.invalidate_caches()\n",
    "    import torch\n",
    "    print(f\"‚úîÔ∏è torch={torch.__version__} | cuda={torch.cuda.is_available()}\")\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# 2) INSTALL PYTORCH-LIGHTNING + FORECASTING\n",
    "# ==============================================\n",
    "try:\n",
    "    import lightning\n",
    "    import pytorch_forecasting\n",
    "    print(\"‚úîÔ∏è PyTorch Lightning + Forecasting ok\")\n",
    "except ImportError:\n",
    "    print(\"üì¶ Instalando Lightning + PyTorch Forecasting...\")\n",
    "    os.system(\"pip install lightning pytorch-forecasting --quiet\")\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# 3) INSTALAR PARQUET LIBS\n",
    "# ==============================================\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    print(f\"‚úîÔ∏è PyArrow {pa.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"üì¶ Instalando PyArrow >= 18\")\n",
    "    os.system(\"pip install 'pyarrow>=18' --quiet\")\n",
    "    importlib.invalidate_caches()\n",
    "    import pyarrow as pa\n",
    "    print(f\"‚úîÔ∏è PyArrow {pa.__version__}\")\n",
    "\n",
    "try:\n",
    "    import fastparquet\n",
    "    print(\"‚úîÔ∏è fastparquet dispon√≠vel (opcional)\")\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# 4) BASE LIBS\n",
    "# ==============================================\n",
    "base_libs = [\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"scikit-learn\",\n",
    "    \"optuna\",\n",
    "    \"python-dotenv\",\n",
    "    \"lxml\",\n",
    "    \"pytz\",\n",
    "    \"optuna statsmodels\",\n",
    "    \"optuna-integration\",\n",
    "    \"tensorboard\"\n",
    "]\n",
    "\n",
    "for lib in base_libs:\n",
    "    try:\n",
    "        importlib.import_module(lib)\n",
    "    except ImportError:\n",
    "        print(f\"üì¶ Instalando {lib}...\")\n",
    "        os.system(f\"pip install --quiet {lib}\")\n",
    "\n",
    "print(\"‚úÖ Todas depend√™ncias do ambiente PyTorch est√£o prontas.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cc7e78",
   "metadata": {},
   "source": [
    "## VARI√ÅVEIS NECESS√ÅRIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89d36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# IMPORTS CENTRAIS DO AMBIENTE PYTORCH\n",
    "# ==============================================\n",
    "\n",
    "import os, json, time, gc, concurrent.futures, datetime\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# ----------------------------------------------\n",
    "# PYTORCH / LIGHTNING / FORECASTING\n",
    "# ----------------------------------------------\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import (\n",
    "    EarlyStopping as LGEarlyStopping,\n",
    "    LearningRateMonitor as LGLearningRateMonitor,\n",
    "    ModelCheckpoint as LGModelCheckpoint,\n",
    ")\n",
    "\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import (\n",
    "    optimize_hyperparameters,\n",
    ")\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# ----------------------------------------------\n",
    "# PARQUET / I/O\n",
    "# ----------------------------------------------\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# ----------------------------------------------\n",
    "# LOCAL PREPROCESSORS (VERS√ïES PYTORCH)\n",
    "# ----------------------------------------------\n",
    "from preprocessor_torch import TFTPreprocessor\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Silenciar warnings\n",
    "# ----------------------------------------------\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================================\n",
    "# GPU CONFIG (PyTorch)\n",
    "# ==============================================\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"üöÄ GPU detectada: {device_name}\")\n",
    "    print(\"üü¢ PyTorch est√° usando CUDA\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhuma GPU encontrada ‚Äî PyTorch usando CPU.\")\n",
    "\n",
    "# ==============================================\n",
    "# Configura env (.env)\n",
    "# ==============================================\n",
    "load_dotenv()\n",
    "print(\"‚úîÔ∏è Ambiente PyTorch carregado com sucesso\")\n",
    "\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# CONFIGURA√á√ïES GERAIS DO PROJETO (PyTorch)\n",
    "# ==============================================\n",
    "\n",
    "COUNTRY_DOMAINS = {\n",
    "    \"FR\": {\"domain\": \"10YFR-RTE------C\"},\n",
    "    \"ES\": {\"domain\": \"10YES-REE------0\"},\n",
    "    \"PT\": {\"domain\": \"10YPT-REN------W\"},\n",
    "}\n",
    "\n",
    "DATA_ITEMS = [\n",
    "    {\n",
    "        \"key\": \"load_total\",\n",
    "        \"documentType\": \"A65\",\n",
    "        \"processType\": \"A16\",\n",
    "        \"domainParam\": \"outBiddingZone_Domain\",\n",
    "        \"parser\": \"load\",\n",
    "    },\n",
    "    {\n",
    "        \"key\": \"market_prices\",\n",
    "        \"documentType\": \"A44\",\n",
    "        \"processType\": \"A07\",\n",
    "        \"domainParamIn\": \"in_Domain\",\n",
    "        \"domainParamOut\": \"out_Domain\",\n",
    "        \"parser\": \"price\",\n",
    "    },\n",
    "]\n",
    "\n",
    "ENTSOE_TOKEN = os.environ.get(\"ENTSOE_SECURITY_TOKEN\")\n",
    "BASE_URL = \"https://web-api.tp.entsoe.eu/api\"\n",
    "\n",
    "RAW_DIR = os.path.join(\"data\", \"raw\")\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "\n",
    "PARQUET_COMPRESSION = \"zstd\"\n",
    "MAX_WORKERS = 100\n",
    "EPOCHS = 200\n",
    "\n",
    "# ==============================================\n",
    "# DICION√ÅRIO DE TREINAMENTO\n",
    "# ==============================================\n",
    "treinamento = {\n",
    "    \"name\": \"treinamento\",\n",
    "    \"data_dir\": \"data/treinamento\",\n",
    "    \"feats\": [\"country\", \"datetime\", \"quantity_MW\"],\n",
    "    \"tgts\": [\"quantity_MW\"],\n",
    "    \"vals\": [\"quantity_MW\"],\n",
    "    \"lag\": 10*24,\n",
    "    \"lead\": 3*24,\n",
    "    \"seq_len\": 10*24,\n",
    "    \"countries\": list(COUNTRY_DOMAINS),\n",
    "    \"noise\": False,\n",
    "    \"train\": True,\n",
    "    \"dataset_keep\": [\"train\", \"val\"]\n",
    "}\n",
    "\n",
    "cv = {\n",
    "    \"name\": \"CV\",\n",
    "    \"data_dir\": \"data/CV\",\n",
    "    \"feats\": [\"country\", \"datetime\", \"quantity_MW\"],\n",
    "    \"tgts\": [\"quantity_MW\"],\n",
    "    \"vals\": [\"quantity_MW\"],\n",
    "    \"lag\": 10*24,\n",
    "    \"lead\": 3*24,\n",
    "    \"seq_len\": 10*24,\n",
    "    \"countries\": list(COUNTRY_DOMAINS),\n",
    "    \"noise\": False,\n",
    "    \"size\": 0.2,\n",
    "    \"dataset_keep\": [\"train\", \"val\"]\n",
    "}\n",
    "\n",
    "perguntas = [\n",
    "    {\n",
    "        \"name\": \"N1A\",\n",
    "        \"data_dir\": \"data/N1A\",\n",
    "        \"feats\": [\"country\", \"datetime\", \"quantity_MW\"],\n",
    "        \"tgts\": [\"quantity_MW\"],\n",
    "        \"vals\": [\"quantity_MW\"],\n",
    "        \"lag\": 10 * 24,\n",
    "        \"lead\": 3 * 24,\n",
    "        \"seq_len\": 3 * 24,\n",
    "        \"countries\": [\"ES\"],\n",
    "        \"noise\": False,\n",
    "        \"dataset_keep\": [\"test\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"N1B\",\n",
    "        \"data_dir\": \"data/N1B\",\n",
    "        \"feats\": [\"country\", \"datetime\", \"quantity_MW\"],\n",
    "        \"tgts\": [\"quantity_MW\"],\n",
    "        \"vals\": [\"quantity_MW\"],\n",
    "        \"lag\": 10 * 24,\n",
    "        \"lead\": 3 * 24,\n",
    "        \"seq_len\": 7 * 24,\n",
    "        \"countries\": [\"ES\"],\n",
    "        \"noise\": False,\n",
    "        \"dataset_keep\": [\"test\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"N1C\",\n",
    "        \"data_dir\": \"data/N1C\",\n",
    "        \"feats\": [\"country\", \"datetime\", \"quantity_MW\"],\n",
    "        \"tgts\": [\"quantity_MW\"],\n",
    "        \"vals\": [\"quantity_MW\"],\n",
    "        \"lag\": 10 * 24,\n",
    "        \"lead\": 3 * 24,\n",
    "        \"seq_len\": 10 * 24,\n",
    "        \"countries\": [\"ES\"],\n",
    "        \"noise\": False,\n",
    "        \"dataset_keep\": [\"test\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"N2A\",\n",
    "        \"data_dir\": \"data/N2A\",\n",
    "        \"feats\": [\"country\", \"datetime\", \"quantity_MW\"],\n",
    "        \"tgts\": [\"quantity_MW\"],\n",
    "        \"vals\": [\"quantity_MW\"],\n",
    "        \"lag\": 10 * 24,\n",
    "        \"lead\": 3 * 24,\n",
    "        \"seq_len\": 3 * 24,\n",
    "        \"countries\": COUNTRY_DOMAINS.keys(),\n",
    "        \"noise\": False,\n",
    "        \"dataset_keep\": [\"test\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"N2B\",\n",
    "        \"data_dir\": \"data/N2B\",\n",
    "        \"feats\": [\"country\", \"datetime\", \"quantity_MW\"],\n",
    "        \"tgts\": [\"quantity_MW\"],\n",
    "        \"vals\": [\"quantity_MW\"],\n",
    "        \"lag\": 10 * 24,\n",
    "        \"lead\": 3 * 24,\n",
    "        \"seq_len\": 7 * 24,\n",
    "        \"countries\": COUNTRY_DOMAINS.keys(),\n",
    "        \"noise\": False,\n",
    "        \"dataset_keep\": [\"test\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"N2C\",\n",
    "        \"data_dir\": \"data/N2C\",\n",
    "        \"feats\": [\"country\", \"datetime\", \"quantity_MW\"],\n",
    "        \"tgts\": [\"quantity_MW\"],\n",
    "        \"vals\": [\"quantity_MW\"],\n",
    "        \"lag\": 10 * 24,\n",
    "        \"lead\": 3 * 24,\n",
    "        \"seq_len\": 10 * 24,\n",
    "        \"countries\": COUNTRY_DOMAINS.keys(),\n",
    "        \"noise\": False,\n",
    "        \"dataset_keep\": [\"test\"]\n",
    "    },\n",
    "        {\n",
    "        \"name\": \"N3A\",\n",
    "        \"data_dir\": \"data/N3A\",\n",
    "        \"feats\": [\"country\", \"datetime\", \"quantity_MW\"],\n",
    "        \"tgts\": [\"quantity_MW\"],\n",
    "        \"vals\": [\"quantity_MW\"],\n",
    "        \"lag\": 10 * 24,\n",
    "        \"lead\": 3 * 24,\n",
    "        \"seq_len\": 3 * 24,\n",
    "        \"countries\": COUNTRY_DOMAINS.keys(),\n",
    "        \"noise\": True,\n",
    "        \"dataset_keep\": [\"test\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"N3B\",\n",
    "        \"data_dir\": \"data/N3B\",\n",
    "        \"feats\": [\"country\", \"datetime\", \"quantity_MW\"],\n",
    "        \"tgts\": [\"quantity_MW\"],\n",
    "        \"vals\": [\"quantity_MW\"],\n",
    "        \"lag\": 10 * 24,\n",
    "        \"lead\": 3 * 24,\n",
    "        \"seq_len\": 7 * 24,\n",
    "        \"countries\": COUNTRY_DOMAINS.keys(),\n",
    "        \"noise\": True,\n",
    "        \"dataset_keep\": [\"test\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"N3C\",\n",
    "        \"data_dir\": \"data/N3C\",\n",
    "        \"feats\": [\"country\", \"datetime\", \"quantity_MW\"],\n",
    "        \"tgts\": [\"quantity_MW\"],\n",
    "        \"vals\": [\"quantity_MW\"],\n",
    "        \"lag\": 10 * 24,\n",
    "        \"lead\": 3 * 24,\n",
    "        \"seq_len\": 10 * 24,\n",
    "        \"countries\": COUNTRY_DOMAINS.keys(),\n",
    "        \"noise\": True,\n",
    "        \"dataset_keep\": [\"test\"]\n",
    "    }\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5b47ef",
   "metadata": {},
   "source": [
    "# Cap√≠tulo 1: Preprocessamento de dados\n",
    "\n",
    "Etapa de contru√ß√£o da pipelines de pre-processamento de dados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd309dd",
   "metadata": {},
   "source": [
    "## Preprocessamento do Modelo TFT (PyTorch)\n",
    "\n",
    "No caso a etapa de preprocessamento do modelo TFT √© inexistente, j√° que, diferentemente dos modelos em tensorflow/keras, a biblioteca do pytorch e mais especificament do TemporalFusionTransformer j√° lida com o pr√©processamento dos dados, necessitando apenas de uma separa√ß√£ em dataset de treinamento, valida√ß√£o e teste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8ece38",
   "metadata": {},
   "source": [
    "# Cap√≠tulo 2 ‚Äî Constru√ß√£o dos Modelos\n",
    "\n",
    "A seguir, definimos construtores simples e eficientes para cada modelo (Linear, LSTM, TFT e TimesFM),\n",
    "prontos para uso em rotinas de otimiza√ß√£o de hiperpar√¢metros (por exemplo, Optuna). Cada construtor\n",
    "recebe um dicion√°rio de par√¢metros (`params`) e retorna um modelo compilado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b72a517",
   "metadata": {},
   "source": [
    "## Constru√ß√£o do Modelo TFT (Temporal Fusion Transformer)\n",
    "\n",
    "**Objetivo:** prever `target_cols` a partir de `feature_cols` utilizando a implementa√ß√£o oficial `keras_tft`,  \n",
    "que integra **sele√ß√£o de vari√°veis din√¢micas**, **blocos LSTM**, **aten√ß√£o temporal multi-cabe√ßas** e **gating residual** em um √∫nico modelo interpretable.\n",
    "\n",
    "**Contrato r√°pido:**\n",
    "- **Entrada:** `tf.data.Dataset` com tensores no formato `(batch, seq_len, x_dim)`  \n",
    "- **Sa√≠da:** tensor cont√≠nuo de tamanho `y_dim` *(ou `dec_len √ó y_dim` para horizontes m√∫ltiplos)*\n",
    "\n",
    "**Par√¢metros (exemplos):**  \n",
    "`hidden_size` (tamanho interno das camadas GRN) ¬∑ `lstm_layers` ¬∑ `num_heads` (aten√ß√£o) ¬∑ `dropout` ¬∑ `learning_rate` ¬∑ `output_size` ¬∑ `seq_len`\n",
    "\n",
    "**Componentes internos (`keras_tft`):**  \n",
    "Variable Selection Network ‚Üí LSTM Encoder/Decoder ‚Üí Multi-Head Temporal Attention ‚Üí Gated Residual Network ‚Üí Camada de proje√ß√£o final\n",
    "\n",
    "**Compatibilidade:**  \n",
    "Totalmente compat√≠vel com o pipeline atual em Parquet do LSTM, recebendo o mesmo formato de dados  \n",
    "(`(batch, seq_len, features)`), permitindo substitui√ß√£o direta do modelo sem alterar o pr√©-processamento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce31bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFT model builder (imports centralizados na c√©lula 5)\n",
    "\n",
    "def build_tft_model(\n",
    "    params: Dict[str, Any]\n",
    "):\n",
    "    \"\"\"\n",
    "    Constr√≥i um Temporal Fusion Transformer (TFT) com par√¢metros configur√°veis.\n",
    "\n",
    "    Args:\n",
    "        x_dim: n√∫mero de features de entrada\n",
    "        y_dim: n√∫mero de targets\n",
    "        seq_len: tamanho da sequ√™ncia temporal\n",
    "        params: dicion√°rio de hiperpar√¢metros (hidden_size, dropout, lstm_layers, etc.)\n",
    "        max_encoder_length: tamanho da janela passada (encoder)\n",
    "        max_prediction_length: tamanho do horizonte de previs√£o (decoder)\n",
    "    \"\"\"\n",
    "\n",
    "    hidden_size = int(params.get(\"hidden_size\", 64))\n",
    "    dropout = float(params.get(\"dropout\", 0.1))\n",
    "    lstm_layers = int(params.get(\"lstm_layers\", 1))\n",
    "    attention_head_size = int(params.get(\"num_heads\", 4))\n",
    "    lr = float(params.get(\"lr\", 1e-3))\n",
    "\n",
    "    model = TemporalFusionTransformer.from_dataset(\n",
    "        params[\"dataset\"],  # dataset preparado via TimeSeriesDataSet\n",
    "        learning_rate=lr,\n",
    "        hidden_size=hidden_size,\n",
    "        dropout=dropout,\n",
    "        lstm_layers=lstm_layers,\n",
    "        attention_head_size=attention_head_size,\n",
    "        loss=QuantileLoss(),\n",
    "        log_interval=10,\n",
    "        log_val_interval=1\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afbf736",
   "metadata": {},
   "source": [
    "# Cap√≠tulo 3 - Contru√ß√£o da Pipelines de dados dos modelos\n",
    "\n",
    "A fun√ß√£o de pipeline organiza o fluxo de dados para, de forma mais concisa e organizada, treinar o modelo, sendo capaz de mostrar a progress√£o das perdas a medida que as √©pocas de treinamento passam - Esse display est√© dispon√≠vel no notebook \"Resultados\"\n",
    "\n",
    "O resultado da pipeline √© um gr√°fico com a evolu√ß√£o de todas as m√©tricas e o salvamento do modelo treinado dentro da pasta ./modelo/{Nome_Problema}/{Nome_Modelo}\n",
    "\n",
    "Assim podendo ser facilmente reutilizado futuramente para um notebook comparativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77058c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilit√°rio para salvar modelos Keras (imports centralizados na c√©lula 5)\n",
    "\n",
    "def save_model(model, path: str, format: str | None = None, include_optimizer: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Salva um modelo Keras em formato padronizado.\n",
    "\n",
    "    Regras:\n",
    "    - Se `path` terminar com .keras ou .h5, salva exatamente nesse arquivo.\n",
    "    - Se `format == 'savedmodel'`, salva no diret√≥rio indicado (SavedModel).\n",
    "    - Caso contr√°rio, adiciona sufixo .keras a `path` (arquivo √∫nico Keras v3).\n",
    "\n",
    "    Retorna o caminho final salvo (arquivo ou diret√≥rio) e grava um meta.json ao lado.\n",
    "    \"\"\"\n",
    "    # Infer√™ncia de formato por extens√£o\n",
    "    ext = None\n",
    "    lower = path.lower()\n",
    "    if lower.endswith(\".keras\"):\n",
    "        ext = \"keras\"\n",
    "    elif lower.endswith(\".h5\") or lower.endswith(\".hdf5\"):\n",
    "        ext = \"h5\"\n",
    "\n",
    "    # Normaliza√ß√£o de destino\n",
    "    if format == \"savedmodel\":\n",
    "        # Diret√≥rio SavedModel\n",
    "        save_dir = path\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        model.save(save_dir, include_optimizer=include_optimizer)\n",
    "        meta_path = os.path.join(save_dir, \"model.meta.json\")\n",
    "        final_path = save_dir\n",
    "    else:\n",
    "        if ext is None:\n",
    "            # For√ßa arquivo .keras por padr√£o\n",
    "            path = f\"{path}.keras\"\n",
    "            ext = \"keras\"\n",
    "        # Cria diret√≥rio pai\n",
    "        parent = os.path.dirname(path) or \".\"\n",
    "        os.makedirs(parent, exist_ok=True)\n",
    "        # Salva arquivo √∫nico\n",
    "        model.save(path, include_optimizer=include_optimizer)\n",
    "        meta_path = f\"{path}.meta.json\"\n",
    "        final_path = path\n",
    "\n",
    "    # Meta b√°sico ao lado do artefato\n",
    "    try:\n",
    "        meta = {\n",
    "            \"saved_at\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
    "            \"keras_version\": getattr(model, \"keras_version\", None),\n",
    "            \"model_name\": getattr(model, \"name\", None),\n",
    "            \"trainable_params\": int(getattr(model, \"count_params\", lambda: 0)()),\n",
    "            \"format\": \"savedmodel\" if format == \"savedmodel\" else ext,\n",
    "        }\n",
    "        with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Falha ao escrever meta.json: {e}\")\n",
    "\n",
    "    print(f\"[üíæ] Modelo salvo em: {final_path}\")\n",
    "    return final_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be66287c",
   "metadata": {},
   "source": [
    "## Pipelines dos modelos TFT\n",
    "Pr√©-processamento em parquet e treino com PyTorch Forecasting (Temporal Fusion Transformer) via Lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e31312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines dos modelos TFT (imports centralizados na c√©lula 5)\n",
    "\n",
    "def tft_preproccess_pipeline(\n",
    "    preproc: TFTPreprocessor,\n",
    "    destino_dir: str,\n",
    "    save_instance: bool = True,\n",
    "    train: bool = False,\n",
    "    size: Optional[float] = 1.0,\n",
    "    noise: bool = False,\n",
    "    dataset_keep: List[str] = ['train', 'val', 'test']\n",
    ") -> Tuple[TFTPreprocessor, Dict[str, Any]]:\n",
    "\n",
    "    print(\"üîÑ Carregando dados brutos ...\")\n",
    "    preproc.load_data(size=size, noise=noise)\n",
    "\n",
    "    preproc.encode(encode_cols='datetime', encode_method='time_parts')\n",
    "\n",
    "    print(\"‚úÇÔ∏è Split train/val/test ...\")\n",
    "    preproc.split_train_val_test(train_size=0.8, val_size=0.15, test_size=0.05, time_col='datetime', dataset_keep=dataset_keep)\n",
    "\n",
    "    print(\"üß± Construindo parquets para TFT ...\")\n",
    "    preproc.build_tft_parquets()\n",
    "\n",
    "    if save_instance and train:\n",
    "        path = os.path.join(destino_dir, \"preprocessor\")\n",
    "        preproc.save_instance(path, name=\"tft_preproc.pkl\")\n",
    "\n",
    "    print(\"üì¶ Parquets TFT salvos. Carregando dataset ...\")\n",
    "\n",
    "    return preproc\n",
    "\n",
    "\n",
    "\n",
    "from lightning.pytorch import Trainer, seed_everything, Callback\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import Logger\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Minimal dummy logger (satisfies callbacks, no files)\n",
    "# ----------------------------------------------------\n",
    "class DummyLogger(Logger):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._version = 0\n",
    "    @property\n",
    "    def name(self): return \"dummy\"\n",
    "    @property\n",
    "    def version(self): return self._version\n",
    "    def log_metrics(self, metrics, step=None): pass\n",
    "    def log_hyperparams(self, params): pass\n",
    "    def experiment(self): return None\n",
    "\n",
    "\n",
    "def tft_train_pipeline(\n",
    "    problem_name: str,\n",
    "    data_dir: str,\n",
    "    feature_cols: List[str],\n",
    "    target_cols: List[str],\n",
    "    seq_len: int,\n",
    "    lead: int,\n",
    "    batch_size: int = 128,\n",
    "    configs: Dict[str, Dict[str, Any]] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Treina modelos TFT (Temporal Fusion Transformer) com PyTorch Lightning.\n",
    "    Retorna (models, histories) ‚Äî histories[name].history['loss'] / ['val_loss'] como em Keras.\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------\n",
    "    # 1. Carregar dados\n",
    "    # ----------------------------\n",
    "    preproc = TFTPreprocessor(\n",
    "        data_dir=data_dir,\n",
    "        model_name=\"tft_model\",\n",
    "        feature_cols=feature_cols,\n",
    "        target_cols=target_cols,\n",
    "        lag=seq_len,\n",
    "        lead=lead,\n",
    "        country_list=[],\n",
    "    )\n",
    "\n",
    "    df_train = preproc.load_tft_dataset(\"train\")\n",
    "    df_val   = preproc.load_tft_dataset(\"val\")\n",
    "\n",
    "    train_loader = df_train.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "    val_loader   = df_val.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\n",
    "    print(f\"üì¶ Dados TFT ‚Äî batches: train={len(train_loader)} | val={len(val_loader)}\")\n",
    "\n",
    "    models, histories = {}, {}\n",
    "    seed_everything(42)\n",
    "    accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # ----------------------------\n",
    "    # 2. Custom loss collector\n",
    "    # ----------------------------\n",
    "    class LossHistoryCallback(Callback):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.history = {\"loss\": [], \"val_loss\": []}\n",
    "\n",
    "        def on_train_epoch_end(self, trainer, pl_module):\n",
    "            metrics = trainer.callback_metrics\n",
    "            if \"train_loss\" in metrics:\n",
    "                self.history[\"loss\"].append(float(metrics[\"train_loss\"]))\n",
    "            elif \"loss\" in metrics:\n",
    "                self.history[\"loss\"].append(float(metrics[\"loss\"]))\n",
    "\n",
    "        def on_validation_epoch_end(self, trainer, pl_module):\n",
    "            metrics = trainer.callback_metrics\n",
    "            if \"val_loss\" in metrics:\n",
    "                self.history[\"val_loss\"].append(float(metrics[\"val_loss\"]))\n",
    "\n",
    "    # ----------------------------\n",
    "    # 3. Loop de presets\n",
    "    # ----------------------------\n",
    "    for name, params in (configs or {}).items():\n",
    "        print(f\"\\nüöÄ Treinando TFT preset: {name} [{accelerator}]\")\n",
    "\n",
    "        model = build_tft_model(params={**params, \"dataset\": df_train})\n",
    "\n",
    "        save_dir = os.path.join(\"modelos\", problem_name, \"TFT\", name)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        loss_collector = LossHistoryCallback()\n",
    "\n",
    "        callbacks = [\n",
    "            loss_collector,\n",
    "            EarlyStopping(monitor=\"val_loss\", patience=int(params.get(\"patience\", 10)), mode=\"min\"),\n",
    "            LearningRateMonitor(logging_interval=\"epoch\"),\n",
    "            ModelCheckpoint(\n",
    "                dirpath=save_dir,\n",
    "                filename=\"best\",\n",
    "                monitor=\"val_loss\",\n",
    "                save_top_k=1,\n",
    "                mode=\"min\",\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        # ‚úÖ Use DummyLogger to satisfy LRMonitor\n",
    "        trainer = Trainer(\n",
    "            max_epochs=int(params.get(\"epochs\", 50)),\n",
    "            accelerator=accelerator,\n",
    "            devices=1,\n",
    "            callbacks=callbacks,\n",
    "            default_root_dir=save_dir,\n",
    "            log_every_n_steps=10,\n",
    "            logger=DummyLogger(),   # <‚Äî in-memory, silent\n",
    "            precision=32,\n",
    "        )\n",
    "\n",
    "        # ----------------------------\n",
    "        # 4. Treinamento\n",
    "        # ----------------------------\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        print(f\"‚úÖ {name} conclu√≠do ‚Äî melhor checkpoint salvo em {save_dir}\")\n",
    "\n",
    "        # ----------------------------\n",
    "        # 5. Hist√≥rico tipo Keras\n",
    "        # ----------------------------\n",
    "        class HistoryLike:\n",
    "            def __init__(self, hist): self.history = hist\n",
    "\n",
    "        history = HistoryLike(loss_collector.history)\n",
    "\n",
    "        print(f\"\\nüìä Hist√≥rico ({name}) ‚Äî √∫ltimas perdas:\")\n",
    "        print(\"train_loss:\", history.history[\"loss\"][-5:])\n",
    "        print(\"val_loss:\", history.history[\"val_loss\"][-5:])\n",
    "\n",
    "        models[name] = model\n",
    "        histories[name] = history\n",
    "\n",
    "    return models, histories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af6da91",
   "metadata": {},
   "source": [
    "# Cap√≠tulo 4: Preprocessamento de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec8a927",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import optuna\n",
    "except ImportError:\n",
    "    optuna = None\n",
    "    print(\"[WARN] Optuna n√£o instalado. Instale com pip install optuna para habilitar HPO.\")\n",
    "\n",
    "full_configs = perguntas.copy()\n",
    "full_configs.append(cv)  # lista de problemas adicionais  e dataset reduzido para CV (N1A.. etc.)\n",
    "\n",
    "# =============================\n",
    "# Preprocessamento base para treinamento (cfg 'treinamento')\n",
    "# =============================\n",
    "preprocess_collector = {}\n",
    "\n",
    "def run_preprocessing(cfg: Dict[str, Any],\n",
    "                      preproc_train_tft: Optional[TFTPreprocessor] = None):\n",
    "    \"\"\"Executa sequencialmente as tr√™s pipelines gerando datasets parquet.\n",
    "    Retorna inst√¢ncias dos preprocessadores com splits salvos.\n",
    "    \"\"\"\n",
    "    destino_dir = cfg[\"data_dir\"]\n",
    "    lag = cfg.get(\"lag\", 24)\n",
    "    lead = cfg.get(\"lead\", 24)\n",
    "    seq_len = cfg.get(\"seq_len\", lag)\n",
    "\n",
    "    preproc_tft = TFTPreprocessor(\n",
    "        model_name=\"TFT\",\n",
    "        lag=seq_len,\n",
    "        lead=lead,\n",
    "        country_list=cfg[\"countries\"],\n",
    "        feature_cols=cfg[\"feats\"],\n",
    "        target_cols=cfg[\"tgts\"],\n",
    "        data_dir=destino_dir,\n",
    "    )\n",
    "    # Ajusta num_cols se n√£o fornecido explicitamente\n",
    "    if not preproc_tft.num_cols and cfg.get(\"vals\"):\n",
    "        preproc_tft.num_cols = list(cfg[\"vals\"])  # compatibilidade\n",
    "\n",
    "    # Herdando encoders / normalizadores se fornecidos\n",
    "    for src, dst in [\n",
    "        (preproc_train_tft, preproc_tft),\n",
    "    ]:\n",
    "        if src:\n",
    "            try:\n",
    "                dst.encod_objects = src.encod_objects\n",
    "                dst.norm_objects = src.norm_objects\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    print(\"==== PIPELINE TFT ====\")\n",
    "    preproc_tft = tft_preproccess_pipeline(\n",
    "        preproc=preproc_tft,\n",
    "        destino_dir=destino_dir,\n",
    "        save_instance=True,\n",
    "        train=cfg.get(\"train\", False),\n",
    "        size=cfg.get(\"size\", 1.0),\n",
    "        noise=cfg.get(\"noise\", False),\n",
    "        dataset_keep=cfg.get(\"dataset_keep\", ['train', 'val', 'test'])\n",
    "    )\n",
    "\n",
    "    preprocess_collector[cfg[\"name\"]] = {\n",
    "        \"tft\": preproc_tft,\n",
    "    }\n",
    "\n",
    "    return preproc_tft\n",
    "\n",
    "\n",
    "## Processamento individual de dataset completo, para setting de par√¢metros de encodding e decoding\n",
    "train_preproc_tft = run_preprocessing(treinamento)\n",
    "\n",
    "## Paraleliza√ß√£o da gera√ß√£o de dataset de Perguntas N1A -> N3C\n",
    "max_workers = max(len(full_configs), 4)\n",
    "\n",
    "print(f\"üöÄ Executando {len(full_configs)} tarefas em {max_workers} workers...\")\n",
    "\n",
    "results = [None] * len(full_configs)\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = {executor.submit(run_preprocessing, \n",
    "        item,\n",
    "        preproc_train_tft=train_preproc_tft): \n",
    "        idx for idx, item in enumerate(full_configs)}\n",
    "    \n",
    "    for i, future in enumerate(as_completed(futures)):\n",
    "        idx = futures[future]\n",
    "        try:\n",
    "            results[idx] = future.result()\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] item {idx}: {e}\")\n",
    "            results[idx] = None\n",
    "        if (i + 1) % max(1, len(full_configs)//10) == 0:\n",
    "            print(f\"  Progresso: {i+1}/{len(full_configs)}\")\n",
    "\n",
    "print(\"‚úÖ Tarefa conclu√≠da.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f14373a",
   "metadata": {},
   "source": [
    "# Cap√≠tulo 5: HPO (Optuna) para TFT usando o dataset CV\n",
    "\n",
    "Nesta se√ß√£o, otimizamos hiperpar√¢metros do Temporal Fusion Transformer com `optimize_hyperparameters` do PyTorch Forecasting.\n",
    "- Dados: split `train`/`val` do problema `cv`\n",
    "- Sa√≠da: melhor conjunto de hiperpar√¢metros salvo em `resultados/hparams/tft_cv_best.json` e estudo completo em `resultados/hparams/tft_cv_study.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d441da94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# PRESETS DE TREINAMENTO TFT (PyTorch Forecasting)\n",
    "# ==============================================\n",
    "config_tft = {\n",
    "    \"tft\": {\n",
    "        \"hidden_size\": 8,\n",
    "        \"lstm_layers\": 1,\n",
    "        \"num_heads\": 2,\n",
    "        \"dropout\": 0,\n",
    "        \"hidden_continuous_size\": 4,\n",
    "        \"attention_head_size\": 1,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"patience\": 20,\n",
    "        \"epochs\": 200,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8751a41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constru√ß√£o de DataLoaders (CV) e rotina de HPO com Optuna\n",
    "import os, json, pickle\n",
    "from datetime import datetime\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "from preprocessor_torch import TFTPreprocessor\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "# Helper: cria dataloaders de train/val para o problema CV\n",
    "\n",
    "def build_cv_dataloaders(batch_size: int = 256):\n",
    "    data_dir = cv[\"data_dir\"]\n",
    "    feature_cols = cv.get(\"feats\") or cv.get(\"feature_cols\")\n",
    "    target_cols = cv.get(\"tgts\") or cv.get(\"target_cols\")\n",
    "    seq_len = cv.get(\"lag\")\n",
    "    lead = cv.get(\"lead\")\n",
    "    countries = list(cv.get(\"countries\", []))\n",
    "\n",
    "    preproc = TFTPreprocessor(\n",
    "        model_name=\"TFT\",\n",
    "        lag=seq_len,\n",
    "        lead=lead,\n",
    "        country_list=countries,\n",
    "        feature_cols=feature_cols,\n",
    "        target_cols=target_cols,\n",
    "        data_dir=data_dir,\n",
    "    )\n",
    "\n",
    "    train_ds = preproc.load_tft_dataset(\"train\")\n",
    "    val_ds   = preproc.load_tft_dataset(\"val\")\n",
    "\n",
    "    train_loader = train_ds.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "    val_loader   = val_ds.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\n",
    "    return preproc, train_loader, val_loader\n",
    "\n",
    "\n",
    "# Executa HPO e salva melhores hiperpar√¢metros\n",
    "\n",
    "def run_tft_optuna_hpo_cv(\n",
    "    n_trials: int = 200,\n",
    "    max_epochs: int = 50,\n",
    "    batch_size: int = 256,\n",
    "    save_dir: str = os.path.join(\"resultados\", \"hparams\"),\n",
    "):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    preproc, train_dataloader, val_dataloader = build_cv_dataloaders(batch_size=batch_size)\n",
    "\n",
    "    study = optimize_hyperparameters(\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        model_path=os.path.join(save_dir, \"tft_cv_optuna\"),\n",
    "        n_trials=n_trials,\n",
    "        max_epochs=max_epochs,\n",
    "        gradient_clip_val_range=(0.01, 1.0),\n",
    "        hidden_size_range=(8, 128),\n",
    "        hidden_continuous_size_range=(8, 128),\n",
    "        attention_head_size_range=(1, 4),\n",
    "        learning_rate_range=(1e-4, 1e-2),\n",
    "        dropout_range=(0.0, 0.4),\n",
    "        trainer_kwargs=dict(limit_train_batches=30),\n",
    "        reduce_on_plateau_patience=4,\n",
    "        use_learning_rate_finder=False,\n",
    "    )\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Persist√™ncia do estudo (pkl) e dos melhores params (json)\n",
    "    study_pkl = os.path.join(save_dir, \"tft_cv_study.pkl\")\n",
    "    with open(study_pkl, \"wb\") as fout:\n",
    "        pickle.dump(study, fout)\n",
    "\n",
    "    best = study.best_trial.params\n",
    "    # Pequenas normaliza√ß√µes de chaves para nosso builder/config\n",
    "    # - attention_head_size -> num_heads (compat)\n",
    "    best_out = dict(best)\n",
    "    if \"attention_head_size\" in best_out and \"num_heads\" not in best_out:\n",
    "        best_out[\"num_heads\"] = int(best_out[\"attention_head_size\"])\n",
    "    # - learning_rate -> lr (compat com build_tft_model)\n",
    "    if \"learning_rate\" in best_out:\n",
    "        best_out[\"lr\"] = float(best_out[\"learning_rate\"])\n",
    "\n",
    "    best_json = {\n",
    "        \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"dataset\": \"cv\",\n",
    "        \"seq_len\": int(cv.get(\"lag\")),\n",
    "        \"lead\": int(cv.get(\"lead\")),\n",
    "        \"search_space\": {\n",
    "            \"gradient_clip_val_range\": [0.01, 1.0],\n",
    "            \"hidden_size_range\": [8, 128],\n",
    "            \"hidden_continuous_size_range\": [8, 128],\n",
    "            \"attention_head_size_range\": [1, 4],\n",
    "            \"learning_rate_range\": [1e-4, 1e-2],\n",
    "            \"dropout_range\": [0.0, 0.4],\n",
    "            \"trainer_kwargs.limit_train_batches\": 20,\n",
    "            \"reduce_on_plateau_patience\": 4,\n",
    "            \"max_epochs\": max_epochs,\n",
    "            \"n_trials\": n_trials,\n",
    "        },\n",
    "        \"best_params\": best_out,\n",
    "    }\n",
    "\n",
    "    best_path = os.path.join(save_dir, \"tft_cv_best.json\")\n",
    "    with open(best_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(best_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"Melhores hiperpar√¢metros (normalizados):\\n\", json.dumps(best_out, indent=2))\n",
    "    print(f\"\\nüíæ Salvos: {best_path} e {study_pkl}\")\n",
    "\n",
    "    return best_out, best_path\n",
    "\n",
    "\n",
    "# Exemplo de execu√ß√£o (descomente para rodar)\n",
    "best, best_path = run_tft_optuna_hpo_cv(n_trials=15, max_epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab5cc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura do JSON de melhores hiperpar√¢metros e integra√ß√£o ao config\n",
    "import json, os\n",
    "\n",
    "BEST_JSON_PATH = os.path.join(\"resultados\", \"hparams\", \"tft_cv_best.json\")\n",
    "\n",
    "if os.path.exists(BEST_JSON_PATH):\n",
    "    with open(BEST_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        best_info = json.load(f)\n",
    "    best_params = best_info.get(\"best_params\", {})\n",
    "\n",
    "    # Atualiza config_tft[\"tft\"] preservando chaves existentes\n",
    "    cfg_tft = config_tft.get(\"tft\", {})\n",
    "    cfg_tft.update(best_params)\n",
    "\n",
    "    # Garantir compatibilidade com nosso builder\n",
    "    if \"learning_rate\" in cfg_tft:\n",
    "        cfg_tft[\"lr\"] = float(cfg_tft[\"learning_rate\"])  # usado por build_tft_model\n",
    "    if \"attention_head_size\" in cfg_tft and \"num_heads\" not in cfg_tft:\n",
    "        cfg_tft[\"num_heads\"] = int(cfg_tft[\"attention_head_size\"])  # nosso builder usa num_heads\n",
    "\n",
    "    config_tft[\"tft\"] = cfg_tft\n",
    "    print(\"‚úîÔ∏è config_tft['tft'] atualizado com melhores hiperpar√¢metros:\")\n",
    "    print(json.dumps(config_tft[\"tft\"], indent=2))\n",
    "else:\n",
    "    print(f\"[INFO] Arquivo n√£o encontrado: {BEST_JSON_PATH}. Execute a HPO antes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e510a9f",
   "metadata": {},
   "source": [
    "# Cap√≠tulo 6: Treinamento dos modelos\n",
    "Este cap√≠tulo executa, por problema: Linear/MLP (configs_linear), MLP (configs_mlp), LSTM (configs_lstm) e TFT (config_tft), liberando mem√≥ria entre execu√ß√µes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2222b57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento sequencial dos modelos (imports centralizados na c√©lula 5)\n",
    "## Gr√°fico de hist√≥rico\n",
    "def plot_training_history(history, title):\n",
    "    \"\"\"Plot training vs validation loss.\"\"\"\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2, linestyle='--')\n",
    "    plt.title('Training vs Validation Loss do modelo ' + title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=':', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "tempo_treino = {}\n",
    "\n",
    "# Carrega configura√ß√£o de treinamento\n",
    "cfg = treinamento\n",
    "\n",
    "if not cfg:\n",
    "    print(\"sem configura√ß√£o de 'treinamento' configurada na lista de treinamento\")\n",
    "else:\n",
    "    histories = {}\n",
    "    name = cfg[\"name\"]\n",
    "    print(f\"\\nüöÄ Iniciando treinamento dos modelos ...\")\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"Modelo TFT...\")\n",
    "    # # Treinamento TFT (Temporal Fusion Transformer)\n",
    "    try:\n",
    "        tempo_treino[\"tft\"] = {}\n",
    "        tempo_treino[\"tft\"][\"inicio\"] = time.time()\n",
    "        models_tft, _ = tft_train_pipeline(\n",
    "            problem_name=name,\n",
    "            data_dir=cfg[\"data_dir\"],\n",
    "            feature_cols=cfg.get(\"feats\") or cfg.get(\"feature_cols\"),\n",
    "            target_cols=cfg.get(\"tgts\") or cfg.get(\"target_cols\"),\n",
    "            seq_len=cfg.get(\"lag\"),\n",
    "            lead=cfg.get(\"lead\"),\n",
    "            batch_size=256,\n",
    "            configs=config_tft,\n",
    "        )\n",
    "        histories = histories | _\n",
    "        del models_tft\n",
    "        tempo_treino[\"tft\"][\"fim\"] = time.time()\n",
    "        tempo_treino[\"tft\"][\"duracao\"] = tempo_treino[\"tft\"][\"fim\"] - tempo_treino[\"tft\"][\"inicio\"]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao treinar TFT para {name}: {e}\")\n",
    "\n",
    "    finally:\n",
    "        gc.collect()\n",
    "\n",
    "    print(f\"‚úÖ Problema {name} conclu√≠do ‚Äî mem√≥ria limpa\\n{'-'*60}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c867bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Desempenho de treinamento dos modelos...\")     \n",
    "for modelo, tempo in tempo_treino.items():   \n",
    "    duracao = tempo.get(\"duracao\", 0)\n",
    "    print(f\"‚è±Ô∏è  Tempo de treino {modelo}: {duracao:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076d5715",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in histories.keys():\n",
    "    print(f\"Hist√≥rico de treinamento do modelo: {model_name}...\")\n",
    "    plot_training_history(histories[model_name], model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfc_venv_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6db5d9d0",
   "metadata": {},
   "source": [
    "# Utilit√°rios Unificados ‚Äî Modelos, Datasets e Ru√≠do\n",
    "\n",
    "Este arquivo re√∫ne tr√™s fun√ß√µes principais, com interface unificada e documenta√ß√£o clara:\n",
    "\n",
    "1. `load_model_unificado(modelo, caminho, ...)`\n",
    "   - Carrega qualquer modelo: `linear`, `mlp`, `lstm` (Keras) ou `tft` (PyTorch Forecasting).\n",
    "   - Aceita arquivo/diret√≥rio EXATO ou uma pasta raiz para descoberta recursiva.\n",
    "\n",
    "2. Carregamento de dados (Parquet)\n",
    "   - `linear/mlp/lstm` ‚Üí Parquet com `*.meta.json` contendo `x_dim`, `y_dim` (e `seq_len`, `lead` no LSTM).\n",
    "   - `tft` ‚Üí Parquet (padr√£o: retorna DataFrame; opcional: cria `TimeSeriesDataSet`).\n",
    "\n",
    "3. `add_noise_features(obj, sigma, tipo, ...)`\n",
    "   - Adiciona ru√≠do GAUSSIANO somente nas FEATURES.\n",
    "   - `tipo='tfdata'` ‚Üí aplica em `tf.data.Dataset` (x,y).\n",
    "   - `tipo='tft'` ‚Üí aplica em batches de `TimeSeriesDataSet`/`DataLoader` (chaves `encoder_cont`/`decoder_cont`).\n",
    "\n",
    "> Observa√ß√£o: O notebook foi simplificado para Parquet apenas (sem TFRecords)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd8ef53",
   "metadata": {},
   "source": [
    "## 1) Carregamento Unificado de Modelos\n",
    "\n",
    "Contrato r√°pido:\n",
    "- Entradas:\n",
    "  - `modelo`: `linear` | `mlp` | `lstm` | `tft`\n",
    "  - `caminho`: arquivo/diret√≥rio exato OU uma pasta para varredura recursiva\n",
    "  - `prefer_exts` (opcional): lista de extens√µes a priorizar (ex.: `[\".cpfg\", \".ckpt\"]` para TFT)\n",
    "  - `allow_unsafe` (bool): permite desserializa√ß√£o insegura apenas para artefatos LOCAIS (Lambda em Keras)\n",
    "- Sa√≠das: `(obj_modelo, info)`\n",
    "  - `obj_modelo`: instancia do modelo carregado (Keras ou TemporalFusionTransformer)\n",
    "  - `info`: dicion√°rio com metadados √∫teis (`path`, `backend`, `kind`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ba9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento dos modelos treinados\n",
    "import torch\n",
    "from tensorflow import keras\n",
    "import json, os\n",
    "\n",
    "def load_model(path: str):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"‚ùå File not found: {path}\")\n",
    "\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "\n",
    "    # === Load model ===\n",
    "    if ext == \".keras\":\n",
    "        model = keras.models.load_model(path)\n",
    "        print(\"‚úÖ TensorFlow model loaded.\")\n",
    "    elif ext in [\".ckpt\", \".cptk\"]:\n",
    "        model = torch.load(path, map_location=\"cpu\")\n",
    "        print(\"‚úÖ PyTorch Lightning checkpoint loaded.\")\n",
    "    else:\n",
    "        raise ValueError(f\"‚ùå Unsupported file extension: {ext}\")\n",
    "\n",
    "    # === Load optional JSON config ===\n",
    "    json_path = f\"{os.path.splitext(path)[0]}.model.json\"\n",
    "    config = None\n",
    "    if os.path.exists(json_path):\n",
    "        with open(json_path, \"r\") as f:\n",
    "            config = json.load(f)\n",
    "        print(f\"üß© Loaded config: {json_path}\")\n",
    "\n",
    "    return model, config\n",
    "\n",
    "# Example usage:\n",
    "# model, config = load_model(\"/path/to/model.keras\")\n",
    "# model, config = load_model(\"/path/to/model.ckpt\")\n",
    "\n",
    "\n",
    "## Carregando modelo linear\n",
    "linear, info_linear = load_model('./modelos/treinamento/linear_Medium.keras')\n",
    "\n",
    "## Carregando modelo MLP\n",
    "mlp, info_mlp = load_model('./modelos/treinamento/mlp_Medium.keras')\n",
    "\n",
    "## Carregando modelo LSTM\n",
    "lstm, info_lstm = load_model('./modelos/treinamento/lstm_Medium.keras')\n",
    "\n",
    "## Carregando modelo TFT\n",
    "tft, info_tft = load_model('./modelos/treinamento/TFT/tft_Medium/best.ckpt')\n",
    "\n",
    "model_list = [\n",
    "    (linear, info_linear),\n",
    "    (mlp, info_mlp),\n",
    "    (lstm, info_lstm),\n",
    "    (tft, info_tft)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b698b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2) Carregando preprocessadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd49a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "# === LINEAR ===\n",
    "linear_path = \"./data/treinamento/preprocessor/linear_preproc.pkl\"\n",
    "with open(linear_path, \"rb\") as f:\n",
    "    linear_preproc = pickle.load(f)\n",
    "\n",
    "\n",
    "# === LSTM ===\n",
    "lstm_path = \"./data/treinamento/preprocessor/lstm_preproc.pkl\"\n",
    "with open(lstm_path, \"rb\") as f:\n",
    "    lstm_preproc = pickle.load(f)\n",
    "\n",
    "# === TFT ===\n",
    "tft_path = \"./data/treinamento/preprocessor/tft_preproc.pkl\"\n",
    "with open(tft_path, \"rb\") as f:\n",
    "    tft_preproc = pickle.load(f)\n",
    "\n",
    "# === PREPROCESSORS DICT ===\n",
    "preprocessors = {\n",
    "    \"linear\": linear_preproc,\n",
    "    \"mlp\": linear_preproc,  # MLP usa o mesmo pr√©-processador do Linear\n",
    "    \"lstm\": lstm_preproc,\n",
    "    \"tft\": tft_preproc,\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Todos os preprocessadores carregados com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0afe84",
   "metadata": {},
   "source": [
    "# Fun√ß√µes helper para an√°lise dos modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e44a40",
   "metadata": {},
   "source": [
    "## Fun√ß√µes de coleta de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85f6b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from preprocessor import LinearPreprocessor, LSTMPreprocessor, TFTPreprocessor\n",
    "import numpy as np\n",
    "\n",
    "def load_dataset_info(model_type: str, dataset_type: str) -> Dict[str, Any]:\n",
    "    info_path = f'./data/treinamento/{model_type}_dataset_{dataset_type}.meta.json'\n",
    "    if not os.path.exists(info_path):\n",
    "        raise FileNotFoundError(f\"‚ùå Dataset info file not found: {info_path}\")\n",
    "    with open(info_path, 'r') as f:\n",
    "        info = json.load(f)\n",
    "    return info\n",
    "\n",
    "\n",
    "def redimension(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    Redimensiona/corta o dataset conforme o tipo de modelo.\n",
    "\n",
    "    Formas de uso esperadas pelo loader:\n",
    "    - Linear: redimension(df, type='linear', lag=..., lead=..., dataset_info=..., mask_value=...)\n",
    "      Onde df √© um DataFrame com colunas lags/leads e features. (Comportamento normal)\n",
    "\n",
    "    - LSTM (duas formas) ‚Äî APENAS X √© ajustado; Y permanece intacto:\n",
    "      a) redimension(df, type='lstm', lag=..., lead=...)\n",
    "         Onde df √© um DataFrame com colunas 'X' e 'Y'; cada c√©lula cont√©m um tensor/ndarray por amostra.\n",
    "      b) redimension(X, Y, type='lstm', lag=..., lead=...)\n",
    "         Onde X e Y s√£o arrays j√° empilhados (batch) no formato (N, T, F) e (N, H, D) respectivamente.\n",
    "\n",
    "    - TFT: redimension(df, type='tft', ...)\n",
    "      Retorna df inalterado.\n",
    "    \"\"\"\n",
    "    model_type = kwargs.get('type') or kwargs.get('model_type')\n",
    "    lag = kwargs.get('lag')\n",
    "    lead = kwargs.get('lead')\n",
    "    mask_value = kwargs.get('mask_value', np.nan)\n",
    "    dataset_info = kwargs.get('dataset_info')\n",
    "\n",
    "    if model_type not in [\"linear\", \"lstm\", \"tft\"]:\n",
    "        raise ValueError(f\"‚ùå Tipo de modelo inv√°lido: {model_type}\")\n",
    "    if not isinstance(lag, int) or not isinstance(lead, int) or lag <= 0 or lead <= 0:\n",
    "        raise ValueError(\"‚ùå Par√¢metros 'lag' e 'lead' devem ser inteiros positivos.\")\n",
    "\n",
    "    # ======== LINEAR (comportamento normal) ======== #\n",
    "    if model_type == 'linear':\n",
    "        if len(args) != 1:\n",
    "            raise TypeError(\"Para 'linear', redimension deve receber um √∫nico argumento: DataFrame df\")\n",
    "        df = args[0]\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise TypeError(\"Para 'linear', o primeiro argumento deve ser um DataFrame\")\n",
    "        if dataset_info is None or not isinstance(dataset_info, dict):\n",
    "            raise ValueError(\"‚ùå √â necess√°rio fornecer 'dataset_info' v√°lido para o modelo linear.\")\n",
    "\n",
    "        print(f\"üîß Redimensionando dataset Linear (lag={lag}, lead={lead})...\")\n",
    "\n",
    "        # Detecta colunas de lag e lead v√°lidas\n",
    "        valid_lags = [f\"quantity_MW_lag{i}\" for i in range(1, lag + 1)]\n",
    "        valid_leads = [f\"quantity_MW_lead{i}\" for i in range(1, lead + 1)]\n",
    "\n",
    "        # Aplica m√°scara nas colunas fora do intervalo permitido\n",
    "        for col in df.columns:\n",
    "            if ('lag' in col or 'lead' in col):\n",
    "                try:\n",
    "                    idx = int(''.join([c for c in col if c.isdigit()]) or 0)\n",
    "                    if ('lag' in col and idx > lag) or ('lead' in col and idx > lead):\n",
    "                        df[col] = mask_value\n",
    "                except ValueError:\n",
    "                    pass  # ignora colunas que n√£o tenham √≠ndice num√©rico\n",
    "\n",
    "        # Mant√©m apenas colunas esperadas\n",
    "        expected_cols = dataset_info.get('feature_cols', []) + dataset_info.get('target_cols', [])\n",
    "        df = df[[c for c in df.columns if c in expected_cols or c in valid_lags + valid_leads]]\n",
    "        print(f\"‚úÖ Dataset Linear redimensionado com {df.shape[0]} linhas e {df.shape[1]} colunas.\")\n",
    "        return df.reset_index(drop=True)\n",
    "\n",
    "    # ======== LSTM (ajusta apenas X; Y intacto) ======== #\n",
    "    if model_type == 'lstm':\n",
    "        print(f\"üîß Redimensionando dataset LSTM (lag={lag}, lead={lead}) ‚Äî APENAS X ser√° ajustado; Y permanece intacto...\")\n",
    "\n",
    "        # Caso (b): X, Y j√° empilhados como arrays (N, T, F) e (N, H, D)\n",
    "        if len(args) >= 2:\n",
    "            X, Y = args[0], args[1]\n",
    "            X = np.array(X)\n",
    "            # N√ÉO alterar Y\n",
    "\n",
    "            if X.ndim < 2:\n",
    "                raise ValueError(f\"‚ùå Esperado X com pelo menos 2 dimens√µes (N, T, ...), obtido {X.shape}\")\n",
    "            # Corta eixo temporal de X\n",
    "            if X.shape[1] > lag:\n",
    "                X = X[:, :lag, ...]\n",
    "            elif X.shape[1] < lag:\n",
    "                raise ValueError(\n",
    "                    f\"‚ùå X possui T={X.shape[1]} < lag={lag}. Ajuste 'lag' ou gere janelas compat√≠veis.\"\n",
    "                )\n",
    "\n",
    "            print(f\"‚úÖ Dataset LSTM (arrays) final ‚Üí X: {X.shape}, Y (inalterado): {np.array(args[1]).shape}\")\n",
    "            return X, args[1]\n",
    "\n",
    "        # Caso (a): df com colunas 'X' e 'Y', cada linha √© uma amostra\n",
    "        if len(args) == 1:\n",
    "            df = args[0]\n",
    "            if isinstance(df, pd.DataFrame) and {'X', 'Y'}.issubset(df.columns):\n",
    "                X_list, Y_list = [], []\n",
    "                for _, row in df.iterrows():\n",
    "                    X_item, Y_item = np.array(row['X']), row['Y']  # mant√©m Y_item como veio\n",
    "\n",
    "                    # Corta excesso/valida m√≠nimos em X\n",
    "                    if X_item.shape[0] > lag:\n",
    "                        X_item = X_item[:lag, ...]\n",
    "                    elif X_item.shape[0] < lag:\n",
    "                        continue  # ignora amostras com X incompleto\n",
    "\n",
    "                    X_list.append(X_item)\n",
    "                    Y_list.append(Y_item)  # Y intacto\n",
    "\n",
    "                if len(X_list) == 0:\n",
    "                    raise ValueError(\"‚ùå Nenhuma amostra v√°lida ap√≥s ajuste por lag em X.\")\n",
    "\n",
    "                X = np.stack(X_list)\n",
    "\n",
    "                # Tenta empilhar Y apenas se compat√≠vel; sen√£o retorna como object array\n",
    "                try:\n",
    "                    Y = np.stack([np.array(y) for y in Y_list])\n",
    "                except Exception:\n",
    "                    Y = np.array(Y_list, dtype=object)\n",
    "\n",
    "                # Sanity check de X\n",
    "                if X.ndim != 3:\n",
    "                    raise ValueError(f\"‚ùå Esperado X com 3 dimens√µes (amostras, lag, features), obtido {X.shape}\")\n",
    "\n",
    "                print(f\"‚úÖ Dataset LSTM (DataFrame) final ‚Üí X: {X.shape}, Y (inalterado): {Y.shape if hasattr(Y,'shape') else 'obj-array'}\")\n",
    "                return X, Y\n",
    "\n",
    "        raise TypeError(\"‚ùå Entrada inv√°lida para LSTM. Use (X, Y) arrays ou DataFrame com colunas 'X' e 'Y'.\")\n",
    "\n",
    "    # ======== TFT ======== #\n",
    "    if model_type == 'tft':\n",
    "        if len(args) != 1:\n",
    "            raise TypeError(\"Para 'tft', redimension deve receber um √∫nico argumento: DataFrame df\")\n",
    "        print(\"‚ÑπÔ∏è Tipo 'tft' detectado: dataset j√° est√° no formato esperado. Nenhum redimensionamento aplicado.\")\n",
    "        return args[0]\n",
    "\n",
    "\n",
    "def get_problem_df(model_type: str, lag, lead, country_list, problem_name) -> pd.DataFrame:\n",
    "    # Instanciando preprocessadores\n",
    "    dataset_info = load_dataset_info(model_type, \"test\")\n",
    "    destino_dir = f'./data/{problem_name}'\n",
    "\n",
    "    if model_type == 'linear':\n",
    "        df, dataset_info = LinearPreprocessor.load_linear_parquet_dataset(\n",
    "        data_dir=destino_dir,\n",
    "        split='test',\n",
    "        batch_size=256,\n",
    "        shuffle=True\n",
    "        )\n",
    "    elif model_type == 'lstm':\n",
    "        df,dataset_info = LSTMPreprocessor.load_lstm_parquet_dataset(\n",
    "        data_dir=destino_dir,\n",
    "        split='test',\n",
    "        batch_size=256,\n",
    "        shuffle=True\n",
    "        )\n",
    "    elif model_type == 'tft':\n",
    "        preproc = TFTPreprocessor(\n",
    "        model_name=\"TFT\",\n",
    "        seq_len=lag,\n",
    "        lead=lead,\n",
    "        country_list=country_list,\n",
    "        feature_cols=dataset_info['feature_cols'],\n",
    "        target_cols=dataset_info['target_cols'],\n",
    "        data_dir=destino_dir\n",
    "    )\n",
    "        df = preproc.load_tft_dataset('test', target_col=dataset_info['target_cols'][0])\n",
    "    else:\n",
    "        raise ValueError(f\"Modelo desconhecido: {model_type}\")\n",
    "    return df, dataset_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b52c05",
   "metadata": {},
   "source": [
    "## Fun√ß√£o de Desnormaliza√ß√£o e Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa68e6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: denormalize + decode using a given Preprocessor instance\n",
    "from typing import Optional, Dict, Any\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def denorm_decode(\n",
    "    preproc,\n",
    "    df: Optional[pd.DataFrame] = None,\n",
    "    normalization_method: Optional[str] = None,\n",
    "    *,\n",
    "    decode_time: bool = True,\n",
    "    decode_label: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Restaura um DataFrame ao estado original usando os m√©todos do *preprocessor*:\n",
    "    - denormalize(): reverte normaliza√ß√£o baseada no scaler salvo em preproc.norm_objects\n",
    "    - decode(): desfaz encodings (label e/ou time_cycle) quando metadados existirem\n",
    "\n",
    "    Par√¢metros\n",
    "    - preproc: inst√¢ncia de LinearPreprocessor | LSTMPreprocessor | TFTPreprocessor\n",
    "    - df: DataFrame a restaurar; se None, usa preproc.df_base\n",
    "    - normalization_method: qual chave usar em preproc.norm_objects; se None, usa a primeira dispon√≠vel\n",
    "    - decode_time: se True, tenta decodificar componentes c√≠clicos de tempo\n",
    "    - decode_label: se True, tenta decodificar labels categ√≥ricos (ex.: country)\n",
    "\n",
    "    Observa√ß√µes\n",
    "    - A opera√ß√£o trabalha em preproc.df_base (conforme API da classe). Sempre retorna uma c√≥pia restaurada.\n",
    "    - Para datasets LSTM salvos como (X,Y) flatten/arrays, a denormaliza√ß√£o direta n√£o se aplica aqui; use a denorm nas s√©ries base antes de gerar janelas.\n",
    "    \"\"\"\n",
    "    if df is not None:\n",
    "        preproc.df_base = df.copy()\n",
    "    elif getattr(preproc, 'df_base', None) is None or preproc.df_base.empty:\n",
    "        raise ValueError(\"df n√£o fornecido e preproc.df_base est√° vazio.\")\n",
    "\n",
    "    # 1) Denormalize, se houver scaler registrado\n",
    "    if normalization_method is None:\n",
    "        # pega a primeira chave dispon√≠vel (ex.: 'minmax' ou 'standard')\n",
    "        norm_keys = list((getattr(preproc, 'norm_objects', {}) or {}).keys())\n",
    "        normalization_method = norm_keys[0] if norm_keys else None\n",
    "\n",
    "    if normalization_method:\n",
    "        try:\n",
    "            preproc.denormalize(normalization_method=normalization_method)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Falha ao denormalizar com m√©todo '{normalization_method}': {e}\")\n",
    "    else:\n",
    "        print(\"[INFO] Nenhum m√©todo de normaliza√ß√£o encontrado no preprocessor; pulando denormalize().\")\n",
    "\n",
    "    # 2) Decodes opcionais\n",
    "    if decode_label and 'label' in getattr(preproc, 'encod_objects', {}):\n",
    "        try:\n",
    "            preproc.decode(encode_method='label')\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Falha ao decodificar labels: {e}\")\n",
    "\n",
    "    if decode_time and 'time_cycle' in getattr(preproc, 'encod_objects', {}):\n",
    "        try:\n",
    "            preproc.decode(encode_method='time_cycle', target_col='decoded_datetime')\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Falha ao decodificar time_cycle: {e}\")\n",
    "\n",
    "    return preproc.df_base.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011a1ff4",
   "metadata": {},
   "source": [
    "## Fun√ß√µes de avalia√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fde0d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def sanity_check_prefetch(ds):\n",
    "    for x, y in ds.take(1):\n",
    "        print(np.std(x.numpy(), axis=0))  # per feature std\n",
    "\n",
    "\n",
    "def avaliar_modelo_keras(\n",
    "    model,\n",
    "    dataset,\n",
    "    titulo=\"Avalia√ß√£o do Modelo\",\n",
    "    problem_name=\"problema\",\n",
    "    max_samples=None,       # None ou <=0 -> usa o m√°ximo poss√≠vel\n",
    "    n_leads: int = 1,\n",
    "    save_dir=\"./resultados/graficos\",\n",
    "    show_plots: bool = True, # üëà se False, apenas salva (sem plt.show)\n",
    "    preproc=None,            # üëà IGNORADO neste fluxo simplificado\n",
    "    normalization_method: str | None = None,  # üëà IGNORADO neste fluxo simplificado\n",
    "    decode_time: bool = True,                 # üëà IGNORADO neste fluxo simplificado\n",
    "    decode_label: bool = True,                # üëà IGNORADO neste fluxo simplificado\n",
    "    model_info: dict | None = None,          # üëà IGNORADO neste fluxo simplificado\n",
    "    run_denorm_decode: bool = False,         # üëà desativado e IGNORADO\n",
    "    salvar_parquet: bool = False,            # üëà desativado e IGNORADO\n",
    "):\n",
    "    \"\"\"\n",
    "    Fluxo simplificado (sem denormalizar/decodificar e sem construir DataFrames):\n",
    "    1) Predict (e evaluate opcional) ‚Üí obt√©m Y_pred e Y_real no estado atual (as-is)\n",
    "    2) Recorta para max_samples\n",
    "    3) Calcula m√©tricas diretamente em arrays e gera gr√°ficos\n",
    "    4) N√ÉO salva datasets (Parquet/CSV) e N√ÉO realiza denormaliza√ß√£o/decodifica√ß√£o\n",
    "    \"\"\"\n",
    "    print(f\"üöÄ Avaliando modelo '{model.name}' (sem denorm/decode, sem DFs)...\")\n",
    "\n",
    "    # === Avalia√ß√£o (evaluate) - mantido para registrar m√©tricas de compile ===\n",
    "    resultados_dict = {}\n",
    "    try:\n",
    "        eval_out = model.evaluate(dataset, verbose=0, return_dict=True)\n",
    "        resultados_dict = {str(k): float(v) for k, v in eval_out.items()}\n",
    "    except TypeError:\n",
    "        try:\n",
    "            resultados = model.evaluate(dataset, verbose=0)\n",
    "            if not isinstance(resultados, (list, tuple)):\n",
    "                resultados = [resultados]\n",
    "            metric_names = getattr(model, \"metrics_names\", []) or []\n",
    "            resultados_dict = {str(k): float(v) for k, v in zip(metric_names, resultados)}\n",
    "        except Exception:\n",
    "            resultados_dict = {}\n",
    "    # === Previs√µes ===\n",
    "    preds, trues = [], []\n",
    "    for X_batch, Y_batch in dataset:\n",
    "        p = model.predict(X_batch, verbose=0)\n",
    "        preds.append(p)\n",
    "        trues.append(Y_batch)\n",
    "    Y_pred = np.concatenate(preds, axis=0)\n",
    "    Y_real = np.concatenate(trues, axis=0)\n",
    "\n",
    "    Y_pred = np.squeeze(Y_pred)\n",
    "    Y_real = np.squeeze(Y_real)\n",
    "\n",
    "    # Limite de amostras\n",
    "    if max_samples is None or max_samples <= 0 or max_samples > len(Y_real):\n",
    "        max_samples = len(Y_real)\n",
    "    print(f\"üìè Utilizando {max_samples} amostras para os gr√°ficos.\")\n",
    "    Y_real = Y_real[:max_samples]\n",
    "    Y_pred = Y_pred[:max_samples]\n",
    "\n",
    "    # === Diret√≥rio de sa√≠da para imagens/JSON de m√©tricas ===\n",
    "    output_dir = os.path.join(save_dir, problem_name, model.name)\n",
    "    if os.path.exists(output_dir):\n",
    "        print(f\"üßπ Limpando diret√≥rio anterior: {output_dir}\")\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    used_scale = 'as-is'\n",
    "\n",
    "    # === Arrays para m√©tricas/plots (j√° est√£o 'as-is') ===\n",
    "    Y_real_used = Y_real\n",
    "    Y_pred_used = Y_pred\n",
    "\n",
    "    # === M√©tricas gerais (overall) ===\n",
    "    diff = Y_pred_used - Y_real_used\n",
    "    abs_diff = np.abs(diff)\n",
    "    mae_all = float(np.mean(abs_diff))\n",
    "    mse_all = float(np.mean(diff ** 2))\n",
    "    rmse_all = float(np.sqrt(mse_all))\n",
    "    denom = np.where(np.abs(Y_real_used) < 1e-8, np.nan, np.abs(Y_real_used))\n",
    "    mape_all = float(np.nanmean(abs_diff / denom))\n",
    "    y_true_flat = Y_real_used.reshape(-1)\n",
    "    y_pred_flat = Y_pred_used.reshape(-1)\n",
    "    ss_res = float(np.sum((y_true_flat - y_pred_flat) ** 2))\n",
    "    ss_tot = float(np.sum((y_true_flat - np.mean(y_true_flat)) ** 2))\n",
    "    r2_all = float(1.0 - ss_res / ss_tot) if ss_tot > 0 else float('nan')\n",
    "\n",
    "    resultados_dict[\"mae\"] = mae_all\n",
    "    resultados_dict[\"mse\"] = mse_all\n",
    "    resultados_dict[\"rmse\"] = rmse_all\n",
    "    resultados_dict[\"mape\"] = mape_all\n",
    "    resultados_dict[\"r2\"] = r2_all\n",
    "    resultados_dict[\"used_scale\"] = used_scale\n",
    "\n",
    "    # Persistir m√©tricas JSON (permanece permitido)\n",
    "    try:\n",
    "        import json\n",
    "        with open(os.path.join(output_dir, \"metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(resultados_dict, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"üíæ metrics.json salvo em: {os.path.join(output_dir, 'metrics.json')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Falha ao salvar metrics.json: {e}\")\n",
    "\n",
    "    # === Tabela de m√©tricas gerais (overall_metrics.png) ===\n",
    "    numeric_items = [(k, v) for k, v in resultados_dict.items() if isinstance(v, (int, float))]\n",
    "    if numeric_items:\n",
    "        fig, ax = plt.subplots(figsize=(8, 0.4 * len(numeric_items) + 1))\n",
    "        ax.axis('off')\n",
    "        col_labels = [\"M√©trica\", \"Valor\"]\n",
    "        table_data = [[k, f\"{v:.6f}\"] for k, v in numeric_items]\n",
    "        table = ax.table(cellText=table_data, colLabels=col_labels, loc='center')\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(9)\n",
    "        table.scale(1, 1.1)\n",
    "        ax.set_title(f\"{titulo} ‚Äî M√©tricas Gerais ({used_scale})\", pad=12)\n",
    "        overall_path = os.path.join(output_dir, \"overall_metrics.png\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(overall_path, dpi=150)\n",
    "        if show_plots:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "        print(f\"üíæ Tabela de m√©tricas gerais salva em: {overall_path}\\n\")\n",
    "\n",
    "    # === M√©tricas por lead + gr√°ficos comparativos ===\n",
    "    per_lead_metrics = {}\n",
    "    if isinstance(Y_real_used, np.ndarray) and Y_real_used.ndim > 1:\n",
    "        effective_leads = min(Y_real_used.shape[1], n_leads)\n",
    "        for i in range(effective_leads):\n",
    "            y_t = Y_real_used[:, i]\n",
    "            y_p = Y_pred_used[:, i]\n",
    "            d = y_p - y_t\n",
    "            ad = np.abs(d)\n",
    "            mae_i = float(np.mean(ad))\n",
    "            mse_i = float(np.mean(d ** 2))\n",
    "            rmse_i = float(np.sqrt(mse_i))\n",
    "            denom_i = np.where(np.abs(y_t) < 1e-8, np.nan, np.abs(y_t))\n",
    "            mape_i = float(np.nanmean(ad / denom_i))\n",
    "            ss_res_i = float(np.sum((y_t - y_p) ** 2))\n",
    "            ss_tot_i = float(np.sum((y_t - np.mean(y_t)) ** 2))\n",
    "            r2_i = float(1.0 - ss_res_i / ss_tot_i) if ss_tot_i > 0 else float('nan')\n",
    "            per_lead_metrics.setdefault('mae', []).append(mae_i)\n",
    "            per_lead_metrics.setdefault('mse', []).append(mse_i)\n",
    "            per_lead_metrics.setdefault('rmse', []).append(rmse_i)\n",
    "            per_lead_metrics.setdefault('mape', []).append(mape_i)\n",
    "            per_lead_metrics.setdefault('r2', []).append(r2_i)\n",
    "\n",
    "        # Gera gr√°fico comparativo para cada m√©trica\n",
    "        def _plot_metric_series(metric_name, values):\n",
    "            fig, ax = plt.subplots(figsize=(9, 4))\n",
    "            xs = np.arange(1, len(values) + 1)\n",
    "            ax.plot(xs, values, marker='o', linewidth=2, color='steelblue')\n",
    "            ax.set_title(f\"{titulo} ‚Äî {metric_name.upper()} por Lead ({used_scale})\")\n",
    "            ax.set_xlabel(\"Lead\")\n",
    "            ax.set_ylabel(metric_name.upper())\n",
    "            ax.grid(True, linestyle='--', alpha=0.6)\n",
    "            for x, v in zip(xs, values):\n",
    "                ax.annotate(f\"{v:.4f}\", (x, v), textcoords=\"offset points\", xytext=(0, -15), ha='center', fontsize=8, color='darkslategray')\n",
    "            plt.tight_layout()\n",
    "            fpath = os.path.join(output_dir, f\"metric_lead_comparison_{metric_name}.png\")\n",
    "            plt.savefig(fpath, dpi=150)\n",
    "            if show_plots:\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.close()\n",
    "            print(f\"üíæ Gr√°fico comparativo por lead salvo: {fpath}\")\n",
    "\n",
    "        for mname, vals in per_lead_metrics.items():\n",
    "            _plot_metric_series(mname, vals)\n",
    "\n",
    "    # === Gr√°ficos Predito vs Real por lead (s√©ries) ===\n",
    "    if isinstance(Y_real_used, np.ndarray) and Y_real_used.ndim > 1:\n",
    "        effective_leads_series = min(Y_real_used.shape[1], n_leads)\n",
    "    else:\n",
    "        effective_leads_series = 1\n",
    "\n",
    "    for i in range(effective_leads_series):\n",
    "        y_true_i = Y_real_used[:, i] if (effective_leads_series > 1) else Y_real_used\n",
    "        y_pred_i = Y_pred_used[:, i] if (effective_leads_series > 1) else Y_pred_used\n",
    "        plt.figure(figsize=(9, 5))\n",
    "        plt.plot(y_true_i, label=f\"Real lead {i+1}\", color=\"black\", linewidth=2, alpha=0.8)\n",
    "        plt.plot(y_pred_i, label=f\"Previsto lead {i+1}\", color=\"tomato\", linewidth=2, alpha=0.8)\n",
    "        plt.title(f\"{titulo} - Lead {i+1} ({used_scale})\")\n",
    "        plt.xlabel(\"Amostra\")\n",
    "        plt.ylabel(\"Valor\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "        plt.tight_layout()\n",
    "        file_name = f\"predito_vs_real_lead{i+1}.png\"\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "        plt.savefig(file_path, dpi=150)\n",
    "        if show_plots:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "        print(f\"üíæ Gr√°fico Lead {i+1} salvo em: {file_path}\")\n",
    "\n",
    "    print(\"‚úÖ Avalia√ß√£o conclu√≠da.\")\n",
    "\n",
    "    result_payload = dict(resultados_dict)\n",
    "    result_payload['y_true'] = Y_real_used\n",
    "    result_payload['y_pred'] = Y_pred_used\n",
    "    if 'per_lead_metrics' in locals() and per_lead_metrics:\n",
    "        result_payload['per_lead_metrics'] = per_lead_metrics\n",
    "    return result_payload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e135642",
   "metadata": {},
   "source": [
    "# N1A ‚Äî S√©rie Univariada (seq_len=72, lead=72)\n",
    "\n",
    "\n",
    "Objetivo\n",
    "- Prever 24 horas de carga √† frente com janelas de 48 horas de hist√≥rico para um √∫nico pa√≠s.\n",
    "\n",
    "\n",
    "Artefatos esperados\n",
    "- Parquet (Linear/MLP): `data/N1A/linear_dataset_{split}.parquet` + `linear_dataset_{split}.meta.json` ‚Üí { x_dim, y_dim }\n",
    "- Parquet (LSTM): `data/N1A/lstm_dataset_{split}.parquet` + `lstm_dataset_{split}.meta.json` ‚Üí { seq_len=240, lead=72, x_dim, y_dim }\n",
    "\n",
    "\n",
    "Modelos a comparar\n",
    "- Linear, MLP, LSTM (Keras) e, opcionalmente, TFT.\n",
    "\n",
    "\n",
    "M√©tricas e checks\n",
    "- MAE, RMSE, MAPE.\n",
    "- Checar: shapes conforme meta.json; aus√™ncia de NaNs; n√∫mero de amostras > 0.\n",
    "\n",
    "\n",
    "Visualiza√ß√µes sugeridas\n",
    "- Boxplot de erro por horizonte; barras de MAE por modelo; curva MAE vs horizonte.\n",
    "\n",
    "\n",
    "Notas\n",
    "- As variantes A/B s√£o obtidas reduzindo a janela/horizonte efetivos na avalia√ß√£o a partir do dataset base (240/72).\n",
    "- Padding (se houver) deve usar sentinela fixo para permitir mascaramento e ru√≠do seletivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b73153dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Gr√°fico Lead 66 salvo em: ./resultados/graficos/N1A/linear_model/predito_vs_real_lead66.png\n",
      "üíæ Gr√°fico Lead 67 salvo em: ./resultados/graficos/N1A/linear_model/predito_vs_real_lead67.png\n",
      "üíæ Gr√°fico Lead 68 salvo em: ./resultados/graficos/N1A/linear_model/predito_vs_real_lead68.png\n",
      "üíæ Gr√°fico Lead 69 salvo em: ./resultados/graficos/N1A/linear_model/predito_vs_real_lead69.png\n",
      "üíæ Gr√°fico Lead 70 salvo em: ./resultados/graficos/N1A/linear_model/predito_vs_real_lead70.png\n",
      "üíæ Gr√°fico Lead 71 salvo em: ./resultados/graficos/N1A/linear_model/predito_vs_real_lead71.png\n",
      "üíæ Gr√°fico Lead 72 salvo em: ./resultados/graficos/N1A/linear_model/predito_vs_real_lead72.png\n",
      "‚úÖ Avalia√ß√£o conclu√≠da.\n",
      "üöÄ Avaliando modelo 'mlp_selu' (sem denorm/decode, sem DFs)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     33\u001b[39m avaliar_modelo_keras(\n\u001b[32m     34\u001b[39m     model=linear,\n\u001b[32m     35\u001b[39m     dataset=ds_linear,\n\u001b[32m   (...)\u001b[39m\u001b[32m     44\u001b[39m     salvar_parquet=\u001b[38;5;28;01mTrue\u001b[39;00m               \n\u001b[32m     45\u001b[39m )\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m### Avaliando modelo MLP\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[43mavaliar_modelo_keras\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mds_linear\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtitulo\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mModelo MLP - Problema N1A\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproblem_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mN1A\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_leads\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m72\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_plots\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreproc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreprocessors\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmlp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlinear_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_denorm_decode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43msalvar_parquet\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m               \u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m### Avaliando modelo LSTM\u001b[39;00m\n\u001b[32m     63\u001b[39m avaliar_modelo_keras(\n\u001b[32m     64\u001b[39m     model=lstm,\n\u001b[32m     65\u001b[39m     dataset=ds_lstm,\n\u001b[32m   (...)\u001b[39m\u001b[32m     74\u001b[39m     salvar_parquet=\u001b[38;5;28;01mTrue\u001b[39;00m               \n\u001b[32m     75\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mavaliar_modelo_keras\u001b[39m\u001b[34m(model, dataset, titulo, problem_name, max_samples, n_leads, save_dir, show_plots, preproc, normalization_method, decode_time, decode_label, model_info, run_denorm_decode, salvar_parquet)\u001b[39m\n\u001b[32m     55\u001b[39m preds, trues = [], []\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, Y_batch \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     p = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m     preds.append(p)\n\u001b[32m     59\u001b[39m     trues.append(Y_batch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/tcc_2025/TCC-2025/tfc_venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/tcc_2025/TCC-2025/tfc_venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:596\u001b[39m, in \u001b[36mTensorFlowTrainer.predict\u001b[39m\u001b[34m(self, x, batch_size, verbose, steps, callbacks)\u001b[39m\n\u001b[32m    594\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    595\u001b[39m callbacks.on_predict_end()\n\u001b[32m--> \u001b[39m\u001b[32m596\u001b[39m outputs = \u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_structure_up_to\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpotentially_ragged_concat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    599\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tree.map_structure(convert_to_np_if_not_ragged, outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/tcc_2025/TCC-2025/tfc_venv/lib/python3.12/site-packages/keras/src/tree/tree_api.py:237\u001b[39m, in \u001b[36mmap_structure_up_to\u001b[39m\u001b[34m(shallow_structure, func, *structures)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mkeras.tree.map_structure_up_to\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmap_structure_up_to\u001b[39m(shallow_structure, func, *structures):\n\u001b[32m    205\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Maps `func` through given structures up to `shallow_structure`.\u001b[39;00m\n\u001b[32m    206\u001b[39m \n\u001b[32m    207\u001b[39m \u001b[33;03m    This is a variant of `map_structure` which only maps the given structures\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    235\u001b[39m \u001b[33;03m            shallower.\u001b[39;00m\n\u001b[32m    236\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree_impl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_structure_up_to\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshallow_structure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mstructures\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/tcc_2025/TCC-2025/tfc_venv/lib/python3.12/site-packages/keras/src/tree/optree_impl.py:127\u001b[39m, in \u001b[36mmap_structure_up_to\u001b[39m\u001b[34m(shallow_structure, func, *structures)\u001b[39m\n\u001b[32m    124\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mStructures don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt have the same nested structure.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args)\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc_with_check_without_shallow_structure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshallow_structure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43mstructures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnone_is_leaf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mkeras\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/tcc_2025/TCC-2025/tfc_venv/lib/python3.12/site-packages/optree/ops.py:766\u001b[39m, in \u001b[36mtree_map\u001b[39m\u001b[34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[39m\n\u001b[32m    764\u001b[39m leaves, treespec = _C.flatten(tree, is_leaf, none_is_leaf, namespace)\n\u001b[32m    765\u001b[39m flat_args = [leaves] + [treespec.flatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rests]\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreespec\u001b[49m\u001b[43m.\u001b[49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/tcc_2025/TCC-2025/tfc_venv/lib/python3.12/site-packages/keras/src/tree/optree_impl.py:125\u001b[39m, in \u001b[36mmap_structure_up_to.<locals>.func_with_check_without_shallow_structure\u001b[39m\u001b[34m(shallow, *args)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m optree.tree_is_leaf(shallow):\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mStructures don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt have the same nested structure.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/tcc_2025/TCC-2025/tfc_venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:985\u001b[39m, in \u001b[36mpotentially_ragged_concat\u001b[39m\u001b[34m(tensors)\u001b[39m\n\u001b[32m    981\u001b[39m non_batch_shapes = tf.stack([tf.shape(tensor)[\u001b[32m1\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m tensors])\n\u001b[32m    982\u001b[39m constant_dims = tf.math.reduce_all(\n\u001b[32m    983\u001b[39m     non_batch_shapes == non_batch_shapes[:\u001b[32m1\u001b[39m], axis=\u001b[32m0\u001b[39m\n\u001b[32m    984\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m985\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduce_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconstant_dims\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.item():\n\u001b[32m    986\u001b[39m     \u001b[38;5;66;03m# All non-batch dims are constant\u001b[39;00m\n\u001b[32m    987\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_scalar(tensors[\u001b[32m0\u001b[39m]):\n\u001b[32m    988\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m tf.stack(tensors, axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/tcc_2025/TCC-2025/tfc_venv/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:440\u001b[39m, in \u001b[36m_EagerTensorBase.numpy\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[32m    418\u001b[39m \n\u001b[32m    419\u001b[39m \u001b[33;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    437\u001b[39m \u001b[33;03m    NumPy dtype.\u001b[39;00m\n\u001b[32m    438\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    439\u001b[39m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m maybe_arr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr.copy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np.ndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/tcc_2025/TCC-2025/tfc_venv/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:406\u001b[39m, in \u001b[36m_EagerTensorBase._numpy\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> npt.ArrayLike:\n\u001b[32m    405\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    407\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m    408\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m core._status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Carregamento dos dataset N1A (Parquet)\n",
    "\n",
    "# linear/mlp (iguais)\n",
    "ds_linear, linear_info = get_problem_df(\n",
    "    model_type='linear',\n",
    "    lag=72,\n",
    "    lead=72,\n",
    "    country_list=['ES'],\n",
    "    problem_name='N1A'\n",
    ")\n",
    "\n",
    "# lstm\n",
    "ds_lstm, lstm_info = get_problem_df(\n",
    "    model_type='lstm',\n",
    "    lag=72,\n",
    "    lead=72,\n",
    "    country_list=['ES'],\n",
    "    problem_name='N1A'\n",
    ")\n",
    "\n",
    "tft\n",
    "ds_tft, tft_info = get_problem_df(\n",
    "    model_type='tft',\n",
    "    lag=72,\n",
    "    lead=72,\n",
    "    country_list=['ES'],\n",
    "    problem_name='N1A'\n",
    ")\n",
    "\n",
    "## Avaliando dados\n",
    "\n",
    "### Avaliando modelo Linear\n",
    "avaliar_modelo_keras(\n",
    "    model=linear,\n",
    "    dataset=ds_linear,\n",
    "    titulo=\"Modelo Linear - Problema N1A\",\n",
    "    problem_name=\"N1A\",\n",
    "    n_leads=72,\n",
    "    max_samples=1000,\n",
    "    show_plots=False,\n",
    "    preproc=preprocessors['linear'],\n",
    "    model_info=linear_info,           \n",
    "    run_denorm_decode=True,           \n",
    "    salvar_parquet=True               \n",
    ")\n",
    "\n",
    "### Avaliando modelo MLP\n",
    "avaliar_modelo_keras(\n",
    "    model=mlp,\n",
    "    dataset=ds_linear,\n",
    "    titulo=\"Modelo MLP - Problema N1A\",\n",
    "    problem_name=\"N1A\",\n",
    "    n_leads=72,\n",
    "    max_samples=1000,\n",
    "    show_plots=False,\n",
    "    preproc=preprocessors['mlp'],\n",
    "    model_info=linear_info,           \n",
    "    run_denorm_decode=True,           \n",
    "    salvar_parquet=True               \n",
    ")\n",
    "\n",
    "### Avaliando modelo LSTM\n",
    "avaliar_modelo_keras(\n",
    "    model=lstm,\n",
    "    dataset=ds_lstm,\n",
    "    titulo=\"Modelo lstm - Problema N1A\",\n",
    "    problem_name=\"N1A\",\n",
    "    n_leads=72,\n",
    "    max_samples=1000,\n",
    "    show_plots=False,\n",
    "    preproc=preprocessors['lstm'],\n",
    "    model_info=linear_info,           \n",
    "    run_denorm_decode=True,           \n",
    "    salvar_parquet=True               \n",
    ")\n",
    "\n",
    "### Avaliando modelo TFT\n",
    "avaliar_modelo_keras(\n",
    "    model=tft,\n",
    "    dataset=ds_tft,\n",
    "    titulo=\"Modelo TFT - Problema N1A\",\n",
    "    problem_name=\"N1A\",\n",
    "    n_leads=72,\n",
    "    max_samples=1000,\n",
    "    show_plots=False,\n",
    "    preproc=preprocessors['tft'],\n",
    "    model_info=linear_info,           \n",
    "    run_denorm_decode=True,           \n",
    "    salvar_parquet=True               \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb06705",
   "metadata": {},
   "source": [
    "# N1B ‚Äî S√©rie Univariada (seq_len=168, lead=48)\n",
    "\n",
    "\n",
    "Objetivo\n",
    "- Prever 48 horas de carga √† frente com janelas de 168 horas de hist√≥rico para um √∫nico pa√≠s.\n",
    "\n",
    "\n",
    "Artefatos esperados\n",
    "- Parquet (Linear/MLP): `data/N1B/linear_dataset_{split}.parquet` + meta { x_dim, y_dim }\n",
    "- Parquet (LSTM): `data/N1B/lstm_dataset_{split}.parquet` + meta { seq_len=240, lead=72, x_dim, y_dim }\n",
    "- TFT (opcional): `data/treinamento/tft_dataset_{split}.parquet`\n",
    "\n",
    "\n",
    "Modelos a comparar\n",
    "- Linear, MLP, LSTM, TFT (opcional).\n",
    "\n",
    "\n",
    "M√©tricas e checks\n",
    "- MAE, RMSE, MAPE; valida√ß√£o de shapes e aus√™ncia de NaNs.\n",
    "\n",
    "\n",
    "Visualiza√ß√µes sugeridas\n",
    "- Barras de MAE m√©dio por modelo; curva de erro por horizonte.\n",
    "\n",
    "\n",
    "Notas\n",
    "- As variantes A/B s√£o derivadas do dataset base (240/72) reduzindo janela/horizonte na avalia√ß√£o, sem retreinar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ca18c5",
   "metadata": {},
   "source": [
    "# N1C ‚Äî S√©rie Univariada (seq_len=240, lead=72)\n",
    "\n",
    "\n",
    "Objetivo\n",
    "- Pr√©-treino/treino com a janela de 240 horas e avaliar horizonte de 72 horas.\n",
    "\n",
    "\n",
    "Artefatos esperados\n",
    "- Parquet (Linear/MLP): `data/N1C/linear_dataset_{split}.parquet` + meta { x_dim, y_dim }\n",
    "- Parquet (LSTM): `data/N1C/lstm_dataset_{split}.parquet` + meta { seq_len=240, lead=72, x_dim, y_dim }\n",
    "- TFT (opcional): `data/treinamento/tft_dataset_{split}.parquet`\n",
    "\n",
    "\n",
    "Modelos a comparar\n",
    "- Linear, MLP, LSTM, TFT (opcional).\n",
    "\n",
    "\n",
    "M√©tricas e checks\n",
    "- MAE, RMSE, MAPE; n√∫mero de amostras por split; coer√™ncia entre seq_len/lead do meta e shapes efetivos.\n",
    "\n",
    "\n",
    "Visualiza√ß√µes sugeridas\n",
    "- Curva comparativa de MAE vs horizonte; top‚Äëk modelos por MAE.\n",
    "\n",
    "\n",
    "Notas\n",
    "- Esta variante (C) √© a base m√°xima de lookback e horizonte; A/B s√£o obtidas por redu√ß√£o na avalia√ß√£o."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264ad282",
   "metadata": {},
   "source": [
    "# N2A ‚Äî M√∫ltiplos Pa√≠ses (seq_len=72, lead=24)\n",
    "\n",
    "\n",
    "Objetivo\n",
    "- Prever 24 horas com 72 horas de hist√≥rico, agrupando por pa√≠s.\n",
    "\n",
    "\n",
    "Artefatos esperados\n",
    "- Parquet (Linear/MLP): `data/N2A/linear_dataset_{split}.parquet` + meta\n",
    "- Parquet (LSTM): `data/N2A/lstm_dataset_{split}.parquet` + meta { seq_len=240, lead=72, x_dim, y_dim }\n",
    "- TFT (recomendado): `data/treinamento/tft_dataset_{split}.parquet` (colunas: _group_id=country, time_idx crescente por grupo, quantity_MW)\n",
    "\n",
    "\n",
    "Modelos a comparar\n",
    "- Linear, MLP, LSTM (podem exigir codifica√ß√£o/flatten por grupo);\n",
    "- TFT (nativamente multi‚Äëgrupo).\n",
    "\n",
    "\n",
    "M√©tricas e checks\n",
    "- MAE/RMSE por pa√≠s e globais; n√∫mero de grupos; equil√≠brio de amostras por grupo.\n",
    "\n",
    "\n",
    "Visualiza√ß√µes sugeridas\n",
    "- Barras de MAE por modelo; facetas por pa√≠s; curva MAE vs horizonte.\n",
    "\n",
    "\n",
    "Notas\n",
    "- As variantes A/B/C partem do dataset base (240/72), aplicando janelas/horizontes reduzidos na avalia√ß√£o."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b4db9d",
   "metadata": {},
   "source": [
    "# N2B ‚Äî M√∫ltiplos Pa√≠ses (seq_len=168, lead=48)\n",
    "\n",
    "\n",
    "Objetivo\n",
    "- Prever 48 horas com 168 horas de hist√≥rico, agrupado por pa√≠s.\n",
    "\n",
    "\n",
    "Artefatos esperados\n",
    "- Parquet (Linear/MLP): `data/N2B/linear_dataset_{split}.parquet` + meta\n",
    "- Parquet (LSTM): `data/N2B/lstm_dataset_{split}.parquet` + meta { seq_len=240, lead=72, x_dim, y_dim }\n",
    "- TFT (recomendado): `data/treinamento/tft_dataset_{split}.parquet` com `_group_id`, `time_idx`, target.\n",
    "\n",
    "\n",
    "Modelos a comparar\n",
    "- Linear, MLP, LSTM; TFT.\n",
    "\n",
    "\n",
    "M√©tricas e checks\n",
    "- MAE/RMSE por pa√≠s e agregadas; distribui√ß√£o de amostras por grupo.\n",
    "\n",
    "\n",
    "Visualiza√ß√µes sugeridas\n",
    "- Barras de MAE por modelo; linhas por horizonte; painel por pa√≠s.\n",
    "\n",
    "\n",
    "Notas\n",
    "- Variantes A/B/C usam janelas/horizontes efetivos na avalia√ß√£o; base: 240/72."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1278bb",
   "metadata": {},
   "source": [
    "# N2C ‚Äî M√∫ltiplos Pa√≠ses (seq_len=240, lead=72)\n",
    "\n",
    "\n",
    "Objetivo\n",
    "- Prever 72 horas com 240 horas de hist√≥rico, agrupado por pa√≠s. Esta variante √© a base para reuso em A/B.\n",
    "\n",
    "\n",
    "Artefatos esperados\n",
    "- Parquet (Linear/MLP): `data/N2C/linear_dataset_{split}.parquet` + meta\n",
    "- Parquet (LSTM): `data/N2C/lstm_dataset_{split}.parquet` + meta { seq_len=240, lead=72, x_dim, y_dim }\n",
    "- TFT (recomendado): `data/treinamento/tft_dataset_{split}.parquet` (agrupado por `_group_id`).\n",
    "\n",
    "\n",
    "Modelos a comparar\n",
    "- Linear, MLP, LSTM, TFT.\n",
    "\n",
    "\n",
    "M√©tricas e checks\n",
    "- MAE, RMSE, MAPE; compara√ß√£o por pa√≠s; checagem de time_idx e integridade por grupo.\n",
    "\n",
    "\n",
    "Visualiza√ß√µes sugeridas\n",
    "- Curva MAE vs horizonte; ranking de modelos por pa√≠s e global.\n",
    "\n",
    "\n",
    "Notas\n",
    "- Base usa seq_len=240 e lead=72; varia√ß√µes A/B podem ser avaliadas reduzindo janela no dataset sem retreino."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dffd95",
   "metadata": {},
   "source": [
    "# N3 ‚Äî Robustez a Ru√≠do (sobre N2)\n",
    "\n",
    "Objetivo\n",
    "- Medir degrada√ß√£o de desempenho sob ru√≠do gaussiano nas FEATURES (teste), mantendo r√≥tulos intactos.\n",
    "\n",
    "Configura√ß√£o\n",
    "- Conjuntos: use os datasets do N2 (A/B/C).\n",
    "- Intensidades: œÉ ‚àà {0.00, 0.01, 0.03, 0.05, 0.10}.\n",
    "- Aplica√ß√£o:\n",
    "  - Keras/tf.data: `add_noise_features(ds, sigma, tipo='tfdata', pad_sentinel=-999.0)`.\n",
    "  - TFT: `add_noise_features(tft_ds ou dataloader, sigma, tipo='tft', batch_size=..., train=False)`.\n",
    "\n",
    "M√©tricas e checks\n",
    "- MAE/RMSE por sigma; checar preserva√ß√£o de sentinela (TF) e invari√¢ncia de Y.\n",
    "\n",
    "Visualiza√ß√µes sugeridas\n",
    "- Curvas MAE vs œÉ por modelo; heatmap de degrada√ß√£o por horizonte e sigma.\n",
    "\n",
    "Notas\n",
    "- Aplique ru√≠do ap√≥s normaliza√ß√£o das features.\n",
    "- N√£o altere o treino; apenas avalia√ß√£o/benchmark.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfc_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a6b9942",
   "metadata": {},
   "source": [
    "# Resultados — Comparação de Modelos\n",
    "Este notebook descobre conjuntos de dados de teste gerados (N1A/N1B/...) e compara os modelos treinados (Linear/MLP, LSTM, TFT) quando disponíveis.\n",
    "\n",
    "Diretrizes: baseado no escopo de coleta (carga real A65), avaliamos previsões multi-passos (lead) por problema. Caso algum artefato/modelo não esteja disponível, a linha correspondente é marcada como 'indisponível'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f9dc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports e utilitários\n",
    "import os, json, sys, glob, math, random\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TensorFlow é opcional — carregaremos sob demanda\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    _HAS_TF = True\n",
    "except Exception:\n",
    "    _HAS_TF = False\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ------------------------- Métricas ------------------------- #\n",
    "def mae(y_true, y_pred):\n",
    "    return float(np.mean(np.abs(y_true - y_pred)))\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(math.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "def mape(y_true, y_pred, eps=1e-8):\n",
    "    denom = np.clip(np.abs(y_true), eps, None)\n",
    "    return float(100.0 * np.mean(np.abs((y_true - y_pred) / denom)))\n",
    "\n",
    "# -------------------- Descoberta de problemas -------------------- #\n",
    "def list_problem_dirs(root=\"data\") -> List[str]:\n",
    "    if not os.path.isdir(root):\n",
    "        return []\n",
    "    problems = [os.path.join(root, d) for d in os.listdir(root) if d.startswith('N') and os.path.isdir(os.path.join(root, d))]\n",
    "    return sorted(problems)\n",
    "\n",
    "# -------------------- Loaders Linear -------------------- #\n",
    "def load_linear_meta(problem_dir: str) -> Dict:\n",
    "    meta_path = os.path.join(problem_dir, 'linear_dataset_test.meta.json')\n",
    "    if os.path.exists(meta_path):\n",
    "        with open(meta_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def load_linear_parquet(problem_dir: str, meta: Dict) -> Tuple[pd.DataFrame, List[str], List[str]]:\n",
    "    pq_path = meta.get('parquet_path') or os.path.join(problem_dir, 'linear_dataset_test.parquet')\n",
    "    if not os.path.exists(pq_path):\n",
    "        return pd.DataFrame(), [], []\n",
    "    df = pd.read_parquet(pq_path)\n",
    "    fcols = list(meta.get('feature_cols') or [])\n",
    "    tcols = list(meta.get('target_cols') or [])\n",
    "    missing = [c for c in (fcols + tcols) if c not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"[WARN] Colunas ausentes no parquet de {problem_dir}: {missing[:5]}{'...' if len(missing)>5 else ''}\")\n",
    "    return df, fcols, tcols\n",
    "\n",
    "def infer_horizon_cols(target_cols: List[str]) -> List[str]:\n",
    "    \"\"\"Retorna apenas colunas de lead (ignora 'quantity_MW' presente como t0).\"\"\"\n",
    "    if not target_cols:\n",
    "        return []\n",
    "    leads = [c for c in target_cols if c.startswith('quantity_MW_lead')]\n",
    "    # fallback: se não houver padrão, usa todas menos a primeira\n",
    "    if not leads and len(target_cols) > 1:\n",
    "        return target_cols[1:]\n",
    "    return leads\n",
    "\n",
    "# -------------------- Descoberta de modelos -------------------- #\n",
    "def discover_keras_models(search_roots: List[str]) -> List[str]:\n",
    "    patterns = []\n",
    "    for root in search_roots:\n",
    "        patterns += glob.glob(os.path.join(root, '**', '*.keras'), recursive=True)\n",
    "        patterns += glob.glob(os.path.join(root, '**', '*.h5'), recursive=True)\n",
    "        # SavedModel dirs (contendo saved_model.pb)\n",
    "        for pb in glob.glob(os.path.join(root, '**', 'saved_model.pb'), recursive=True):\n",
    "            patterns.append(os.path.dirname(pb))\n",
    "    # Filtrar ambientes (venv)\n",
    "    patterns = [p for p in patterns if '/tfc_venv/' not in p and '.venv/' not in p and '/.git/' not in p]\n",
    "    return sorted(set(patterns))\n",
    "\n",
    "def safe_load_keras_model(path: str):\n",
    "    if not _HAS_TF:\n",
    "        return None\n",
    "    try:\n",
    "        if os.path.isdir(path):\n",
    "            return tf.keras.models.load_model(path)\n",
    "        return tf.keras.models.load_model(path, compile=False)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Falha ao carregar modelo Keras {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def model_label_from_path(path: str) -> str:\n",
    "    base = os.path.basename(path.rstrip('/'))\n",
    "    if base == 'variables':\n",
    "        base = os.path.basename(os.path.dirname(path))\n",
    "    return base\n",
    "\n",
    "# -------------------- Avaliação -------------------- #\n",
    "def align_predictions(Y: np.ndarray, Yp: np.ndarray, target_cols: List[str]) -> Tuple[np.ndarray, np.ndarray, List[str]]:\n",
    "    \"\"\"Alinha Y e Yp para mesma largura temporal e retorna colunas de lead usadas.\"\"\"\n",
    "    leads = infer_horizon_cols(target_cols)\n",
    "    # Se Y inclui t0 + leads, recortar para apenas leads\n",
    "    if Y.ndim == 2 and len(target_cols) >= 2 and target_cols[0] == 'quantity_MW' and len(leads) > 0:\n",
    "        Yt = Y[:, 1:1+len(leads)]\n",
    "    else:\n",
    "        Yt = Y\n",
    "    Yp = np.asarray(Yp)\n",
    "    if Yp.ndim == 3:\n",
    "        Yp = Yp.reshape(Yp.shape[0], -1)\n",
    "    # Cortar para mesma largura\n",
    "    if Yt.ndim == 2 and Yp.ndim == 2:\n",
    "        H = min(Yt.shape[1], Yp.shape[1])\n",
    "        if H <= 0:\n",
    "            return Yt[:, :0], Yp[:, :0], []\n",
    "        return Yt[:, :H], Yp[:, :H], leads[:H] if leads else [f'lead{i+1}' for i in range(H)]\n",
    "    # fallback escalar\n",
    "    H = 1\n",
    "    return Yt.reshape(-1, 1), Yp.reshape(-1, 1), [leads[0] if leads else 'lead1']\n",
    "\n",
    "def evaluate_overall(Yt: np.ndarray, Yp: np.ndarray) -> Dict[str, float]:\n",
    "    return {\n",
    "        'MAE': mae(Yt, Yp),\n",
    "        'RMSE': rmse(Yt, Yp),\n",
    "        'MAPE': mape(Yt, Yp)\n",
    "    }\n",
    "\n",
    "def evaluate_per_horizon(Yt: np.ndarray, Yp: np.ndarray) -> pd.DataFrame:\n",
    "    if Yt.ndim != 2 or Yp.ndim != 2:\n",
    "        return pd.DataFrame()\n",
    "    H = Yt.shape[1]\n",
    "    rows = []\n",
    "    for h in range(H):\n",
    "        yt = Yt[:, h]\n",
    "        yp = Yp[:, h]\n",
    "        rows.append({'h': h+1, 'MAE': mae(yt, yp), 'RMSE': rmse(yt, yp), 'MAPE': mape(yt, yp)})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def baseline_repeat_last(Y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Baseline: repetir o valor do passo t (primeira coluna de Y se existir) para todos os horizontes.\"\"\"\n",
    "    if Y.ndim == 1:\n",
    "        return Y.copy()\n",
    "    # Se Y inclui t0 + leads, use t0 (coluna 0); caso contrário, use a coluna 0 como proxy\n",
    "    base = Y[:, 0].reshape(-1, 1)\n",
    "    return np.repeat(base, Y.shape[1], axis=1)\n",
    "\n",
    "def evaluate_keras_on_linear(model, X: np.ndarray, Y: np.ndarray, target_cols: List[str]) -> Tuple[Dict[str, float], pd.DataFrame]:\n",
    "    try:\n",
    "        Yp = model.predict(X, verbose=0)\n",
    "        Yt, Yp, used_leads = align_predictions(Y, Yp, target_cols)\n",
    "        overall = evaluate_overall(Yt, Yp)\n",
    "        per_h = evaluate_per_horizon(Yt, Yp)\n",
    "        if not per_h.empty:\n",
    "            per_h['lead_name'] = used_leads[:len(per_h)]\n",
    "        return overall, per_h\n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}, pd.DataFrame()\n",
    "\n",
    "def add_input_noise(X: np.ndarray, sigma: float, pad_sentinel: float = -999.0) -> np.ndarray:\n",
    "    if sigma <= 0:\n",
    "        return X\n",
    "    noise = np.random.normal(loc=0.0, scale=sigma, size=X.shape).astype(X.dtype)\n",
    "    if pad_sentinel is not None:\n",
    "        mask = (X == pad_sentinel)\n",
    "        Xn = X + noise\n",
    "        Xn[mask] = pad_sentinel\n",
    "        return Xn\n",
    "    return X + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46801f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inventário de problemas e dados disponíveis\n",
    "problems = list_problem_dirs('data')\n",
    "inventory = []\n",
    "for p in problems:\n",
    "    lin_meta = load_linear_meta(p)\n",
    "    lin_pq = os.path.exists(lin_meta.get('parquet_path', os.path.join(p, 'linear_dataset_test.parquet')))\n",
    "    lstm_meta = os.path.exists(os.path.join(p, 'lstm_dataset_test.meta.json'))\n",
    "    tft_pq = os.path.exists(os.path.join(p, 'tft_dataset_test.parquet'))\n",
    "    n_rows = 0\n",
    "    if lin_pq:\n",
    "        try:\n",
    "            n_rows = int(pd.read_parquet(lin_meta.get('parquet_path', os.path.join(p, 'linear_dataset_test.parquet')), columns=['quantity_MW']).shape[0])\n",
    "        except Exception:\n",
    "            pass\n",
    "    inventory.append({\n",
    "        'problem': os.path.basename(p),\n",
    "        'linear_test_parquet': lin_pq,\n",
    "        'lstm_test_meta': lstm_meta,\n",
    "        'tft_test_parquet': tft_pq,\n",
    "        'linear_meta_ok': bool(lin_meta),\n",
    "        'rows': n_rows\n",
    "    })\n",
    "inv_df = pd.DataFrame(inventory).sort_values('problem').reset_index(drop=True)\n",
    "inv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3020c619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descoberta de modelos Keras (Linear/MLP/LSTM) salvos — opcional\n",
    "# Procura por .keras, .h5 e SavedModel no repositório (exclui venv)\n",
    "search_roots = ['.']\n",
    "keras_models = discover_keras_models(search_roots)\n",
    "print(f'Encontrados {len(keras_models)} possíveis modelos Keras.')\n",
    "for m in keras_models[:10]:\n",
    "    print(' -', m)\n",
    "if len(keras_models) > 10:\n",
    "    print(' ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc21aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação: aplicar modelos Keras em datasets lineares (quando compatíveis)\n",
    "# - Coleta métricas globais e por horizonte\n",
    "# - Inclui baseline de repetição do último valor\n",
    "results = []\n",
    "per_h_results = []\n",
    "\n",
    "search_roots = ['.']\n",
    "keras_models = discover_keras_models(search_roots)\n",
    "print(f'Encontrados {len(keras_models)} possíveis modelos Keras.')\n",
    "for p in problems:\n",
    "    meta = load_linear_meta(p)\n",
    "    if not meta:\n",
    "        continue\n",
    "    df, fcols, tcols = load_linear_parquet(p, meta)\n",
    "    if df.empty or not fcols or not tcols:\n",
    "        continue\n",
    "    # Checagem de tipos: evitar colunas não numéricas\n",
    "    numeric_ok = all(np.issubdtype(df[c].dtype, np.number) for c in fcols)\n",
    "    if not numeric_ok:\n",
    "        print(f\"[SKIP] {os.path.basename(p)}: features incluem colunas não numéricas.\")\n",
    "        continue\n",
    "    X = df[fcols].to_numpy(dtype='float32', copy=False)\n",
    "    Y = df[tcols].to_numpy(dtype='float32', copy=False)\n",
    "    # Subamostra opcional para acelerar\n",
    "    max_rows = 30000\n",
    "    if len(X) > max_rows:\n",
    "        X = X[:max_rows]\n",
    "        Y = Y[:max_rows]\n",
    "\n",
    "    # Baseline (repetir último valor)\n",
    "    Yt, Yb, used_leads = align_predictions(Y, baseline_repeat_last(Y), tcols)\n",
    "    base_overall = evaluate_overall(Yt, Yb)\n",
    "    results.append({\n",
    "        'problem': os.path.basename(p),\n",
    "        'dataset': 'linear_test',\n",
    "        'model_path': 'baseline_repeat_last',\n",
    "        'model_label': 'baseline_repeat_last',\n",
    "        **base_overall\n",
    "    })\n",
    "    ph = evaluate_per_horizon(Yt, Yb)\n",
    "    if not ph.empty:\n",
    "        ph['problem'] = os.path.basename(p)\n",
    "        ph['model_label'] = 'baseline_repeat_last'\n",
    "        per_h_results.append(ph)\n",
    "\n",
    "    # Avaliar modelos descobertos\n",
    "    for model_path in keras_models:\n",
    "        model = safe_load_keras_model(model_path)\n",
    "        if model is None:\n",
    "            continue\n",
    "        # Checagem leve de compatibilidade de entrada\n",
    "        try:\n",
    "            in_shape = getattr(model, 'input_shape', None)\n",
    "        except Exception:\n",
    "            in_shape = None\n",
    "        if in_shape is not None:\n",
    "            # Se múltiplas entradas, deixar passar\n",
    "            if isinstance(in_shape, (list, tuple)) and len(in_shape) and isinstance(in_shape[0], (list, tuple)):\n",
    "                pass\n",
    "            else:\n",
    "                last_dim = in_shape[-1] if isinstance(in_shape, (list, tuple)) else None\n",
    "                if isinstance(last_dim, int) and last_dim > 0 and last_dim != X.shape[1]:\n",
    "                    continue\n",
    "\n",
    "        overall, per_h = evaluate_keras_on_linear(model, X, Y, tcols)\n",
    "        label = model_label_from_path(model_path)\n",
    "        row = {\n",
    "            'problem': os.path.basename(p),\n",
    "            'dataset': 'linear_test',\n",
    "            'model_path': model_path,\n",
    "            'model_label': label,\n",
    "        }\n",
    "        row.update(overall)\n",
    "        results.append(row)\n",
    "        if not per_h.empty:\n",
    "            per_h['problem'] = os.path.basename(p)\n",
    "            per_h['model_label'] = label\n",
    "            per_h_results.append(per_h)\n",
    "\n",
    "res_df = pd.DataFrame(results)\n",
    "per_h_df = pd.concat(per_h_results, ignore_index=True) if per_h_results else pd.DataFrame()\n",
    "\n",
    "if res_df.empty:\n",
    "    print('Nenhum resultado disponível. Verifique se os modelos foram treinados e salvos neste repositório.')\n",
    "else:\n",
    "    display(res_df.sort_values(['problem', 'MAE']).reset_index(drop=True))\n",
    "    if not per_h_df.empty:\n",
    "        display(per_h_df.groupby(['problem', 'model_label'])['MAE'].mean().rename('MAE_mean').reset_index().sort_values(['problem','MAE_mean']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180b5b7a",
   "metadata": {},
   "source": [
    "## Visualização (opcional)\n",
    "Se houver resultados:\n",
    "- Tabela com top-3 modelos por problema (MAE).\n",
    "- Gráfico de barras de MAE por modelo (média por horizonte).\n",
    "- Curva MAE vs. horizonte para um problema selecionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd5854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-3 por problema (MAE)\n",
    "if 'res_df' in globals() and not res_df.empty:\n",
    "    top3 = (res_df\n",
    "            .dropna(subset=['MAE'])\n",
    "            .sort_values(['problem', 'MAE'])\n",
    "            .groupby('problem')\n",
    "            .head(3)\n",
    "            .reset_index(drop=True))\n",
    "    display(top3)\n",
    "else:\n",
    "    print('Sem resultados gerais.')\n",
    "\n",
    "# Barras: MAE médio por modelo (por problema)\n",
    "if 'per_h_df' in globals() and not per_h_df.empty:\n",
    "    agg = (per_h_df\n",
    "           .groupby(['problem','model_label'])['MAE']\n",
    "           .mean()\n",
    "           .rename('MAE_mean')\n",
    "           .reset_index())\n",
    "    for prob in agg['problem'].unique():\n",
    "        sub = agg[agg['problem']==prob].sort_values('MAE_mean')\n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.barh(sub['model_label'], sub['MAE_mean'], color='teal')\n",
    "        plt.xlabel('MAE médio (por horizonte)')\n",
    "        plt.title(f'{prob} — Comparação por modelo')\n",
    "        plt.grid(True, axis='x', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print('Sem métricas por horizonte para gráficos.')\n",
    "\n",
    "# Curva MAE vs horizonte (problema com mais modelos)\n",
    "if 'per_h_df' in globals() and not per_h_df.empty:\n",
    "    # Escolher o problema com mais linhas\n",
    "    prob_counts = per_h_df['problem'].value_counts()\n",
    "    sel_prob = prob_counts.index[0]\n",
    "    sub = per_h_df[per_h_df['problem']==sel_prob]\n",
    "    plt.figure(figsize=(10,4))\n",
    "    for ml, g in sub.groupby('model_label'):\n",
    "        plt.plot(g['h'], g['MAE'], label=ml, alpha=0.8)\n",
    "    plt.xlabel('Horizonte (h)')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.title(f'{sel_prob} — MAE vs Horizonte')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Sem dados suficientes para curva por horizonte.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7487ef46",
   "metadata": {},
   "source": [
    "### Notas\n",
    "- LSTM e TFT: este notebook pode ser estendido para ler TFRecords/parquets específicos e checkpoints (PyTorch Lightning) quando disponíveis.\n",
    "- Para garantir avaliação completa, execute os treinamentos em `Modelos.ipynb` antes e confirme que os artefatos de modelo são salvos dentro do repositório (evitando pastas de ambiente)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b215f35",
   "metadata": {},
   "source": [
    "## Robustez a ruído (opcional)\n",
    "\n",
    "Nesta seção, avaliamos a degradação de erro quando adicionamos ruído gaussiano às entradas X (apenas datasets lineares).\n",
    "Execute após haver pelo menos um modelo avaliado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9de837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação com ruído nas entradas (σ em {0.00, 0.01, 0.03, 0.05, 0.10})\n",
    "if 'problems' in globals() and len(problems) and 'keras_models' in globals():\n",
    "    noise_rows = []\n",
    "    sigmas = [0.00, 0.01, 0.03, 0.05, 0.10]\n",
    "    for p in problems:\n",
    "        meta = load_linear_meta(p)\n",
    "        if not meta:\n",
    "            continue\n",
    "        df, fcols, tcols = load_linear_parquet(p, meta)\n",
    "        if df.empty or not fcols or not tcols:\n",
    "            continue\n",
    "        if not all(np.issubdtype(df[c].dtype, np.number) for c in fcols):\n",
    "            continue\n",
    "        X0 = df[fcols].to_numpy(dtype='float32', copy=False)\n",
    "        Y = df[tcols].to_numpy(dtype='float32', copy=False)\n",
    "        # Subamostra opcional\n",
    "        max_rows = 15000\n",
    "        if len(X0) > max_rows:\n",
    "            X0 = X0[:max_rows]\n",
    "            Y = Y[:max_rows]\n",
    "        # Precisa de ao menos um modelo compatível\n",
    "        any_model = False\n",
    "        for model_path in keras_models:\n",
    "            model = safe_load_keras_model(model_path)\n",
    "            if model is None:\n",
    "                continue\n",
    "            # Checagem leve de compatibilidade\n",
    "            in_shape = getattr(model, 'input_shape', None) if hasattr(model, 'input_shape') else None\n",
    "            if in_shape is not None:\n",
    "                if isinstance(in_shape, (list, tuple)) and len(in_shape) and isinstance(in_shape[0], (list, tuple)):\n",
    "                    pass\n",
    "                else:\n",
    "                    last_dim = in_shape[-1] if isinstance(in_shape, (list, tuple)) else None\n",
    "                    if isinstance(last_dim, int) and last_dim > 0 and last_dim != X0.shape[1]:\n",
    "                        continue\n",
    "            any_model = True\n",
    "            label = model_label_from_path(model_path)\n",
    "            for s in sigmas:\n",
    "                Xn = add_input_noise(X0, sigma=s, pad_sentinel=-999.0)\n",
    "                overall, _ = evaluate_keras_on_linear(model, Xn, Y, tcols)\n",
    "                noise_rows.append({\n",
    "                    'problem': os.path.basename(p),\n",
    "                    'model_label': label,\n",
    "                    'sigma': s,\n",
    "                    **overall\n",
    "                })\n",
    "        if not any_model:\n",
    "            print(f\"[INFO] {os.path.basename(p)}: sem modelo compatível para teste de ruído.\")\n",
    "    noise_df = pd.DataFrame(noise_rows)\n",
    "    if not noise_df.empty:\n",
    "        display(noise_df.sort_values(['problem','model_label','sigma']))\n",
    "        for prob in noise_df['problem'].unique():\n",
    "            sub = noise_df[noise_df['problem']==prob]\n",
    "            plt.figure(figsize=(10,4))\n",
    "            for ml, g in sub.groupby('model_label'):\n",
    "                plt.plot(g['sigma'], g['MAE'], marker='o', label=ml)\n",
    "            plt.title(f'{prob} — MAE vs σ (ruído de entrada)')\n",
    "            plt.xlabel('σ')\n",
    "            plt.ylabel('MAE')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    else:\n",
    "        print('Sem resultados de robustez (nenhum modelo compatível encontrado).')\n",
    "else:\n",
    "    print('Execute a avaliação principal antes (modelos e problemas).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de58dae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exportar resultados (opcional)\n",
    "\n",
    "# Salvar tabelas em data/results/ para consulta posterior\n",
    "out_dir = os.path.join('data', 'results')\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "if 'res_df' in globals() and not res_df.empty:\n",
    "    res_df.to_parquet(os.path.join(out_dir, 'keras_linear_overall.parquet'), index=False)\n",
    "    res_df.to_csv(os.path.join(out_dir, 'keras_linear_overall.csv'), index=False)\n",
    "    print('[OK] Salvo overall em data/results/.')\n",
    "else:\n",
    "    print('[INFO] res_df vazio — nada salvo (overall).')\n",
    "\n",
    "if 'per_h_df' in globals() and not per_h_df.empty:\n",
    "    per_h_df.to_parquet(os.path.join(out_dir, 'keras_linear_per_horizon.parquet'), index=False)\n",
    "    per_h_df.to_csv(os.path.join(out_dir, 'keras_linear_per_horizon.csv'), index=False)\n",
    "    print('[OK] Salvo per_horizon em data/results/.')\n",
    "else:\n",
    "    print('[INFO] per_h_df vazio — nada salvo (per_horizon).')\n",
    "\n",
    "if 'noise_df' in globals() and not noise_df.empty:\n",
    "    noise_df.to_parquet(os.path.join(out_dir, 'keras_linear_noise.parquet'), index=False)\n",
    "    noise_df.to_csv(os.path.join(out_dir, 'keras_linear_noise.csv'), index=False)\n",
    "    print('[OK] Salvo noise em data/results/.')\n",
    "else:\n",
    "    print('[INFO] noise_df vazio — nada salvo (noise).')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

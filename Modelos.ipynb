{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c49e9f73",
   "metadata": {},
   "source": [
    "# Introdu√ß√£o\n",
    "\n",
    "Esse Notebook ser√° respons√°vel pelo preprocessamento dos dados contidos em ./data/raw para formatos compat√≠veis e otimizados para o treinamento de cada modelo\n",
    "As defini√ß√µes dos problemas aos quais os modelos dever√£o solucionar j√° foram definidas no notebook \"Coleta de Dados\"\n",
    "\n",
    "Modelos a serem Criados:\n",
    "\n",
    "1. Modelo Linear: MLP sem fun√ß√µes de ativa√ß√£o, composta apenas de somas lineares\n",
    "2. MLP: rede neural - efetivamente identica ao modelo linear, no entanto, apresenta fun√ß√£o de ativa√ß√£o ao final do somat√≥rio de fun√ß√µes lineares\n",
    "3. LSTM: Um modelo de rede neural recorrente, com capacidade de diferencia√ß√£o de informa√ß√£o de curto e longo prazo\n",
    "4. TFT: Modelo baseado em LLMs desenvolvido pela microsoft - servir√° como um comparativo mais moderno\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1149ce94",
   "metadata": {},
   "source": [
    "## Depend√™ncias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce23fd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch=2.5.1+cu121 | cuda=True\n",
      "Verificando depend√™ncias (pyarrow para Parquet)...\n",
      "PyArrow dispon√≠vel: 22.0.0\n",
      "Instalando python-dotenv...\n",
      "Instalando scikit-learn...\n",
      "Instalando tensorflow[and-cuda]...\n",
      "Depend√™ncias prontas\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "# Depend√™ncias m√≠nimas para TFT ‚Äî simples e com foco em GPU\n",
    "try:\n",
    "    import torch\n",
    "except Exception:\n",
    "    import shutil\n",
    "    has_gpu = shutil.which(\"nvidia-smi\") is not None\n",
    "    if has_gpu:\n",
    "        # tenta instalar com suporte CUDA (ajuste a vers√£o cu de acordo com sua stack, p.ex. cu121)\n",
    "        %pip install -q torch --index-url https://download.pytorch.org/whl/cu121\n",
    "    else:\n",
    "        %pip install -q torch --index-url https://download.pytorch.org/whl/cpu\n",
    "    # libs do pipeline TFT\n",
    "    %pip install -q pytorch-lightning pytorch-forecasting\n",
    "\n",
    "    import torch\n",
    "\n",
    "\n",
    "\n",
    "print(f\"torch={torch.__version__} | cuda={torch.cuda.is_available()}\")\n",
    "\n",
    "print(\"Verificando depend√™ncias (pyarrow para Parquet)...\")\n",
    "\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    print(f\"PyArrow dispon√≠vel: {pa.__version__}\")\n",
    "except Exception:\n",
    "    print(\"Instalando pyarrow...\")\n",
    "    !pip install --upgrade \"pyarrow>=18\" --quiet\n",
    "    import importlib\n",
    "    importlib.invalidate_caches()\n",
    "    import pyarrow as pa\n",
    "    print(f\"PyArrow instalado: {pa.__version__}\")\n",
    "\n",
    "# fastparquet √© opcional\n",
    "try:\n",
    "    import fastparquet  # noqa: F401\n",
    "    print(\"fastparquet dispon√≠vel (opcional)\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Outras bibliotecas sob demanda\n",
    "for lib in [\n",
    "    \"numpy\", \"python-dotenv\", \"pandas\", \"matplotlib\", \"seaborn\",\n",
    "    \"scikit-learn\", \"tensorflow[and-cuda]\", \"keras\", \"lxml\", \"pytz\"\n",
    "]:\n",
    "    try:\n",
    "        __import__(lib)\n",
    "    except ImportError:\n",
    "        print(f\"Instalando {lib}...\")\n",
    "        !pip install {lib} --quiet\n",
    "\n",
    "print(\"Depend√™ncias prontas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cc7e78",
   "metadata": {},
   "source": [
    "## VARI√ÅVEIS NECESS√ÅRIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d89d36dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GPU detected (/physical_device:GPU:0) - using mixed precision.\n"
     ]
    }
   ],
   "source": [
    "# Imports para a API e utilidades\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import mixed_precision\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "# Silenciando Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)  # last resort\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# GPU CONFIGURATION\n",
    "# ==============================================\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    mixed_precision.set_global_policy('mixed_float16')\n",
    "    print(f\"‚úÖ GPU detected ({gpus[0].name}) - using mixed precision.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected, running on CPU.\")\n",
    "\n",
    "# Carregar vari√°veis de ambiente do .env\n",
    "load_dotenv()\n",
    "# ---------------- CONFIG ---------------- #\n",
    "COUNTRY_DOMAINS = {\n",
    "    \"FR\": {\"domain\": \"10YFR-RTE------C\"},\n",
    "    \"ES\": {\"domain\": \"10YES-REE------0\"},\n",
    "    \"PT\": {\"domain\": \"10YPT-REN------W\"}\n",
    "}\n",
    "\n",
    "DATA_ITEMS = [\n",
    "    {'key': 'load_total', 'documentType': 'A65', 'processType': 'A16', 'domainParam': 'outBiddingZone_Domain', 'parser': 'load'},\n",
    "    {'key': 'market_prices', 'documentType': 'A44', 'processType': 'A07', 'domainParamIn': 'in_Domain', 'domainParamOut': 'out_Domain', 'parser': 'price'}\n",
    "]\n",
    "\n",
    "ENTSOE_TOKEN = os.environ.get(\"ENTSOE_SECURITY_TOKEN\")\n",
    "BASE_URL = \"https://web-api.tp.entsoe.eu/api\"\n",
    "MAX_WORKERS = 100\n",
    "RAW_DIR = os.path.join(\"data\", \"raw\")\n",
    "PARQUET_COMPRESSION = \"zstd\"\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "\n",
    "# ==============================================\n",
    "# DICION√ÅRIO DE PROBLEMAS\n",
    "# ==============================================\n",
    "problemas = [\n",
    "    dict(name=\"treinamento\", data_dir=\"data/treinamento\", feats=[\"country\",\"datetime\",\"quantity_MW\"], tgts=[\"quantity_MW\"], lag=10*24, lead=3*24, vals=[\"quantity_MW\"], countries=COUNTRY_DOMAINS.keys()),\n",
    "    dict(name=\"N1A\", data_dir=\"data/N1A\", feats=[\"country\",\"datetime\",\"quantity_MW\"], tgts=[\"quantity_MW\"], lag=10*24, lead=3*24, vals=[\"quantity_MW\"], countries=[\"ES\"], reduced_dim = 3*24, mask_value=-999.0),\n",
    "    dict(name=\"N1B\", data_dir=\"data/N1B\", feats=[\"country\",\"datetime\",\"quantity_MW\"], tgts=[\"quantity_MW\"], lag=10*24, lead=3*24, vals=[\"quantity_MW\"], countries=[\"ES\"], reduced_dim = 7*24, mask_value=-999.0),\n",
    "    dict(name=\"N1C\", data_dir=\"data/N1C\", feats=[\"country\",\"datetime\",\"quantity_MW\"], tgts=[\"quantity_MW\"], lag=10*24, lead=3*24, vals=[\"quantity_MW\"], countries=[\"ES\"]),\n",
    "    dict(name=\"N2A\", data_dir=\"data/N2A\", feats=[\"country\",\"datetime\",\"quantity_MW\"], tgts=[\"quantity_MW\"], lag=10*24, lead=3*24, vals=[\"quantity_MW\"], countries=COUNTRY_DOMAINS.keys(), reduced_dim = 3*24, mask_value=-999.0),\n",
    "    dict(name=\"N2B\", data_dir=\"data/N2B\", feats=[\"country\",\"datetime\",\"quantity_MW\"], tgts=[\"quantity_MW\"], lag=10*24, lead=3*24, vals=[\"quantity_MW\"], countries=COUNTRY_DOMAINS.keys(), reduced_dim = 7*24, mask_value=-999.0),\n",
    "    dict(name=\"N2C\", data_dir=\"data/N2C\", feats=[\"country\",\"datetime\",\"quantity_MW\"], tgts=[\"quantity_MW\"], lag=10*24, lead=3*24, vals=[\"quantity_MW\"], countries=COUNTRY_DOMAINS.keys()),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5b47ef",
   "metadata": {},
   "source": [
    "# Cap√≠tulo 1: Pr√©processamento de dados\n",
    "\n",
    "Etapa de contru√ß√£o da pipelines de pre-processamento de dados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f852c4",
   "metadata": {},
   "source": [
    "## Classe geral de preprocessamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "302b0d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional, List, Tuple\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class Preprocessor:\n",
    "    \"\"\"Pr√©-processador base.\n",
    "\n",
    "    - lag/lead como inteiros s√£o expandidos para ranges [1..N] quando apropriado.\n",
    "    - feature_cols/target_cols definem bases permitidas e servem como sele√ß√£o no export.\n",
    "    - Nenhuma coluna √© removida dos dados; sele√ß√£o ocorre apenas na exporta√ß√£o.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        lag: int,\n",
    "        lead: int,\n",
    "        country_list: Optional[List[str]] = None,\n",
    "        *,\n",
    "        model_name: str = \"linear\",\n",
    "        data_dir: str = \"data/processed\",\n",
    "        feature_cols: Optional[List[str]] = None,\n",
    "        target_cols: Optional[List[str]] = None,\n",
    "    ):\n",
    "        self.lag = lag\n",
    "        self.lead = lead\n",
    "        self.country_list = country_list\n",
    "        self.model_name = model_name\n",
    "        self.data_dir = data_dir\n",
    "        self.save_dir = self.data_dir\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "        self.feature_cols: List[str] = list(feature_cols) if feature_cols else []\n",
    "        self.target_cols: List[str] = list(target_cols) if target_cols else []\n",
    "\n",
    "        self.norm_objects = {}\n",
    "        self.encod_objects = {}\n",
    "        self.df_base = pd.DataFrame()\n",
    "\n",
    "    def _expand_steps(self, steps, default_max: Optional[int]) -> List[int]:\n",
    "        \"\"\"Normaliza passos: int‚Üí[1..N], None‚Üí[1..default_max], lista‚Üícomo est√°.\"\"\"\n",
    "        if isinstance(steps, int):\n",
    "            return list(range(1, steps + 1)) if steps > 0 else [1]\n",
    "        if steps is None and isinstance(default_max, int) and default_max > 0:\n",
    "            return list(range(1, default_max + 1))\n",
    "        if isinstance(steps, (list, tuple)):\n",
    "            return list(steps)\n",
    "        return [1]\n",
    "\n",
    "    def load_data(self, raw_dir: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"Carrega Parquet unificado em data/raw (ou raw_dir) e atualiza self.df_base.\"\"\"\n",
    "        base_raw = raw_dir or os.path.join('data', 'raw')\n",
    "        unified_path = os.path.join(base_raw, f'raw_dataset.parquet')\n",
    "        if not os.path.exists(unified_path):\n",
    "            raise FileNotFoundError(f\"Arquivo unificado n√£o encontrado: {unified_path}. Execute a coleta primeiro.\")\n",
    "        df = pd.read_parquet(unified_path, engine='pyarrow')\n",
    "        if 'datetime' in df.columns:\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "        if self.country_list and 'country' in df.columns:\n",
    "            df = df[df['country'].isin(self.country_list)].copy()\n",
    "        sort_cols = [c for c in ['country', 'datetime'] if c in df.columns]\n",
    "        if sort_cols:\n",
    "            df = df.sort_values(sort_cols).reset_index(drop=True)\n",
    "            \n",
    "        # Filtrando Colunas apenas para as necess√°rias\n",
    "        cols = list(set([c for c in self.feature_cols + self.target_cols if c in df.columns]))\n",
    "        df = df.loc[:, ~df.columns.duplicated()]  # optional: remove duplicates\n",
    "        df = df[cols]\n",
    "\n",
    "        self.df_base = df\n",
    "        return self.df_base\n",
    "\n",
    "    def encode(self, encode_cols: str = 'datetime', encode_method: str = 'label') -> pd.DataFrame:\n",
    "        \"\"\"Codifica de forma n√£o destrutiva e atualiza self.df_base.\n",
    "\n",
    "        - label: usa LabelEncoder com suporte a NaN via placeholder interno que √© revertido no decode.\n",
    "        - time_cycle: adiciona features de calend√°rio e c√≠clicas sem remover datetime.\n",
    "        \"\"\"\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            print(\"df_base vazio. Chame load_data() primeiro.\")\n",
    "            return self.df_base\n",
    "        df = self.df_base.copy()\n",
    "        if encode_method == 'label':\n",
    "            le = LabelEncoder()\n",
    "            s = df[encode_cols].astype(object)\n",
    "            le.fit(s)\n",
    "            df[encode_cols] = le.transform(s)\n",
    "            # salva metadados incluindo o code do NaN\n",
    "            self.encod_objects['label'] = {\n",
    "                'encode_cols': encode_cols,\n",
    "                'label_encoder': le,\n",
    "            }\n",
    "        elif encode_method == 'time_cycle':\n",
    "            if encode_cols not in df.columns:\n",
    "                print(f\"Coluna {encode_cols} n√£o encontrada para time_cycle.\")\n",
    "                self.df_base = df\n",
    "                return df\n",
    "            dt = pd.to_datetime(df[encode_cols], utc=True)\n",
    "            # Mant√©m a coluna original e adiciona componentes discretos e c√≠clicos\n",
    "            df['year'] = dt.dt.year\n",
    "            df['month'] = dt.dt.month\n",
    "            df['day'] = dt.dt.day\n",
    "            df['hour'] = dt.dt.hour\n",
    "            df['minute'] = dt.dt.minute\n",
    "            current_year = time.localtime().tm_year\n",
    "            df['year_sin'] = np.sin(2 * np.pi * df['year'] / max(current_year, 1))\n",
    "            df['year_cos'] = np.cos(2 * np.pi * df['year'] / max(current_year, 1))\n",
    "            df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "            df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "            df['day_sin'] = np.sin(2 * np.pi * df['day'] / 31)\n",
    "            df['day_cos'] = np.cos(2 * np.pi * df['day'] / 31)\n",
    "            df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "            df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "            df['minute_sin'] = np.sin(2 * np.pi * df['minute'] / 60)\n",
    "            df['minute_cos'] = np.cos(2 * np.pi * df['minute'] / 60)\n",
    "            self.encod_objects['time_cycle'] = {'encode_cols': encode_cols}\n",
    "            self.feature_cols.extend([\"year_sin\", \"year_cos\",\n",
    "                                                     \"month_sin\", \"month_cos\",\n",
    "                                                     \"day_sin\", \"day_cos\",\n",
    "                                                     \"hour_sin\", \"hour_cos\",\n",
    "                                                     \"minute_sin\", \"minute_cos\"])\n",
    "        else:\n",
    "            print(f\"encode_method '{encode_method}' n√£o suportado.\")\n",
    "        self.df_base = df\n",
    "        return self.df_base\n",
    "\n",
    "    def decode(self, encode_method: str = 'label', target_col: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"Reverte codifica√ß√µes suportadas (label, time_cycle).\"\"\"\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            print(\"df_base vazio. Nada para decodificar.\")\n",
    "            return self.df_base\n",
    "        df = self.df_base.copy()\n",
    "        if encode_method == 'label':\n",
    "            info = self.encod_objects.get('label')\n",
    "            if not info:\n",
    "                print(\"Nenhuma informa√ß√£o de label encoding salva.\")\n",
    "                return self.df_base\n",
    "            col = info['encode_cols']\n",
    "            le: LabelEncoder = info['label_encoder']\n",
    "            placeholder = info.get('na_placeholder', '__NA__')\n",
    "            try:\n",
    "                inv = le.inverse_transform(df[col].astype(int))\n",
    "                # mapeia placeholder de volta para NaN\n",
    "                inv = pd.Series(inv).replace(placeholder, np.nan).values\n",
    "                df[col] = inv\n",
    "            except Exception as e:\n",
    "                print(f\"Falha ao decodificar label para coluna {col}: {e}\")\n",
    "        elif encode_method == 'time_cycle':\n",
    "            if 'year' not in df.columns:\n",
    "                print(\"Componentes de tempo ausentes para reconstru√ß√£o.\")\n",
    "                return self.df_base\n",
    "            tgt = target_col or 'decoded_datetime'\n",
    "            def _recover_component(sin_col, cos_col, period, offset):\n",
    "                if sin_col not in df.columns or cos_col not in df.columns:\n",
    "                    return pd.Series([np.nan] * len(df))\n",
    "                ang = np.arctan2(df[sin_col], df[cos_col])\n",
    "                ang = (ang + 2 * np.pi) % (2 * np.pi)\n",
    "                idx = np.round((ang / (2 * np.pi)) * period).astype('Int64') % period\n",
    "                return idx + offset\n",
    "            month = _recover_component('month_sin', 'month_cos', 12, 1)\n",
    "            day = _recover_component('day_sin', 'day_cos', 31, 1)\n",
    "            hour = _recover_component('hour_sin', 'hour_cos', 24, 0)\n",
    "            minute = _recover_component('minute_sin', 'minute_cos', 60, 0)\n",
    "            year = df['year'] if 'year' in df.columns else pd.Series([np.nan] * len(df))\n",
    "            dt = pd.to_datetime({\n",
    "                'year': year.astype('Int64'),\n",
    "                'month': month.astype('Int64'),\n",
    "                'day': day.astype('Int64'),\n",
    "                'hour': hour.astype('Int64'),\n",
    "                'minute': minute.astype('Int64'),\n",
    "            }, errors='coerce', utc=True)\n",
    "            df[tgt] = dt\n",
    "        else:\n",
    "            print(f\"encode_method '{encode_method}' n√£o suportado para decode.\")\n",
    "        self.df_base = df\n",
    "        return self.df_base\n",
    "\n",
    "    def normalize(self, value_cols: List[str], normalization_method: str = 'minmax') -> pd.DataFrame:\n",
    "        \"\"\"Normaliza colunas e atualiza self.df_base.\"\"\"\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            print(\"df_base vazio. Chame load_data() primeiro.\")\n",
    "            return self.df_base\n",
    "        df = self.df_base.copy()\n",
    "        scaler = MinMaxScaler() if normalization_method == 'minmax' else (\n",
    "            StandardScaler() if normalization_method == 'standard' else None)\n",
    "        if scaler is None:\n",
    "            raise ValueError(\"normalization_method deve ser 'minmax' ou 'standard'\")\n",
    "        df[value_cols] = scaler.fit_transform(df[value_cols])\n",
    "        self.norm_objects[normalization_method] = {'value_cols': value_cols, 'scaler': scaler}\n",
    "        self.df_base = df\n",
    "        return self.df_base\n",
    "\n",
    "    def normalize_splits(self, value_cols: List[str], normalization_method: str = 'minmax') -> dict:\n",
    "        \"\"\"Normaliza os conjuntos de treino, valida√ß√£o e teste.\"\"\"\n",
    "        if not self.splits:\n",
    "            print(\"Nenhum conjunto dividido encontrado.\")\n",
    "            return {}\n",
    "        normalized_splits = {}\n",
    "        for split_name, split_df in self.splits.items():\n",
    "            self.df_base = split_df\n",
    "            normalized_df = self.normalize(value_cols=value_cols, normalization_method=normalization_method)\n",
    "            normalized_splits[split_name] = normalized_df\n",
    "        self.splits = normalized_splits\n",
    "        return normalized_splits\n",
    "\n",
    "    def denormalize(self, normalization_method: str = 'minmax') -> pd.DataFrame:\n",
    "        \"\"\"Reverte normaliza√ß√£o usando metadados salvos.\"\"\"\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            print(\"df_base vazio. Nada para denormalizar.\")\n",
    "            return self.df_base\n",
    "        info = self.norm_objects.get(normalization_method)\n",
    "        if not info:\n",
    "            print(f\"Nenhum scaler salvo para o m√©todo '{normalization_method}'.\")\n",
    "            return self.df_base\n",
    "        cols: List[str] = info['value_cols']\n",
    "        scaler = info['scaler']\n",
    "        df = self.df_base.copy()\n",
    "        try:\n",
    "            df[cols] = scaler.inverse_transform(df[cols])\n",
    "        except Exception as e:\n",
    "            print(f\"Falha ao denormalizar colunas {cols}: {e}\")\n",
    "            return self.df_base\n",
    "        self.df_base = df\n",
    "        return self.df_base\n",
    "\n",
    "    def save_df_base(self, filename: Optional[str] = None, compression: Optional[str] = None, partition_by: Optional[List[str]] = None) -> Optional[str]:\n",
    "        \"\"\"Salva self.df_base em Parquet dentro de data_dir/{model_name}.\"\"\"\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            print(\"df_base vazio. Nada para salvar.\")\n",
    "            return None\n",
    "        comp = compression\n",
    "        if comp is None:\n",
    "            try:\n",
    "                comp = PARQUET_COMPRESSION\n",
    "            except NameError:\n",
    "                comp = 'zstd'\n",
    "        filename = \"raw_dataset.parquet\"\n",
    "        out_path = os.path.join(self.save_dir, filename)\n",
    "        df = self.df_base.copy()\n",
    "        if 'datetime' in df.columns:\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "        try:\n",
    "            if partition_by:\n",
    "                df.to_parquet(out_path, engine='pyarrow', compression=comp, index=False, partition_cols=partition_by)\n",
    "            else:\n",
    "                df.to_parquet(out_path, engine='pyarrow', compression=comp, index=False)\n",
    "            print(f\"[SALVO] df_base: {len(df):,} linhas ‚Üí {out_path}\")\n",
    "            return out_path\n",
    "        except Exception as e:\n",
    "            print(f\"Falha ao salvar df_base em {out_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def split_train_val_test(self, train_size: float = 0.7, val_size: float = 0.15, test_size: float = 0.15, time_col: str = 'datetime') -> Optional[dict]:\n",
    "        \"\"\"Divide df_base em conjuntos de treino, valida√ß√£o e teste com base em time_col.\"\"\"\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            print(\"df_base vazio. Nada para dividir.\")\n",
    "            return None\n",
    "        if not np.isclose(train_size + val_size + test_size, 1.0):\n",
    "            print(\"train_size, val_size e test_size devem somar 1.0\")\n",
    "            return None\n",
    "        df = self.df_base.copy()\n",
    "        if time_col not in df.columns:\n",
    "            print(f\"Coluna de tempo '{time_col}' n√£o encontrada em df_base.\")\n",
    "            return None\n",
    "        df = df.sort_values(time_col).reset_index(drop=True)\n",
    "        n = len(df)\n",
    "        train_end = int(n * train_size)\n",
    "        val_end = train_end + int(n * val_size)\n",
    "        splits = {\n",
    "            'train': df.iloc[:train_end].reset_index(drop=True),\n",
    "            'val': df.iloc[train_end:val_end].reset_index(drop=True),\n",
    "            'test': df.iloc[val_end:].reset_index(drop=True),\n",
    "        }\n",
    "        for split_name, split_df in splits.items():\n",
    "            print(f\"[DIVIDIDO] {split_name}: {len(split_df):,} linhas\")\n",
    "        self.splits = splits\n",
    "        return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa7c121",
   "metadata": {},
   "source": [
    "## Preprocessamento do Modelo Linear\n",
    "\n",
    "Esse modelo deve ser√° contruido a partir de lags e leads passados como par√¢metros na fun√ß√£o, resultando na contru√ß√£o de novas colunas lead lag, assim gerando uma flat matrix 2D que ser√° usada no modelo linear\n",
    "\n",
    "Observa√ß√£o importante: lag e lead s√£o inteiros e representam o m√°ximo de passos; o pipeline expande para intervalos 1..N automaticamente. Por exemplo, lag=96 gera features com defasagens de 1 a 96; lead=96 gera alvos de 1 a 96.\n",
    "\n",
    "Os arquivos do modelo ser√£o salvos em Parquet, j√° que o modelo linear ser√° constru√≠do usando TensorFlow (carregado via Parquet‚ÜíNumPy‚Üítf.data)\n",
    "\n",
    "No caso o Preprocessador do modelo linear ser√° igual ao pr√©-processador do MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75dd3ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearPreprocessor(Preprocessor):\n",
    "    \"\"\"Pr√©-processador linear: gera matriz flat (lags/leads) e exporta apenas Parquet.\"\"\"\n",
    "\n",
    "    def build_flat_matrix(\n",
    "        self,\n",
    "        value_cols: Optional[List[str]] = None,\n",
    "        target_cols: Optional[List[str]] = None,\n",
    "        lags: Optional[int] = None,\n",
    "        leads: Optional[int] = None,\n",
    "        reduced_dim: Optional[int] = None,\n",
    "        mask_value: float = 0.0,\n",
    "        dropna: bool = True,\n",
    "        group_cols: Optional[List[str]] = None,\n",
    "        time_col: str = \"datetime\",\n",
    "    ) -> pd.DataFrame:\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            print(\"df_base vazio. Chame load_data() primeiro.\")\n",
    "            return self.df_base\n",
    "\n",
    "        df = self.df_base.copy()\n",
    "        feats = value_cols or self.feature_cols\n",
    "        tgts = target_cols or self.target_cols\n",
    "        if not feats:\n",
    "            raise ValueError(\"Nenhuma coluna de feature informada.\")\n",
    "        if not tgts:\n",
    "            raise ValueError(\"Nenhum target informado.\")\n",
    "\n",
    "        group_cols = group_cols or [c for c in [\"country\"] if c in df.columns]\n",
    "        if time_col not in df.columns:\n",
    "            raise ValueError(f\"Coluna temporal '{time_col}' n√£o encontrada no DataFrame.\")\n",
    "\n",
    "        # Ordena\n",
    "        sort_cols = (group_cols or []) + [time_col]\n",
    "        df = df.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "        if group_cols:\n",
    "            df[\"_group_id\"] = df[group_cols].astype(str).agg(\"_\".join, axis=1)\n",
    "        else:\n",
    "            df[\"_group_id\"] = \"global\"\n",
    "\n",
    "        lag_steps = list(range(1, (lags or self.lag or 0) + 1))\n",
    "        lead_steps = list(range(1, (leads or self.lead or 0) + 1))\n",
    "        new_cols = []\n",
    "\n",
    "        # Determina redu√ß√£o\n",
    "        if reduced_dim is not None and reduced_dim < len(lag_steps):\n",
    "            active_lags = lag_steps[:reduced_dim]\n",
    "            padded_lags = lag_steps[reduced_dim:]\n",
    "            print(f\"[‚ÑπÔ∏è] Reduzindo lags: usando {len(active_lags)} e mascarando {len(padded_lags)} restantes com {mask_value}\")\n",
    "        else:\n",
    "            active_lags = lag_steps\n",
    "            padded_lags = []\n",
    "\n",
    "        # ---- Lags ----\n",
    "        for col in feats:\n",
    "            if col not in df.columns:\n",
    "                print(f\"[WARN] Coluna de feature '{col}' n√£o encontrada.\")\n",
    "                continue\n",
    "            # Lags ativos\n",
    "            for k in active_lags:\n",
    "                cname = f\"{col}_lag{k}\"\n",
    "                df[cname] = df.groupby(\"_group_id\", group_keys=False, sort=False)[col].shift(k)\n",
    "                new_cols.append(cname)\n",
    "            # Lags mascarados (padding)\n",
    "            for k in padded_lags:\n",
    "                cname = f\"{col}_lag{k}\"\n",
    "                df[cname] = mask_value\n",
    "                new_cols.append(cname)\n",
    "\n",
    "        # ---- Leads ----\n",
    "        for tgt in tgts:\n",
    "            if tgt in df.columns:\n",
    "                for k in lead_steps:\n",
    "                    cname = f\"{tgt}_lead{k}\"\n",
    "                    df[cname] = df.groupby(\"_group_id\", group_keys=False, sort=False)[tgt].shift(-k)\n",
    "                    new_cols.append(cname)\n",
    "            else:\n",
    "                print(f\"[WARN] Target '{tgt}' n√£o encontrado. Ignorando leads.\")\n",
    "\n",
    "        # ---- Drop NA ----\n",
    "        if dropna and new_cols:\n",
    "            df = df.dropna(subset=[c for c in new_cols if mask_value not in df[c].unique()]).reset_index(drop=True)\n",
    "\n",
    "        df.drop(columns=[\"_group_id\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "        # Atualiza atributos\n",
    "        self.df_base = df\n",
    "        self.feature_cols.extend([c for c in new_cols if \"_lag\" in c and c not in self.feature_cols])\n",
    "        self.target_cols.extend([c for c in new_cols if \"_lead\" in c and c not in self.target_cols])\n",
    "\n",
    "        return self.df_base\n",
    "\n",
    "    def build_flat_matrices_splits(self, *args, **kwargs) -> Optional[dict]:\n",
    "        \"\"\"Constr√≥i matrizes flat para cada split (train/val/test).\"\"\"\n",
    "        if not self.splits:\n",
    "            print(\"Nenhum conjunto dividido encontrado.\")\n",
    "            return None\n",
    "        built_splits = {}\n",
    "        for split_name, split_df in self.splits.items():\n",
    "            self.df_base = split_df\n",
    "            built_df = self.build_flat_matrix(*args, **kwargs)\n",
    "            built_splits[split_name] = built_df\n",
    "        self.splits = built_splits\n",
    "        return built_splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e82e82",
   "metadata": {},
   "source": [
    "## Preprocessamento do Modelo LSTM\n",
    "\n",
    "O preprocessador do LSTM deve ser capaz de gerar tensores de dimens√£o 3, no seguinte formato (n_batch, seq_len, features) e (n_batch, seq_len, features)  para alimenta√ß√£o do modelo e valida√ß√£o das m√©tricas do modelo\n",
    "\n",
    "Os arquivos do modelo ser√£o salvos em Parquet (colunas X e Y como listas fixas), e carregados via Parquet‚ÜíNumPy‚Üítf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c42af4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "\n",
    "class LSTMPreprocessor(Preprocessor):\n",
    "    \"\"\"Pr√©-processador sequencial para LSTM: gera janelas 3D (N, seq_len, features) e Y (N, lead, targets).\"\"\"\n",
    "\n",
    "    # =====================================================\n",
    "    # BUILD MATRIX\n",
    "    # =====================================================\n",
    "    def build_sequence_matrix(\n",
    "        self,\n",
    "        value_cols: Optional[List[str]] = None,\n",
    "        target_cols: Optional[List[str]] = None,\n",
    "        seq_len: Optional[int] = None,\n",
    "        lead: Optional[int] = None,\n",
    "        group_cols: Optional[List[str]] = None,\n",
    "        time_col: str = \"datetime\",\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Constr√≥i tensores X (entradas) e Y (alvos) para modelo LSTM multivariado.\n",
    "        Agora suporta m√∫ltiplos passos de previs√£o (lead > 1).\n",
    "        \"\"\"\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            raise ValueError(\"df_base vazio. Chame load_data() primeiro.\")\n",
    "\n",
    "        df = self.df_base.copy()\n",
    "        feats = value_cols or self.feature_cols\n",
    "        tgts = target_cols or self.target_cols\n",
    "        if not feats:\n",
    "            raise ValueError(\"Nenhuma coluna de feature informada.\")\n",
    "        if not tgts:\n",
    "            raise ValueError(\"Nenhum target informado.\")\n",
    "        if time_col not in df.columns:\n",
    "            raise ValueError(f\"Coluna temporal '{time_col}' n√£o encontrada.\")\n",
    "\n",
    "        group_cols = group_cols or [c for c in [\"country\"] if c in df.columns]\n",
    "        sort_cols = (group_cols or []) + [time_col]\n",
    "        df = df.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "        if group_cols:\n",
    "            df[\"_group_id\"] = df[group_cols].astype(str).agg(\"_\".join, axis=1)\n",
    "        else:\n",
    "            df[\"_group_id\"] = \"global\"\n",
    "\n",
    "        seq_len = seq_len or getattr(self, \"seq_len\", 24)\n",
    "        lead = lead or getattr(self, \"lead\", 1)\n",
    "\n",
    "        X_list, Y_list = [], []\n",
    "        for gid, g in df.groupby(\"_group_id\", sort=False):\n",
    "            g = g.reset_index(drop=True)\n",
    "            if len(g) < seq_len + lead:\n",
    "                continue\n",
    "\n",
    "            X_src = g[feats].to_numpy(np.float32)\n",
    "            Y_src = g[tgts].to_numpy(np.float32)\n",
    "\n",
    "            # cria janelas deslizantes\n",
    "            for i in range(len(g) - seq_len - lead + 1):\n",
    "                x_win = X_src[i : i + seq_len]\n",
    "                y_seq = Y_src[i + seq_len : i + seq_len + lead]  # <--- multi-step\n",
    "                X_list.append(x_win)\n",
    "                Y_list.append(y_seq)\n",
    "\n",
    "        if not X_list:\n",
    "            print(\"[WARN] Nenhuma janela gerada.\")\n",
    "            return {}\n",
    "\n",
    "        X = np.stack(X_list)  # (N, seq_len, x_dim)\n",
    "        Y = np.stack(Y_list)  # (N, lead, y_dim)\n",
    "        print(f\"[JANELAS] X={X.shape}, Y={Y.shape}, seq_len={seq_len}, lead={lead}\")\n",
    "        self._seq_data = dict(\n",
    "            X=X, Y=Y, seq_len=seq_len, lead=lead, x_dim=X.shape[-1], y_dim=Y.shape[-1]\n",
    "        )\n",
    "        return self._seq_data\n",
    "\n",
    "    # =====================================================\n",
    "    # SAVE PARQUET (streaming por splits)\n",
    "    # =====================================================\n",
    "    def save_splits_parquet(self, basename: str = \"lstm_dataset\", chunk_windows: int = 10_000, verbose: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Gera janelas (seq_len -> lead) diretamente a partir de self.splits e salva Parquet por split usando\n",
    "        pyarrow ParquetWriter com FixedSizeList para evitar picos de mem√≥ria.\n",
    "        \"\"\"\n",
    "        import pyarrow as pa\n",
    "        import pyarrow.parquet as pq\n",
    "\n",
    "        def _uniq(seq):\n",
    "            seen = set()\n",
    "            out = []\n",
    "            for x in seq:\n",
    "                if x not in seen:\n",
    "                    out.append(x)\n",
    "                    seen.add(x)\n",
    "            return out\n",
    "\n",
    "        if not getattr(self, 'splits', None):\n",
    "            raise ValueError(\"Nenhum split encontrado. Execute split_train_val_test() antes.\")\n",
    "\n",
    "        # Resolve seq_len/lead a partir de atributos do preproc\n",
    "        seq_len_attr = getattr(self, 'seq_len', None)\n",
    "        if seq_len_attr is None:\n",
    "            seq_len_attr = getattr(self, 'lag', None)\n",
    "        if seq_len_attr is None:\n",
    "            raise AttributeError(\"Pr√©-processador n√£o possui atributos 'seq_len' ou 'lag'.\")\n",
    "        seq_len = int(seq_len_attr)\n",
    "\n",
    "        lead_attr = getattr(self, 'lead', None)\n",
    "        if lead_attr is None:\n",
    "            lead_attr = getattr(self, 'horizon', None)\n",
    "        if lead_attr is None:\n",
    "            raise AttributeError(\"Pr√©-processador n√£o possui atributo 'lead' (ou 'horizon').\")\n",
    "        lead = int(lead_attr)\n",
    "\n",
    "        out: Dict[str, Any] = {}\n",
    "        for split_name, df in self.splits.items():\n",
    "            # Sele√ß√£o e de-duplica√ß√£o de colunas\n",
    "            fraw = [c for c in self.feature_cols if c in df.columns]\n",
    "            traw = [c for c in self.target_cols if c in df.columns]\n",
    "            tcols = _uniq(traw)\n",
    "            fcols = [c for c in _uniq(fraw) if c not in set(tcols)]\n",
    "\n",
    "            x_dim = len(fcols)\n",
    "            y_dim = len(tcols)\n",
    "            Xf = df[fcols].to_numpy(dtype=np.float32)\n",
    "            Yf = df[tcols].to_numpy(dtype=np.float32)\n",
    "            n = len(df)\n",
    "            n_windows = max(0, n - seq_len - lead + 1)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"[LSTM:{split_name}] linhas={n:,}  windows={n_windows:,}  X=({seq_len},{x_dim})  Y=({lead},{y_dim})  chunk={chunk_windows:,}\")\n",
    "\n",
    "            os.makedirs(self.data_dir, exist_ok=True)\n",
    "            pq_path = os.path.join(self.data_dir, f\"{basename}_{split_name}.parquet\")\n",
    "\n",
    "            fx = seq_len * x_dim\n",
    "            fy = lead * y_dim\n",
    "            schema = pa.schema([\n",
    "                pa.field(\"start_idx\", pa.int32()),\n",
    "                pa.field(\"X\", pa.list_(pa.float32(), fx)),\n",
    "                pa.field(\"Y\", pa.list_(pa.float32(), fy)),\n",
    "            ])\n",
    "            writer = pq.ParquetWriter(pq_path, schema, compression=\"snappy\")\n",
    "\n",
    "            try:\n",
    "                for s in range(0, n_windows, chunk_windows):\n",
    "                    e = min(s + chunk_windows, n_windows)\n",
    "                    csize = e - s\n",
    "                    X_chunk = np.empty((csize, fx), dtype=np.float32)\n",
    "                    Y_chunk = np.empty((csize, fy), dtype=np.float32)\n",
    "                    starts = np.arange(s, e, dtype=np.int32)\n",
    "                    for i, start in enumerate(starts):\n",
    "                        xw = Xf[start:start+seq_len]\n",
    "                        yw = Yf[start+seq_len:start+seq_len+lead]\n",
    "                        X_chunk[i, :] = xw.reshape(-1)\n",
    "                        Y_chunk[i, :] = yw.reshape(-1)\n",
    "                    X_values = pa.array(X_chunk.reshape(-1), type=pa.float32())\n",
    "                    Y_values = pa.array(Y_chunk.reshape(-1), type=pa.float32())\n",
    "                    X_arr = pa.FixedSizeListArray.from_arrays(X_values, fx)\n",
    "                    Y_arr = pa.FixedSizeListArray.from_arrays(Y_values, fy)\n",
    "                    table = pa.Table.from_arrays([pa.array(starts), X_arr, Y_arr], names=[\"start_idx\", \"X\", \"Y\"])\n",
    "                    writer.write_table(table)\n",
    "            finally:\n",
    "                writer.close()\n",
    "\n",
    "            meta = {\n",
    "                \"seq_len\": seq_len,\n",
    "                \"lead\": lead,\n",
    "                \"x_dim\": x_dim,\n",
    "                \"y_dim\": y_dim,\n",
    "                \"feature_cols\": fcols,\n",
    "                \"target_cols\": tcols,\n",
    "                \"parquet_path\": pq_path,\n",
    "                \"basename\": f\"{basename}_{split_name}\",\n",
    "            }\n",
    "            meta_path = os.path.join(self.data_dir, f\"{basename}_{split_name}.meta.json\")\n",
    "            with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "            out[split_name] = {\"path\": pq_path, \"meta\": meta}\n",
    "            if verbose:\n",
    "                print(f\"[üíæ] Parquet LSTM salvo: {pq_path}\")\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd309dd",
   "metadata": {},
   "source": [
    "## Preprocessamento do Modelo TFT (PyTorch)\n",
    "\n",
    "O preprocessador do LSTM deve ser capaz de gerar tensores de dimens√£o 3, no seguinte formato (n_batch, seq_len, features) e (n_batch, seq_len, features)  para alimenta√ß√£o do modelo e valida√ß√£o das m√©tricas do modelo\n",
    "\n",
    "Os artefatos de dados ser√£o salvos em Parquet (Keras e TFT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a0a89f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "class TFTPreprocessor(Preprocessor):\n",
    "    \"\"\"\n",
    "    Preprocessador espec√≠fico para o modelo Temporal Fusion Transformer (PyTorch Forecasting).\n",
    "    Herdando de Preprocessor, apenas adiciona a etapa final de estrutura√ß√£o e salvamento\n",
    "    dos splits no formato compat√≠vel com o PyTorch Forecasting.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        model_name: str,\n",
    "        feature_cols: List[str],\n",
    "        target_cols: List[str],\n",
    "        country_list: List[str],\n",
    "        seq_len: int,\n",
    "        lead: int,\n",
    "    ):\n",
    "        # Corrigido: alinhar com assinatura de Preprocessor\n",
    "        super().__init__(\n",
    "            lag=seq_len,\n",
    "            lead=lead,\n",
    "            country_list=country_list,\n",
    "            model_name=model_name,\n",
    "            data_dir=data_dir,\n",
    "            feature_cols=feature_cols,\n",
    "            target_cols=target_cols,\n",
    "        )\n",
    "        self.seq_len = seq_len\n",
    "        self.lead = lead\n",
    "\n",
    "\n",
    "    def build_tft_parquets(\n",
    "        self,\n",
    "        group_cols: Optional[List[str]] = [\"country\"],\n",
    "        time_col: str = \"datetime\",\n",
    "        dropna: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Estrutura os splits existentes (j√° criados na classe-base) para uso no TFT e salva em parquet.\n",
    "        Simples e direto:\n",
    "        - Ordena por (group_cols + time_col)\n",
    "        - Opcionalmente remove nulos nas colunas cr√≠ticas [time_col] + group_cols + target_cols\n",
    "        - Define '_group_id' e calcula 'time_idx' por grupo via cumcount() (0..N-1 por s√©rie)\n",
    "        - Salva parquet por split\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"splits\") or not self.splits:\n",
    "            raise ValueError(\"Os splits ainda n√£o foram gerados. Execute split_train_val_test() primeiro.\")\n",
    "\n",
    "        for name, df in self.splits.items():\n",
    "            df = df.copy()\n",
    "            # tipos e ordena√ß√£o\n",
    "            df[time_col] = pd.to_datetime(df[time_col], utc=True)\n",
    "            sort_cols = (group_cols or []) + [time_col]\n",
    "            df = df.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "            # drop nulos b√°sico\n",
    "            if dropna:\n",
    "                subset_cols = ([time_col] if time_col else []) + (group_cols or []) + (self.target_cols or [])\n",
    "                present = [c for c in subset_cols if c in df.columns]\n",
    "                before = len(df)\n",
    "                df = df.dropna(subset=present).reset_index(drop=True)\n",
    "                if before - len(df) > 0:\n",
    "                    print(f\"üßπ Drop NA ({name}): {before - len(df)} linhas removidas nas colunas {present}.\")\n",
    "\n",
    "            # id de grupo e time_idx por grupo\n",
    "            if group_cols:\n",
    "                df[\"_group_id\"] = df[group_cols].astype(str).agg(\"_\".join, axis=1)\n",
    "            else:\n",
    "                df[\"_group_id\"] = \"global\"\n",
    "\n",
    "            # contador sequencial por grupo (n√£o global)\n",
    "            df[\"time_idx\"] = df.groupby(\"_group_id\").cumcount().astype(\"int64\")\n",
    "\n",
    "            # salvar parquet\n",
    "            path = os.path.join(self.data_dir, f\"tft_dataset_{name}.parquet\")\n",
    "            df.to_parquet(path, index=False)\n",
    "            print(f\"üíæ Split '{name}' salvo em {path} ({df.shape[0]} linhas, grupos={df['_group_id'].nunique()}, max local time_idx={df.groupby('_group_id')['time_idx'].max().max()}).\")\n",
    "\n",
    "\n",
    "    def load_tft_dataset(\n",
    "        self,\n",
    "        split_name: str,\n",
    "        target_col: str,\n",
    "        known_reals: Optional[List[str]] = None,\n",
    "        return_df: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Carrega o parquet salvo como DataFrame ou cria um TimeSeriesDataSet compat√≠vel com o TFT PyTorch.\n",
    "\n",
    "        Args:\n",
    "            split_name: 'train' | 'val' | 'test' (parte do nome do arquivo parquet gerado)\n",
    "            target_col: coluna alvo principal (string)\n",
    "            known_reals: lista de features conhecidas no tempo (overrides self.feature_cols quando fornecida)\n",
    "            return_df: se True retorna o DataFrame bruto em vez do TimeSeriesDataSet\n",
    "\n",
    "        Retorna:\n",
    "            DataFrame (quando return_df=True) ou TimeSeriesDataSet\n",
    "        \"\"\"\n",
    "        path = os.path.join(self.data_dir, f\"tft_dataset_{split_name}.parquet\")\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"Arquivo n√£o encontrado: {path}\")\n",
    "\n",
    "        df = pd.read_parquet(path)\n",
    "\n",
    "        if return_df:\n",
    "            print(f\"üì• Parquet '{split_name}' carregado ({len(df)} linhas) ‚Äî retornando DataFrame.\")\n",
    "            return df\n",
    "\n",
    "        # determina known/unknown reals\n",
    "        known_reals = known_reals or [c for c in (self.feature_cols or []) if c not in (self.target_cols or [])]\n",
    "\n",
    "        ds = TimeSeriesDataSet(\n",
    "            df,\n",
    "            time_idx=\"time_idx\",\n",
    "            target=target_col,\n",
    "            group_ids=[\"_group_id\"],\n",
    "            max_encoder_length=self.seq_len,\n",
    "            max_prediction_length=self.lead,\n",
    "            time_varying_known_reals=known_reals,\n",
    "            time_varying_unknown_reals=self.target_cols,\n",
    "            add_relative_time_idx=True,\n",
    "            add_target_scales=True,\n",
    "            add_encoder_length=True,\n",
    "        )\n",
    "        print(f\"üì¶ TimeSeriesDataSet ({split_name}) criado com {len(df)} amostras.\")\n",
    "        return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8ece38",
   "metadata": {},
   "source": [
    "# Cap√≠tulo 2 ‚Äî Constru√ß√£o dos Modelos\n",
    "\n",
    "A seguir, definimos construtores simples e eficientes para cada modelo (Linear, LSTM, TFT e TimesFM),\n",
    "prontos para uso em rotinas de otimiza√ß√£o de hiperpar√¢metros (por exemplo, Optuna). Cada construtor\n",
    "recebe um dicion√°rio de par√¢metros (`params`) e retorna um modelo compilado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556d5525",
   "metadata": {},
   "source": [
    "## Constru√ß√£o do Modelo Linear/MLP\n",
    "\n",
    "Objetivo: Criar um regressor simples (MLP), com capacidade de redu√ß√£o para um modelo apenas lienar - pela exclus√£o da camada de ativa√ß√£o - para prever `target_cols` a partir de `feature_cols`.\n",
    "\n",
    "Contrato r√°pido:\n",
    "- Entrada: vetor de tamanho `x_dim` (n√∫mero de features)\n",
    "- Sa√≠da: vetor de tamanho `y_dim` (n√∫mero de targets)\n",
    "- Par√¢metros (exemplos): hidden_units, activation, dropout, lr, l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "727619dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear/MLP model + loaders (Parquet only)\n",
    "from typing import Dict, Any\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "def build_linear_model(x_dim: int, y_dim: int, params: Dict[str, Any], linear: bool = False) -> keras.Model:\n",
    "    \"\"\"\n",
    "    Constr√≥i um modelo linear puro ou um MLP, dependendo do argumento `linear`.\n",
    "\n",
    "    params:\n",
    "      - hidden_units: List[int] (apenas usado se linear=False)\n",
    "      - activation: str (apenas usado se linear=False)\n",
    "      - dropout: float (0..1)\n",
    "      - l2: float (regulariza√ß√£o L2)\n",
    "      - lr: float (learning rate)\n",
    "    \"\"\"\n",
    "    hidden_units = params.get('hidden_units', [128, 64])\n",
    "    activation = params.get('activation', 'relu')\n",
    "    dropout = float(params.get('dropout', 0.0))\n",
    "    l2 = float(params.get('l2', 0.0))\n",
    "    lr = float(params.get('lr', 1e-3))\n",
    "    mask_value = params.get('mask_value', -999.0)\n",
    "\n",
    "    inputs = keras.Input(shape=(x_dim,), name='features')\n",
    "\n",
    "    x = layers.Masking(mask_value=mask_value, name='masking')(inputs)\n",
    "\n",
    "    if linear:\n",
    "        # Modelo puramente linear (sem ativa√ß√£o)\n",
    "        outputs = layers.Dense(\n",
    "            y_dim,\n",
    "            activation=None,\n",
    "            kernel_regularizer=keras.regularizers.l2(l2),\n",
    "            name='linear_output'\n",
    "        )(inputs)\n",
    "        model = keras.Model(inputs, outputs, name='linear_model_true')\n",
    "\n",
    "    else:\n",
    "        # Modelo MLP (n√£o linear)\n",
    "        x = inputs\n",
    "        for i, units in enumerate(hidden_units):\n",
    "            x = layers.Dense(\n",
    "                units,\n",
    "                activation=activation,\n",
    "                kernel_regularizer=keras.regularizers.l2(l2),\n",
    "                name=f'dense_{i}'\n",
    "            )(x)\n",
    "            if dropout > 0:\n",
    "                x = layers.Dropout(dropout, name=f'dropout_{i}')(x)\n",
    "        outputs = layers.Dense(y_dim, name='targets')(x)\n",
    "        model = keras.Model(inputs, outputs, name='mlp_model')\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# -------- Parquet helpers for Linear/MLP --------\n",
    "\n",
    "def save_linear_splits_parquet(preproc, basename: str = \"linear_dataset\", row_group_size: int = 200_000, verbose: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Salva cada split (train/val/test) como Parquet + meta.json com x_dim/y_dim/colunas.\n",
    "    De-duplica listas de colunas e evita sobreposi√ß√£o entre features e targets.\n",
    "    Usa pyarrow ParquetWriter com row groups (chunks) para reduzir picos de mem√≥ria.\n",
    "    \"\"\"\n",
    "    import json, os\n",
    "    import pandas as pd\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "\n",
    "    def _uniq(seq):\n",
    "        seen = set()\n",
    "        out = []\n",
    "        for x in seq:\n",
    "            if x not in seen:\n",
    "                out.append(x)\n",
    "                seen.add(x)\n",
    "        return out\n",
    "\n",
    "    out = {}\n",
    "    for split_name, df in (getattr(preproc, 'splits', {}) or {}).items():\n",
    "        path = os.path.join(preproc.data_dir, f\"{basename}_{split_name}.parquet\")\n",
    "        num_cols = df.select_dtypes(include=[\"number\", \"bool\"]).columns.tolist()\n",
    "        fraw = [c for c in preproc.feature_cols if c in num_cols]\n",
    "        traw = [c for c in preproc.target_cols if c in num_cols]\n",
    "        tcols = _uniq(traw)\n",
    "        fcols = [c for c in _uniq(fraw) if c not in set(tcols)]\n",
    "        combined_cols = fcols + tcols\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[Linear:{split_name}] linhas={len(df):,}  X={len(fcols)}  Y={len(tcols)}  escrevendo em chunks de {row_group_size:,}‚Ä¶\")\n",
    "\n",
    "        writer = None\n",
    "        try:\n",
    "            n = len(df)\n",
    "            for start in range(0, n, row_group_size):\n",
    "                end = min(start + row_group_size, n)\n",
    "                chunk = df.iloc[start:end][combined_cols]\n",
    "                table = pa.Table.from_pandas(chunk, preserve_index=False)\n",
    "                if writer is None:\n",
    "                    writer = pq.ParquetWriter(path, table.schema, compression=\"snappy\")\n",
    "                writer.write_table(table)\n",
    "            if writer is not None:\n",
    "                writer.close()\n",
    "        finally:\n",
    "            if writer is not None:\n",
    "                try:\n",
    "                    writer.close()\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        meta = {\n",
    "            \"x_dim\": int(len(fcols)),\n",
    "            \"y_dim\": int(len(tcols)),\n",
    "            \"feature_cols\": fcols,\n",
    "            \"target_cols\": tcols,\n",
    "            \"parquet_path\": path,\n",
    "            \"basename\": f\"{basename}_{split_name}\"\n",
    "        }\n",
    "        with open(os.path.join(preproc.data_dir, f\"{basename}_{split_name}.meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "        out[split_name] = {\"path\": path, \"meta\": meta}\n",
    "        if verbose:\n",
    "            print(f\"[üíæ] Parquet salvo: {path}\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_linear_parquet_dataset(data_dir: str, split: str, batch_size: int = 256, shuffle: bool = True):\n",
    "    \"\"\"\n",
    "    Carrega Parquet para NumPy e monta tf.data a partir de mem√≥ria (r√°pido).\n",
    "    \"\"\"\n",
    "    import os, json\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "\n",
    "    meta_path = os.path.join(data_dir, f\"linear_dataset_{split}.meta.json\")\n",
    "    if not os.path.exists(meta_path):\n",
    "        raise FileNotFoundError(f\"Meta JSON n√£o encontrado: {meta_path}\")\n",
    "    with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        meta = json.load(f)\n",
    "    df = pd.read_parquet(meta[\"parquet_path\"])\n",
    "    X = df[meta[\"feature_cols\"]].to_numpy(\"float32\", copy=False)\n",
    "    Y = df[meta[\"target_cols\"]].to_numpy(\"float32\", copy=False)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(min(len(df), 10000), seed=42, reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds, meta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0c1b3c",
   "metadata": {},
   "source": [
    "## Constru√ß√£o do Modelo LSTM\n",
    "\n",
    "Objetivo: um regressor denso simples (MLP) para prever `target_cols` a partir de `feature_cols`.\n",
    "\n",
    "Contrato r√°pido:\n",
    "- Entrada: vetor de tamanho `x_dim` (n√∫mero de features)\n",
    "- Sa√≠da: vetor de tamanho `y_dim` (n√∫mero de targets)\n",
    "- Par√¢metros (exemplos): hidden_units, activation, dropout, lr, l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71b6c091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from typing import Dict, Any, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "\n",
    "def build_lstm_model(seq_len: int, x_dim: int, y_dim: int, lead: int, params: Dict[str, Any]) -> keras.Model:\n",
    "    \"\"\"\n",
    "    LSTM seq2seq simples para previs√£o multi-passo:\n",
    "      encoder LSTM -> RepeatVector(lead) -> decoder LSTM -> TimeDistributed(Dense(y_dim)).\n",
    "\n",
    "    Par√¢metros aceitos (em params):\n",
    "      - lstm_units: int OU lista/tupla de int\n",
    "          * int: usa o mesmo valor no encoder e decoder\n",
    "          * lista/tupla: usa primeiro para encoder e √∫ltimo para decoder\n",
    "      - dropout: float (0..1)\n",
    "      - recurrent_dropout ou rec_dropout: float (0..1)\n",
    "      - lr: float (learning rate)\n",
    "      - bidirectional: bool (usa Bidirectional no encoder)\n",
    "    \"\"\"\n",
    "    # lstm_units flex√≠vel: int ou lista/tupla\n",
    "    lu = params.get('lstm_units', 128)\n",
    "    if isinstance(lu, (list, tuple)):\n",
    "        try:\n",
    "            enc_units = int(lu[0])\n",
    "            dec_units = int(lu[-1])\n",
    "        except Exception:\n",
    "            raise ValueError(f\"lstm_units inv√°lido (esperado int ou lista de ints), recebido: {lu}\")\n",
    "    else:\n",
    "        enc_units = dec_units = int(lu)\n",
    "\n",
    "    dropout = float(params.get('dropout', 0.0))\n",
    "    rdrop = float(params.get('recurrent_dropout', params.get('rec_dropout', 0.0)))\n",
    "    lr = float(params.get('lr', 1e-3))\n",
    "    bidir = bool(params.get('bidirectional', False))\n",
    "\n",
    "    inputs = keras.Input(shape=(seq_len, x_dim), name=\"X\")\n",
    "    if bidir:\n",
    "        enc = layers.Bidirectional(\n",
    "            layers.LSTM(enc_units, dropout=dropout, recurrent_dropout=rdrop, return_sequences=False),\n",
    "            name=\"enc_bi\"\n",
    "        )(inputs)\n",
    "    else:\n",
    "        enc = layers.LSTM(enc_units, dropout=dropout, recurrent_dropout=rdrop, return_sequences=False, name=\"enc\")(inputs)\n",
    "\n",
    "    rep = layers.RepeatVector(lead, name=\"repeat_lead\")(enc)\n",
    "    dec = layers.LSTM(dec_units, dropout=dropout, recurrent_dropout=rdrop, return_sequences=True, name=\"dec\")(rep)\n",
    "    outputs = layers.TimeDistributed(layers.Dense(y_dim), name=\"td_out\")(dec)\n",
    "\n",
    "    model = keras.Model(inputs, outputs, name=\"lstm_seq2seq\")\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), loss=\"mse\", metrics=[\"mae\"]) \n",
    "    return model\n",
    "\n",
    "def load_lstm_parquet_dataset(data_dir: str, split: str, batch_size: int = 64, shuffle: bool = True):\n",
    "    \"\"\"\n",
    "    L√™ Parquet com colunas X(list<float>), Y(list<float>) e reconstr√≥i tensores:\n",
    "      X -> (seq_len, x_dim), Y -> (lead, y_dim)\n",
    "    Retorna tf.data.Dataset[(X, Y)] e meta.\n",
    "    \"\"\"\n",
    "    meta_path = os.path.join(data_dir, f\"lstm_dataset_{split}.meta.json\")\n",
    "    if not os.path.exists(meta_path):\n",
    "        raise FileNotFoundError(f\"Meta JSON n√£o encontrado: {meta_path}\")\n",
    "    with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        meta = json.load(f)\n",
    "    pq = meta[\"parquet_path\"]\n",
    "    df = pd.read_parquet(pq)\n",
    "\n",
    "    seq_len = int(meta[\"seq_len\"]) ; lead = int(meta[\"lead\"]) ; x_dim = int(meta[\"x_dim\"]) ; y_dim = int(meta[\"y_dim\"]) \n",
    "\n",
    "    # Converte listas -> numpy e reshape por janela\n",
    "    X_list = df[\"X\"].to_list()\n",
    "    Y_list = df[\"Y\"].to_list()\n",
    "    X = np.asarray(X_list, dtype=np.float32).reshape((-1, seq_len, x_dim))\n",
    "    Y = np.asarray(Y_list, dtype=np.float32).reshape((-1, lead, y_dim))\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(min(len(df), 10000), seed=42, reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds, meta\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b72a517",
   "metadata": {},
   "source": [
    "## Constru√ß√£o do Modelo TFT (Temporal Fusion Transformer)\n",
    "\n",
    "**Objetivo:** prever `target_cols` a partir de `feature_cols` utilizando a implementa√ß√£o oficial `keras_tft`,  \n",
    "que integra **sele√ß√£o de vari√°veis din√¢micas**, **blocos LSTM**, **aten√ß√£o temporal multi-cabe√ßas** e **gating residual** em um √∫nico modelo interpretable.\n",
    "\n",
    "**Contrato r√°pido:**\n",
    "- **Entrada:** `tf.data.Dataset` com tensores no formato `(batch, seq_len, x_dim)`  \n",
    "- **Sa√≠da:** tensor cont√≠nuo de tamanho `y_dim` *(ou `dec_len √ó y_dim` para horizontes m√∫ltiplos)*\n",
    "\n",
    "**Par√¢metros (exemplos):**  \n",
    "`hidden_size` (tamanho interno das camadas GRN) ¬∑ `lstm_layers` ¬∑ `num_heads` (aten√ß√£o) ¬∑ `dropout` ¬∑ `learning_rate` ¬∑ `output_size` ¬∑ `seq_len`\n",
    "\n",
    "**Componentes internos (`keras_tft`):**  \n",
    "Variable Selection Network ‚Üí LSTM Encoder/Decoder ‚Üí Multi-Head Temporal Attention ‚Üí Gated Residual Network ‚Üí Camada de proje√ß√£o final\n",
    "\n",
    "**Compatibilidade:**  \n",
    "Totalmente compat√≠vel com o pipeline atual em Parquet do LSTM, recebendo o mesmo formato de dados  \n",
    "(`(batch, seq_len, features)`), permitindo substitui√ß√£o direta do modelo sem alterar o pr√©-processamento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ce31bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import torch\n",
    "from torch import nn\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "\n",
    "def build_tft_model(\n",
    "    params: Dict[str, Any]\n",
    "):\n",
    "    \"\"\"\n",
    "    Constr√≥i um Temporal Fusion Transformer (TFT) com par√¢metros configur√°veis.\n",
    "\n",
    "    Args:\n",
    "        x_dim: n√∫mero de features de entrada\n",
    "        y_dim: n√∫mero de targets\n",
    "        seq_len: tamanho da sequ√™ncia temporal\n",
    "        params: dicion√°rio de hiperpar√¢metros (hidden_size, dropout, lstm_layers, etc.)\n",
    "        max_encoder_length: tamanho da janela passada (encoder)\n",
    "        max_prediction_length: tamanho do horizonte de previs√£o (decoder)\n",
    "    \"\"\"\n",
    "\n",
    "    from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "    hidden_size = int(params.get(\"hidden_size\", 64))\n",
    "    dropout = float(params.get(\"dropout\", 0.1))\n",
    "    lstm_layers = int(params.get(\"lstm_layers\", 1))\n",
    "    attention_head_size = int(params.get(\"num_heads\", 4))\n",
    "    lr = float(params.get(\"lr\", 1e-3))\n",
    "\n",
    "    model = TemporalFusionTransformer.from_dataset(\n",
    "        params[\"dataset\"],  # dataset preparado via TimeSeriesDataSet\n",
    "        learning_rate=lr,\n",
    "        hidden_size=hidden_size,\n",
    "        dropout=dropout,\n",
    "        lstm_layers=lstm_layers,\n",
    "        attention_head_size=attention_head_size,\n",
    "        loss=QuantileLoss([0.5]),\n",
    "        log_interval=10,\n",
    "        log_val_interval=1\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afbf736",
   "metadata": {},
   "source": [
    "# Cap√≠tulo 3 - Contru√ß√£o da Pipelines de dados dos modelos\n",
    "\n",
    "A fun√ß√£o de pipeline organiza o fluxo de dados para, de forma mais concisa e organizada, treinar o modelo, sendo capaz de mostrar a progress√£o das perdas a medida que as √©pocas de treinamento passam - Esse display est√© dispon√≠vel no notebook \"Resultados\"\n",
    "\n",
    "O resultado da pipeline √© um gr√°fico com a evolu√ß√£o de todas as m√©tricas e o salvamento do modelo treinado dentro da pasta ./modelo/{Nome_Problema}/{Nome_Modelo}\n",
    "\n",
    "Assim podendo ser facilmente reutilizado futuramente para um notebook comparativo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d03c22",
   "metadata": {},
   "source": [
    "## Pipeline dos Modelos Lineares\n",
    "\n",
    "Pipeline de preprocessamento e de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b77058c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilit√°rio para salvar modelos Keras de forma padronizada\n",
    "# Uso: save_model(model, path=\"./modelos/<problema>/<nome>\") ‚Üí salva \"<nome>.keras\" e um meta.json\n",
    "\n",
    "def save_model(model, path: str, format: str | None = None, include_optimizer: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Salva um modelo Keras em formato padronizado.\n",
    "\n",
    "    Regras:\n",
    "    - Se `path` terminar com .keras ou .h5, salva exatamente nesse arquivo.\n",
    "    - Se `format == 'savedmodel'`, salva no diret√≥rio indicado (SavedModel).\n",
    "    - Caso contr√°rio, adiciona sufixo .keras a `path` (arquivo √∫nico Keras v3).\n",
    "\n",
    "    Retorna o caminho final salvo (arquivo ou diret√≥rio) e grava um meta.json ao lado.\n",
    "    \"\"\"\n",
    "    import os, json, datetime\n",
    "\n",
    "    # Infer√™ncia de formato por extens√£o\n",
    "    ext = None\n",
    "    lower = path.lower()\n",
    "    if lower.endswith(\".keras\"):\n",
    "        ext = \"keras\"\n",
    "    elif lower.endswith(\".h5\") or lower.endswith(\".hdf5\"):\n",
    "        ext = \"h5\"\n",
    "\n",
    "    # Normaliza√ß√£o de destino\n",
    "    if format == \"savedmodel\":\n",
    "        # Diret√≥rio SavedModel\n",
    "        save_dir = path\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        model.save(save_dir, include_optimizer=include_optimizer)\n",
    "        meta_path = os.path.join(save_dir, \"model.meta.json\")\n",
    "        final_path = save_dir\n",
    "    else:\n",
    "        if ext is None:\n",
    "            # For√ßa arquivo .keras por padr√£o\n",
    "            path = f\"{path}.keras\"\n",
    "            ext = \"keras\"\n",
    "        # Cria diret√≥rio pai\n",
    "        parent = os.path.dirname(path) or \".\"\n",
    "        os.makedirs(parent, exist_ok=True)\n",
    "        # Salva arquivo √∫nico\n",
    "        model.save(path, include_optimizer=include_optimizer)\n",
    "        meta_path = f\"{path}.meta.json\"\n",
    "        final_path = path\n",
    "\n",
    "    # Meta b√°sico ao lado do artefato\n",
    "    try:\n",
    "        meta = {\n",
    "            \"saved_at\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
    "            \"keras_version\": getattr(model, \"keras_version\", None),\n",
    "            \"model_name\": getattr(model, \"name\", None),\n",
    "            \"trainable_params\": int(getattr(model, \"count_params\", lambda: 0)()),\n",
    "            \"format\": \"savedmodel\" if format == \"savedmodel\" else ext,\n",
    "        }\n",
    "        with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Falha ao escrever meta.json: {e}\")\n",
    "\n",
    "    print(f\"[üíæ] Modelo salvo em: {final_path}\")\n",
    "    return final_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e97fb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping as TFEarlyStopping, ReduceLROnPlateau as TFReduceLROnPlateau\n",
    "\n",
    "\n",
    "def linear_preproccess_pipeline(\n",
    "    data_dir: str,\n",
    "    country_list: List[str],\n",
    "    feature_cols: List[str],\n",
    "    target_cols: List[str],\n",
    "    lag: int,\n",
    "    lead: int,\n",
    "    value_cols: List[str],\n",
    "    reduced_dim: Optional[int] = None,\n",
    "    mask_value: Optional[float] = -999.0,\n",
    ") -> Tuple[LinearPreprocessor, Dict[str, keras.Model]]:\n",
    "    \"\"\"\n",
    "    Pipeline completa de pr√©-processamento (Parquet only) e treinamento de 3 modelos\n",
    "    (simple, medium, deep) para compara√ß√£o direta de desempenho.\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------\n",
    "    # Pr√©-processamento\n",
    "    # ----------------------------\n",
    "    preproc = LinearPreprocessor(\n",
    "        data_dir=data_dir,\n",
    "        model_name='linear_model',\n",
    "        feature_cols=feature_cols,\n",
    "        target_cols=target_cols,\n",
    "        lag=lag,\n",
    "        lead=lead,\n",
    "        country_list=country_list\n",
    "    )\n",
    "\n",
    "    preproc.load_data()\n",
    "    preproc.encode(encode_cols='datetime', encode_method='time_cycle')\n",
    "    preproc.encode(encode_cols='country', encode_method='label')\n",
    "    preproc.split_train_val_test(train_size=0.8, val_size=0.1, test_size=0.1, time_col='datetime')\n",
    "    preproc.normalize_splits(value_cols=value_cols, normalization_method='minmax')\n",
    "    preproc.build_flat_matrices_splits(\n",
    "        value_cols=value_cols,\n",
    "        target_cols=target_cols,\n",
    "        dropna=True,\n",
    "        group_cols=['country'],\n",
    "        time_col='datetime',\n",
    "        reduced_dim=reduced_dim,\n",
    "        mask_value=mask_value\n",
    "    )\n",
    "    # Parquet only\n",
    "    save_linear_splits_parquet(preproc, basename='linear_dataset')\n",
    "    print(\"‚úÖ Pr√©-processamento linear conclu√≠do (Parquet).\")\n",
    "\n",
    "    return preproc\n",
    "\n",
    "\n",
    "def linear_train_pipeline(\n",
    "    problem_name: str,\n",
    "    data_dir: str,\n",
    "    batch_size: int = 128,\n",
    "    configs: Dict[str, Dict[str, Any]] = None\n",
    "):\n",
    "    # ----------------------------\n",
    "    # Parquet datasets\n",
    "    # ----------------------------\n",
    "    dataset_train, meta_tr = load_linear_parquet_dataset(data_dir=data_dir, split='train', batch_size=batch_size, shuffle=True)\n",
    "    dataset_val, meta_va = load_linear_parquet_dataset(data_dir=data_dir, split='val', batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    x_dim = int(meta_tr[\"x_dim\"])\n",
    "    y_dim = int(meta_tr[\"y_dim\"])\n",
    "    print(\"üì¶ Dataset Parquet carregado para treinamento.\")\n",
    "\n",
    "    histories = {}\n",
    "    models = {}\n",
    "\n",
    "    early_stopping = TFEarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=True, verbose=0)\n",
    "    reduce_lr = TFReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=6, min_lr=1e-6, verbose=0)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Treinamento de cada modelo\n",
    "    # ----------------------------\n",
    "    for name, params in configs.items():\n",
    "        print(f\"\\nüöÄ Treinando modelo {name}...\")\n",
    "        if params.get(\"linear\", False):\n",
    "            # Modelo Linear\n",
    "            model = build_linear_model(x_dim=x_dim, y_dim=y_dim, params=params, linear=True)\n",
    "        else:\n",
    "            # Modelo MLP\n",
    "            model = build_linear_model(x_dim=x_dim, y_dim=y_dim, params=params, linear=False)\n",
    "        hist = model.fit(\n",
    "            dataset_train,\n",
    "            validation_data=dataset_val,\n",
    "            epochs=100,\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=2\n",
    "        )\n",
    "\n",
    "        histories[name] = hist\n",
    "        models[name] = model\n",
    "        print(f\"‚úÖ {name} conclu√≠do - Val Loss: {min(hist.history['val_loss']):.6f}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Salvando modelos\n",
    "    # ----------------------------\n",
    "    for name, model in models.items():\n",
    "        # Salvando modelo no path /modelos/{nome do problema}/{nome do modelo}\n",
    "        save_model(model, path = f\"./modelos/{problem_name}/{name}\")\n",
    "    \n",
    "    \n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6250a234",
   "metadata": {},
   "source": [
    "## Pipelines dos modelos LSTM\n",
    "Implementa√ß√£o e uso dos preprocessors e treinadores LSTM para s√©ries temporais (janelas seq_len e lead)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39f66f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping as TFEarlyStopping, ReduceLROnPlateau as TFReduceLROnPlateau\n",
    "\n",
    "\n",
    "def lstm_preproccess_pipeline(\n",
    "    data_dir: str,\n",
    "    country_list: List[str],\n",
    "    feature_cols: List[str],\n",
    "    target_cols: List[str],\n",
    "    seq_len: int,\n",
    "    lead: int,\n",
    "    value_cols: List[str]\n",
    ") -> Tuple[LSTMPreprocessor, Dict[str, keras.Model]]:\n",
    "    \"\"\"\n",
    "    Pipeline completa de pr√©-processamento (Parquet only) para LSTM,\n",
    "    salvando janelas (seq_len -> lead) em Parquet.\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------\n",
    "    # Pr√©-processamento\n",
    "    # ----------------------------\n",
    "    preproc = LSTMPreprocessor(\n",
    "        data_dir=data_dir,\n",
    "        model_name=\"lstm_model\",\n",
    "        feature_cols=feature_cols,\n",
    "        target_cols=target_cols,\n",
    "        country_list=country_list,\n",
    "        lag=seq_len,\n",
    "        lead=lead,\n",
    "    )\n",
    "\n",
    "    preproc.load_data()\n",
    "    preproc.encode(encode_cols=\"datetime\", encode_method=\"time_cycle\")\n",
    "    preproc.encode(encode_cols=\"country\", encode_method=\"label\")\n",
    "    preproc.split_train_val_test(train_size=0.6, val_size=0.2, test_size=0.2, time_col=\"datetime\")\n",
    "    preproc.normalize_splits(value_cols=value_cols, normalization_method=\"minmax\")\n",
    "\n",
    "    # Salva janelas por split direto em Parquet (m√©todo do preprocessor)\n",
    "    preproc.save_splits_parquet(basename=\"lstm_dataset\")\n",
    "    print(\"‚úÖ Pr√©-processamento sequencial conclu√≠do (Parquet).\")\n",
    "    return preproc\n",
    "\n",
    "\n",
    "def lstm_train_pipeline(\n",
    "    problem_name: str,\n",
    "    data_dir: str,\n",
    "    seq_len: int,\n",
    "    batch_size: int = 128,\n",
    "    configs: Dict[str, Dict[str, Any]] = None\n",
    ") -> Tuple[LSTMPreprocessor, Dict[str, keras.Model]]:\n",
    "    # ----------------------------\n",
    "    # Parquet datasets\n",
    "    # ----------------------------\n",
    "    dataset_train, meta_tr = load_lstm_parquet_dataset(data_dir=data_dir, split=\"train\", batch_size=batch_size, shuffle=True)\n",
    "    dataset_val, meta_va = load_lstm_parquet_dataset(data_dir=data_dir, split=\"val\", batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    x_dim = int(meta_tr[\"x_dim\"]) ; y_dim = int(meta_tr[\"y_dim\"]) ; seq_len_m = int(meta_tr[\"seq_len\"]) ; lead_m = int(meta_tr[\"lead\"]) \n",
    "    if seq_len and seq_len != seq_len_m:\n",
    "        print(f\"[WARN] seq_len fornecido ({seq_len}) difere do meta ({seq_len_m}). Usando meta.\")\n",
    "    print(\"üì¶ Dataset Parquet carregado para treinamento.\")\n",
    "\n",
    "    histories = {}\n",
    "    models = {}\n",
    "\n",
    "    early_stopping = TFEarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=True, verbose=0)\n",
    "    reduce_lr = TFReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=6, min_lr=1e-6, verbose=0)\n",
    "\n",
    "    for name, params in configs.items():\n",
    "        print(f\"\\nüöÄ Treinando modelo {name}...\")\n",
    "        model = build_lstm_model(seq_len=seq_len_m, x_dim=x_dim, y_dim=y_dim, lead=lead_m, params=params)\n",
    "        hist = model.fit(\n",
    "            dataset_train,\n",
    "            validation_data=dataset_val,\n",
    "            epochs=100,\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=2,\n",
    "        )\n",
    "\n",
    "        histories[name] = hist\n",
    "        models[name] = model\n",
    "\n",
    "        print(f\"‚úÖ {name} conclu√≠do - Val Loss: {min(hist.history['val_loss']):.6f}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Salvando modelos\n",
    "    # ----------------------------\n",
    "    for name, model in models.items():\n",
    "        # Salvando modelo no path /modelos/{nome do problema}/{nome do modelo}\n",
    "        save_model(model, path = f\"./modelos/{problem_name}/{name}\")\n",
    "    \n",
    "    \n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be66287c",
   "metadata": {},
   "source": [
    "## Pipelines dos modelos TFT\n",
    "Pr√©-processamento em parquet e treino com PyTorch Forecasting (Temporal Fusion Transformer) via Lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87e31312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Dict, Any, List\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import EarlyStopping as LGEarlyStopping, LearningRateMonitor as LGLearningRateMonitor, ModelCheckpoint as LGModelCheckpoint\n",
    "\n",
    "\n",
    "def tft_preproccess_pipeline(\n",
    "    data_dir: str,\n",
    "    country_list: List[str],\n",
    "    feature_cols: List[str],\n",
    "    target_cols: List[str],\n",
    "    seq_len: int,\n",
    "    lead: int,\n",
    "    value_cols: List[str]\n",
    ") -> Tuple[TFTPreprocessor, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Pipeline completa de pr√©-processamento para TFT.\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------\n",
    "    # Pr√©-processamento\n",
    "    # ----------------------------\n",
    "    preproc = TFTPreprocessor(\n",
    "        data_dir=data_dir,\n",
    "        model_name='linear_model',\n",
    "        feature_cols=feature_cols,\n",
    "        target_cols=target_cols,\n",
    "        seq_len=seq_len,\n",
    "        lead=lead,\n",
    "        country_list=country_list\n",
    "    )\n",
    "\n",
    "    preproc.load_data()\n",
    "    preproc.encode(encode_cols='datetime', encode_method='time_cycle')\n",
    "    preproc.encode(encode_cols='country', encode_method='label')\n",
    "    preproc.split_train_val_test(train_size=0.8, val_size=0.1, test_size=0.1, time_col='datetime')\n",
    "    preproc.normalize_splits(value_cols=value_cols, normalization_method='minmax')\n",
    "    preproc.build_tft_parquets(\n",
    "        group_cols=['country'],\n",
    "        time_col='datetime'\n",
    "    )\n",
    "    print(\"‚úÖ Pr√©-processamento tft conclu√≠do.\")\n",
    "\n",
    "    return preproc\n",
    "\n",
    "\n",
    "\n",
    "def tft_train_pipeline(\n",
    "    problem_name: str,\n",
    "    data_dir: str,\n",
    "    feature_cols: List[str],\n",
    "    target_cols: List[str],\n",
    "    seq_len: int,\n",
    "    lead: int,\n",
    "    batch_size: int = 128,\n",
    "    configs: Dict[str, Dict[str, Any]] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Treinamento de modelos TFT (Temporal Fusion Transformer) usando PyTorch Forecasting + Lightning.\n",
    "\n",
    "    - Consome os parquets gerados por TFTPreprocessor: tft_dataset_train.parquet e tft_dataset_val.parquet\n",
    "    - Cria TimeSeriesDataSet para treino/valida√ß√£o\n",
    "    - Constr√≥i o modelo via TemporalFusionTransformer.from_dataset\n",
    "    - Treina com EarlyStopping e salva checkpoints por preset\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------\n",
    "    # 1. Carregar dados pr√©-processados via TFTPreprocessor reutilizando load_tft_dataset\n",
    "    # ----------------------------\n",
    "    preproc = TFTPreprocessor(\n",
    "        data_dir=data_dir,\n",
    "        model_name='tft_model',\n",
    "        feature_cols=feature_cols,\n",
    "        target_cols=target_cols,\n",
    "        seq_len=seq_len,\n",
    "        lead=lead,\n",
    "        country_list=[],\n",
    "    )\n",
    "\n",
    "    # usa a fun√ß√£o para retornar DataFrames ‚Äî permite aplicar dtypes e criar TimeSeriesDataSet de forma consistente\n",
    "    df_train = preproc.load_tft_dataset('train', target_col=target_cols[0])\n",
    "    df_val = preproc.load_tft_dataset('val', target_col=target_cols[0])\n",
    "\n",
    "    # ----------------------------\n",
    "    # 2. TimeSeriesDataSet (encoder/decoder feitos internamente)\n",
    "    # ----------------------------\n",
    "\n",
    "    train_loader = df_train.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "    val_loader   = df_val.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "    print(f\"üì¶ Dados TFT ‚Äî batches: train={len(train_loader)} | val={len(val_loader)}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # 3. Treinamento por preset\n",
    "    # ----------------------------\n",
    "    models = {}\n",
    "    seed_everything(42)\n",
    "\n",
    "    accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    for name, params in (configs or {}).items():\n",
    "        print(f\"\\nüöÄ Treinando TFT preset: {name} [{accelerator}]\")\n",
    "\n",
    "        model = build_tft_model(\n",
    "            params={\n",
    "                **params,\n",
    "                \"dataset\": df_train,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        save_dir = os.path.join(\"modelos\", problem_name, \"TFT\", name)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        callbacks = [\n",
    "            LGEarlyStopping(monitor=\"val_loss\", patience=int(params.get(\"patience\", 5)), mode=\"min\"),\n",
    "            LGLearningRateMonitor(logging_interval=\"epoch\"),\n",
    "            LGModelCheckpoint(\n",
    "                dirpath=save_dir,\n",
    "                filename=\"best\",\n",
    "                monitor=\"val_loss\",\n",
    "                save_top_k=1,\n",
    "                mode=\"min\",\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        trainer = Trainer(\n",
    "            max_epochs=int(params.get(\"epochs\", 50)),\n",
    "            accelerator=accelerator,\n",
    "            devices=1,\n",
    "            callbacks=callbacks,\n",
    "            default_root_dir=save_dir,\n",
    "            log_every_n_steps=10,\n",
    "            logger=True,\n",
    "            precision=32,\n",
    "        )\n",
    "\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        print(f\"‚úÖ {name} conclu√≠do ‚Äî melhor checkpoint salvo em {save_dir}\")\n",
    "\n",
    "        models[name] = model\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b138dc",
   "metadata": {},
   "source": [
    "# Cap√≠tulo 4: Defini√ß√£o da estrutura dos modelos - N√£o foi feita otimiza√ß√£o de hiperparm\n",
    "Configura√ß√£o dos problemas (dados, features, janelas) e presets de hiperpar√¢metros para Linear/MLP, LSTM e TFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17d8e826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, tensorflow as tf\n",
    "\n",
    "\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "# Configura√ß√µes dos modelos\n",
    "\n",
    "configs_linear = {\n",
    "    # \"linear_Simple\": {\n",
    "    #     \"linear\": True,\n",
    "    #     \"units\": [],\n",
    "    #     \"dropout\": 0.0,\n",
    "    #     \"lr\": 1e-3,\n",
    "    #     \"l2\": 0.0,\n",
    "    #     \"layer_norm\": False,\n",
    "    # },\n",
    "    \"linear_Medium\": {\n",
    "        \"linear\": True,\n",
    "        \"units\": [128, 64],\n",
    "        \"dropout\": 0.1,\n",
    "        \"lr\": 1e-3,\n",
    "        \"l2\": 1e-6,\n",
    "        \"act\": \"relu\",\n",
    "        \"layer_norm\": False,\n",
    "    },\n",
    "    # \"linear_Deep\": {\n",
    "    #     \"linear\": False,\n",
    "    #     \"units\": [256, 128, 64],\n",
    "    #     \"dropout\": 0.2,\n",
    "    #     \"lr\": 1e-3,\n",
    "    #     \"l2\": 1e-6,\n",
    "    #     \"act\": \"relu\",\n",
    "    #     \"layer_norm\": True,\n",
    "    # },\n",
    "}\n",
    "\n",
    "configs_mlp = {\n",
    "    # \"mlp_Simple\": {\n",
    "    #     \"linear\": False,\n",
    "    #     \"units\": [64],\n",
    "    #     \"dropout\": 0.05,\n",
    "    #     \"lr\": 1e-3,\n",
    "    #     \"l2\": 1e-6,\n",
    "    #     \"act\": \"relu\",\n",
    "    #     \"layer_norm\": False,\n",
    "    # },\n",
    "    \"mlp_Medium\": {\n",
    "        \"linear\": False,\n",
    "        \"units\": [128, 64],\n",
    "        \"dropout\": 0.1,\n",
    "        \"lr\": 1e-3,\n",
    "        \"l2\": 1e-6,\n",
    "        \"act\": \"relu\",\n",
    "        \"layer_norm\": False,\n",
    "    },\n",
    "    # \"mlp_Deep\": {\n",
    "    #     \"linear\": False,\n",
    "    #     \"units\": [256, 128, 64],\n",
    "    #     \"dropout\": 0.2,\n",
    "    #     \"lr\": 1e-3,\n",
    "    #     \"l2\": 1e-6,\n",
    "    #     \"act\": \"relu\",\n",
    "    #     \"layer_norm\": True,\n",
    "    # },\n",
    "}\n",
    "\n",
    "configs_lstm = {\n",
    "    # \"lstm_Simple\": {\n",
    "    #     \"lstm_units\": [64],\n",
    "    #     \"dense_units\": [64],\n",
    "    #     \"dropout\": 0.05,\n",
    "    #     \"rec_dropout\": 0.0,\n",
    "    #     \"act\": \"tanh\",\n",
    "    #     \"lr\": 1e-3,\n",
    "    #     \"l2\": 1e-6,\n",
    "    #     \"layer_norm\": False,\n",
    "    # },\n",
    "    \"lstm_Medium\": {\n",
    "        \"lstm_units\": [128, 64],\n",
    "        \"dense_units\": [64],\n",
    "        \"dropout\": 0 if gpu_devices else 0.15,\n",
    "        \"rec_dropout\": 0 if gpu_devices else 0.05,\n",
    "        \"act\": \"tanh\",\n",
    "        \"lr\": 1e-3,\n",
    "        \"l2\": 1e-6,\n",
    "        \"layer_norm\": True,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Presets de TFT compat√≠veis com build_tft_model (PyTorch Forecasting)\n",
    "# Campos utilizados: hidden_size, dropout, lstm_layers, num_heads, lr, epochs, patience\n",
    "config_tft = {\n",
    "    # \"tft_Simple\": {\n",
    "    #     \"hidden_size\": 64,\n",
    "    #     \"dropout\": 0.1,\n",
    "    #     \"lstm_layers\": 1,\n",
    "    #     \"num_heads\": 4,\n",
    "    #     \"lr\": 1e-3,\n",
    "    #     \"epochs\": 50,\n",
    "    #     \"patience\": 5,\n",
    "    # },\n",
    "    \"tft_Medium\": {\n",
    "        \"hidden_size\": 64,\n",
    "        \"lstm_layers\": 1,\n",
    "        \"num_heads\": 2,\n",
    "        \"dropout\": 0.2,\n",
    "        \"hidden_continuous_size\": 32,\n",
    "        \"attention_head_size\": 2,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"patience\": 20,\n",
    "        \"epochs\": 50,\n",
    "    }, \n",
    "    \n",
    "    # \"tft_Deep\": {\n",
    "    #     \"hidden_size\": 256,\n",
    "    #     \"dropout\": 0.2,\n",
    "    #     \"lstm_layers\": 2,\n",
    "    #     \"num_heads\": 8,\n",
    "    #     \"lr\": 1e-3,\n",
    "    #     \"epochs\": 100,\n",
    "    #     \"patience\": 10,\n",
    "    # },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af6da91",
   "metadata": {},
   "source": [
    "# Cap√≠tulo 5: Pr√©processamento de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bec8a927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Executando Preprocessamento do problema treinamento ...\n",
      "üß† Pr√©-processando dados do modelo linear/MLP\n",
      "\n",
      "üöÄ Executando Preprocessamento do problema N1A ...\n",
      "üß† Pr√©-processando dados do modelo linear/MLP\n",
      "\n",
      "üöÄ Executando Preprocessamento do problema N1B ...\n",
      "üß† Pr√©-processando dados do modelo linear/MLP\n",
      "\n",
      "üöÄ Executando Preprocessamento do problema N1C ...\n",
      "üß† Pr√©-processando dados do modelo linear/MLP\n",
      "\n",
      "üöÄ Executando Preprocessamento do problema N2A ...\n",
      "üß† Pr√©-processando dados do modelo linear/MLP\n",
      "\n",
      "üöÄ Executando Preprocessamento do problema N2B ...\n",
      "üß† Pr√©-processando dados do modelo linear/MLP\n",
      "\n",
      "üöÄ Executando Preprocessamento do problema N2C ...\n",
      "üß† Pr√©-processando dados do modelo linear/MLP\n",
      "[DIVIDIDO] train: 27,931 linhas\n",
      "[DIVIDIDO] val: 3,491 linhas\n",
      "[DIVIDIDO] test: 3,492 linhas\n",
      "[DIVIDIDO] train: 27,931 linhas\n",
      "[DIVIDIDO] val: 3,491 linhas\n",
      "[DIVIDIDO] test: 3,492 linhas\n",
      "[DIVIDIDO] train: 27,931 linhas\n",
      "[DIVIDIDO] val: 3,491 linhas\n",
      "[DIVIDIDO] test: 3,492 linhas\n",
      "[‚ÑπÔ∏è] Reduzindo lags: usando 168 e mascarando 72 restantes com -999.0\n",
      "[‚ÑπÔ∏è] Reduzindo lags: usando 72 e mascarando 168 restantes com -999.0\n",
      "[DIVIDIDO] train: 83,831 linhas\n",
      "[DIVIDIDO] val: 10,478 linhas\n",
      "[DIVIDIDO] test: 10,480 linhas\n",
      "[DIVIDIDO] train: 83,831 linhas\n",
      "[DIVIDIDO] val: 10,478 linhas\n",
      "[DIVIDIDO] test: 10,480 linhas\n",
      "[DIVIDIDO] train: 83,831 linhas\n",
      "[DIVIDIDO] val: 10,478 linhas\n",
      "[DIVIDIDO] test: 10,480 linhas\n",
      "[DIVIDIDO] train: 83,831 linhas\n",
      "[DIVIDIDO] val: 10,478 linhas\n",
      "[DIVIDIDO] test: 10,480 linhas\n",
      "[‚ÑπÔ∏è] Reduzindo lags: usando 72 e mascarando 168 restantes com -999.0\n",
      "[‚ÑπÔ∏è] Reduzindo lags: usando 168 e mascarando 72 restantes com -999.0\n",
      "[‚ÑπÔ∏è] Reduzindo lags: usando 72 e mascarando 168 restantes com -999.0\n",
      "[‚ÑπÔ∏è] Reduzindo lags: usando 168 e mascarando 72 restantes com -999.0\n",
      "[‚ÑπÔ∏è] Reduzindo lags: usando 72 e mascarando 168 restantes com -999.0\n",
      "[‚ÑπÔ∏è] Reduzindo lags: usando 72 e mascarando 168 restantes com -999.0\n",
      "[Linear:train] linhas=27,787  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[‚ÑπÔ∏è] Reduzindo lags: usando 168 e mascarando 72 restantes com -999.0\n",
      "[üíæ] Parquet salvo: data/N1A/linear_dataset_train.parquet\n",
      "[Linear:val] linhas=3,347  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[üíæ] Parquet salvo: data/N1A/linear_dataset_val.parquet\n",
      "[Linear:test] linhas=3,348  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[‚ÑπÔ∏è] Reduzindo lags: usando 168 e mascarando 72 restantes com -999.0\n",
      "[üíæ] Parquet salvo: data/N1A/linear_dataset_test.parquet\n",
      "‚úÖ Pr√©-processamento linear conclu√≠do (Parquet).\n",
      "üß† Pr√©-processando dados do modelo LSTM\n",
      "[DIVIDIDO] train: 20,948 linhas\n",
      "[DIVIDIDO] val: 6,982 linhas\n",
      "[DIVIDIDO] test: 6,984 linhas\n",
      "[LSTM:train] linhas=20,948  windows=20,637  X=(240,12)  Y=(72,1)  chunk=10,000\n",
      "[Linear:train] linhas=27,691  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[‚ÑπÔ∏è] Reduzindo lags: usando 72 e mascarando 168 restantes com -999.0\n",
      "[üíæ] Parquet salvo: data/N1B/linear_dataset_train.parquet\n",
      "[Linear:val] linhas=3,251  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[üíæ] Parquet salvo: data/N1B/linear_dataset_val.parquet\n",
      "[Linear:test] linhas=3,252  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[Linear:train] linhas=27,619  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[üíæ] Parquet salvo: data/N1B/linear_dataset_test.parquet\n",
      "‚úÖ Pr√©-processamento linear conclu√≠do (Parquet).\n",
      "üß† Pr√©-processando dados do modelo LSTM\n",
      "[DIVIDIDO] train: 20,948 linhas\n",
      "[DIVIDIDO] val: 6,982 linhas\n",
      "[DIVIDIDO] test: 6,984 linhas\n",
      "[LSTM:train] linhas=20,948  windows=20,637  X=(240,12)  Y=(72,1)  chunk=10,000\n",
      "[‚ÑπÔ∏è] Reduzindo lags: usando 168 e mascarando 72 restantes com -999.0\n",
      "[Linear:train] linhas=83,399  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[üíæ] Parquet salvo: data/N1C/linear_dataset_train.parquet\n",
      "[Linear:val] linhas=3,179  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[üíæ] Parquet salvo: data/N1C/linear_dataset_val.parquet\n",
      "[Linear:test] linhas=3,180  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[üíæ] Parquet salvo: data/N1C/linear_dataset_test.parquet\n",
      "‚úÖ Pr√©-processamento linear conclu√≠do (Parquet).\n",
      "üß† Pr√©-processando dados do modelo LSTM\n",
      "[üíæ] Parquet salvo: data/N2A/linear_dataset_train.parquet\n",
      "[Linear:val] linhas=10,046  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[DIVIDIDO] train: 20,948 linhas\n",
      "[DIVIDIDO] val: 6,982 linhas\n",
      "[DIVIDIDO] test: 6,984 linhas\n",
      "[LSTM:train] linhas=20,948  windows=20,637  X=(240,12)  Y=(72,1)  chunk=10,000\n",
      "[üíæ] Parquet salvo: data/N2A/linear_dataset_val.parquet\n",
      "[Linear:test] linhas=10,048  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[üíæ] Parquet salvo: data/N2A/linear_dataset_test.parquet\n",
      "‚úÖ Pr√©-processamento linear conclu√≠do (Parquet).\n",
      "üß† Pr√©-processando dados do modelo LSTM\n",
      "[DIVIDIDO] train: 62,873 linhas\n",
      "[DIVIDIDO] val: 20,957 linhas\n",
      "[DIVIDIDO] test: 20,959 linhas\n",
      "[LSTM:train] linhas=62,873  windows=62,562  X=(240,12)  Y=(72,1)  chunk=10,000\n",
      "[Linear:train] linhas=83,111  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[Linear:train] linhas=82,895  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[Linear:train] linhas=82,895  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[üíæ] Parquet salvo: data/N2B/linear_dataset_train.parquet\n",
      "[Linear:val] linhas=9,758  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[üíæ] Parquet salvo: data/N2B/linear_dataset_val.parquet\n",
      "[Linear:test] linhas=9,760  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[üíæ] Parquet salvo: data/N2B/linear_dataset_test.parquet\n",
      "‚úÖ Pr√©-processamento linear conclu√≠do (Parquet).\n",
      "üß† Pr√©-processando dados do modelo LSTM\n",
      "[üíæ] Parquet salvo: data/treinamento/linear_dataset_train.parquet\n",
      "[üíæ] Parquet salvo: data/N2C/linear_dataset_train.parquet\n",
      "[Linear:val] linhas=9,542  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[Linear:val] linhas=9,542  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[üíæ] Parquet LSTM salvo: data/N1A/lstm_dataset_train.parquet\n",
      "[LSTM:val] linhas=6,982  windows=6,671  X=(240,12)  Y=(72,1)  chunk=10,000\n",
      "[DIVIDIDO] train: 62,873 linhas\n",
      "[DIVIDIDO] val: 20,957 linhas\n",
      "[DIVIDIDO] test: 20,959 linhas\n",
      "[üíæ] Parquet LSTM salvo: data/N1B/lstm_dataset_train.parquet\n",
      "[LSTM:val] linhas=6,982  windows=6,671  X=(240,12)  Y=(72,1)  chunk=10,000\n",
      "[üíæ] Parquet LSTM salvo: data/N1C/lstm_dataset_train.parquet\n",
      "[LSTM:train] linhas=62,873  windows=62,562  X=(240,12)  Y=(72,1)  chunk=10,000\n",
      "[LSTM:val] linhas=6,982  windows=6,671  X=(240,12)  Y=(72,1)  chunk=10,000\n",
      "[üíæ] Parquet salvo: data/N2C/linear_dataset_val.parquet\n",
      "[Linear:test] linhas=9,544  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[üíæ] Parquet salvo: data/treinamento/linear_dataset_val.parquet\n",
      "[Linear:test] linhas=9,544  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[üíæ] Parquet salvo: data/N2C/linear_dataset_test.parquet\n",
      "‚úÖ Pr√©-processamento linear conclu√≠do (Parquet).\n",
      "üß† Pr√©-processando dados do modelo LSTM\n",
      "[üíæ] Parquet salvo: data/treinamento/linear_dataset_test.parquet\n",
      "‚úÖ Pr√©-processamento linear conclu√≠do (Parquet).\n",
      "üß† Pr√©-processando dados do modelo LSTM\n",
      "[DIVIDIDO] train: 62,873 linhas\n",
      "[DIVIDIDO] val: 20,957 linhas\n",
      "[DIVIDIDO] test: 20,959 linhas\n",
      "[DIVIDIDO] train: 62,873 linhas\n",
      "[DIVIDIDO] val: 20,957 linhas\n",
      "[DIVIDIDO] test: 20,959 linhas\n",
      "[LSTM:train] linhas=62,873  windows=62,562  X=(240,12)  Y=(72,1)  chunk=10,000\n",
      "[LSTM:train] linhas=62,873  windows=62,562  X=(240,12)  Y=(72,1)  chunk=10,000\n",
      "[üíæ] Parquet LSTM salvo: data/N1C/lstm_dataset_val.parquet\n",
      "[LSTM:test] linhas=6,984  windows=6,673  X=(240,12)  Y=(72,1)  chunk=10,000\n",
      "[üíæ] Parquet LSTM salvo: data/N1A/lstm_dataset_val.parquet\n",
      "[LSTM:test] linhas=6,984  windows=6,673  X=(240,12)  Y=(72,1)  chunk=10,000\n",
      "[üíæ] Parquet LSTM salvo: data/N1B/lstm_dataset_val.parquet\n",
      "[LSTM:test] linhas=6,984  windows=6,673  X=(240,12)  Y=(72,1)  chunk=10,000\n",
      "[üíæ] Parquet LSTM salvo: data/N1A/lstm_dataset_test.parquet\n",
      "‚úÖ Pr√©-processamento sequencial conclu√≠do (Parquet).\n",
      "üß† Pr√©-processando dados do modelo TFT\n",
      "[üíæ] Parquet LSTM salvo: data/N1C/lstm_dataset_test.parquet\n",
      "‚úÖ Pr√©-processamento sequencial conclu√≠do (Parquet).\n",
      "[üíæ] Parquet LSTM salvo: data/N1B/lstm_dataset_test.parquet\n",
      "‚úÖ Pr√©-processamento sequencial conclu√≠do (Parquet).\n",
      "üß† Pr√©-processando dados do modelo TFT\n",
      "üß† Pr√©-processando dados do modelo TFT\n",
      "[DIVIDIDO] train: 27,931 linhas\n",
      "[DIVIDIDO] val: 3,491 linhas\n",
      "[DIVIDIDO] test: 3,492 linhas\n",
      "[DIVIDIDO] train: 27,931 linhas\n",
      "[DIVIDIDO] val: 3,491 linhas\n",
      "[DIVIDIDO] test: 3,492 linhas\n",
      "[DIVIDIDO] train: 27,931 linhas\n",
      "[DIVIDIDO] val: 3,491 linhas\n",
      "[DIVIDIDO] test: 3,492 linhas\n",
      "üíæ Split 'train' salvo em data/N1A/tft_dataset_train.parquet (27931 linhas, grupos=1, max local time_idx=27930).\n",
      "üíæ Split 'train' salvo em data/N1C/tft_dataset_train.parquet (27931 linhas, grupos=1, max local time_idx=27930).\n",
      "üíæ Split 'train' salvo em data/N1B/tft_dataset_train.parquet (27931 linhas, grupos=1, max local time_idx=27930).\n",
      "üíæ Split 'val' salvo em data/N1A/tft_dataset_val.parquet (3491 linhas, grupos=1, max local time_idx=3490).\n",
      "üíæ Split 'val' salvo em data/N1C/tft_dataset_val.parquet (3491 linhas, grupos=1, max local time_idx=3490).\n",
      "üíæ Split 'val' salvo em data/N1B/tft_dataset_val.parquet (3491 linhas, grupos=1, max local time_idx=3490).\n",
      "üíæ Split 'test' salvo em data/N1A/tft_dataset_test.parquet (3492 linhas, grupos=1, max local time_idx=3491).\n",
      "‚úÖ Pr√©-processamento tft conclu√≠do.\n",
      "‚úÖ Finalizado N1A - mem√≥ria liberada\n",
      "------------------------------------------------------------\n",
      "üß© Resultado N1A: OK\n",
      "üíæ Split 'test' salvo em data/N1C/tft_dataset_test.parquet (3492 linhas, grupos=1, max local time_idx=3491).\n",
      "‚úÖ Pr√©-processamento tft conclu√≠do.\n",
      "‚úÖ Finalizado N1C - mem√≥ria liberada\n",
      "------------------------------------------------------------\n",
      "üß© Resultado N1C: OK\n",
      "üíæ Split 'test' salvo em data/N1B/tft_dataset_test.parquet (3492 linhas, grupos=1, max local time_idx=3491).\n",
      "‚úÖ Pr√©-processamento tft conclu√≠do.\n",
      "‚úÖ Finalizado N1B - mem√≥ria liberada\n",
      "------------------------------------------------------------\n",
      "üß© Resultado N1B: OK\n",
      "[üíæ] Parquet LSTM salvo: data/N2B/lstm_dataset_train.parquet\n",
      "[LSTM:val] linhas=20,957  windows=20,646  X=(240,12)  Y=(72,1)  chunk=10,000\n",
      "[üíæ] Parquet LSTM salvo: data/N2A/lstm_dataset_train.parquet\n",
      "[LSTM:val] linhas=20,957  windows=20,646  X=(240,12)  Y=(72,1)  chunk=10,000\n",
      "[üíæ] Parquet LSTM salvo: data/N2C/lstm_dataset_train.parquet\n",
      "[LSTM:val] linhas=20,957  windows=20,646  X=(240,12)  Y=(72,1)  chunk=10,000\n",
      "[üíæ] Parquet LSTM salvo: data/treinamento/lstm_dataset_train.parquet\n",
      "[LSTM:val] linhas=20,957  windows=20,646  X=(240,12)  Y=(72,1)  chunk=10,000\n",
      "[üíæ] Parquet LSTM salvo: data/N2B/lstm_dataset_val.parquet\n",
      "[LSTM:test] linhas=20,959  windows=20,648  X=(240,12)  Y=(72,1)  chunk=10,000\n",
      "[üíæ] Parquet LSTM salvo: data/N2C/lstm_dataset_val.parquet\n",
      "[LSTM:test] linhas=20,959  windows=20,648  X=(240,12)  Y=(72,1)  chunk=10,000\n",
      "[üíæ] Parquet LSTM salvo: data/N2A/lstm_dataset_val.parquet\n",
      "[LSTM:test] linhas=20,959  windows=20,648  X=(240,12)  Y=(72,1)  chunk=10,000\n",
      "[üíæ] Parquet LSTM salvo: data/treinamento/lstm_dataset_val.parquet\n",
      "[LSTM:test] linhas=20,959  windows=20,648  X=(240,12)  Y=(72,1)  chunk=10,000\n",
      "[üíæ] Parquet LSTM salvo: data/N2B/lstm_dataset_test.parquet\n",
      "‚úÖ Pr√©-processamento sequencial conclu√≠do (Parquet).\n",
      "üß† Pr√©-processando dados do modelo TFT\n",
      "[DIVIDIDO] train: 83,831 linhas\n",
      "[DIVIDIDO] val: 10,478 linhas\n",
      "[DIVIDIDO] test: 10,480 linhas\n",
      "üíæ Split 'train' salvo em data/N2B/tft_dataset_train.parquet (83831 linhas, grupos=3, max local time_idx=28018).\n",
      "üíæ Split 'val' salvo em data/N2B/tft_dataset_val.parquet (10478 linhas, grupos=3, max local time_idx=3506).\n",
      "üíæ Split 'test' salvo em data/N2B/tft_dataset_test.parquet (10480 linhas, grupos=3, max local time_idx=3513).\n",
      "‚úÖ Pr√©-processamento tft conclu√≠do.\n",
      "‚úÖ Finalizado N2B - mem√≥ria liberada\n",
      "------------------------------------------------------------\n",
      "üß© Resultado N2B: OK\n",
      "[üíæ] Parquet LSTM salvo: data/N2C/lstm_dataset_test.parquet\n",
      "‚úÖ Pr√©-processamento sequencial conclu√≠do (Parquet).\n",
      "üß† Pr√©-processando dados do modelo TFT\n",
      "[üíæ] Parquet LSTM salvo: data/N2A/lstm_dataset_test.parquet\n",
      "‚úÖ Pr√©-processamento sequencial conclu√≠do (Parquet).\n",
      "[üíæ] Parquet LSTM salvo: data/treinamento/lstm_dataset_test.parquetüß† Pr√©-processando dados do modelo TFT\n",
      "\n",
      "‚úÖ Pr√©-processamento sequencial conclu√≠do (Parquet).\n",
      "üß† Pr√©-processando dados do modelo TFT\n",
      "[DIVIDIDO] train: 83,831 linhas\n",
      "[DIVIDIDO] val: 10,478 linhas\n",
      "[DIVIDIDO] test: 10,480 linhas\n",
      "[DIVIDIDO] train: 83,831 linhas\n",
      "[DIVIDIDO] val: 10,478 linhas\n",
      "[DIVIDIDO] test: 10,480 linhas\n",
      "[DIVIDIDO] train: 83,831 linhas\n",
      "[DIVIDIDO] val: 10,478 linhas\n",
      "[DIVIDIDO] test: 10,480 linhas\n",
      "üíæ Split 'train' salvo em data/N2A/tft_dataset_train.parquet (83831 linhas, grupos=3, max local time_idx=28018).\n",
      "üíæ Split 'train' salvo em data/N2C/tft_dataset_train.parquet (83831 linhas, grupos=3, max local time_idx=28018).\n",
      "üíæ Split 'train' salvo em data/treinamento/tft_dataset_train.parquet (83831 linhas, grupos=3, max local time_idx=28018).\n",
      "üíæ Split 'val' salvo em data/N2C/tft_dataset_val.parquet (10478 linhas, grupos=3, max local time_idx=3506).\n",
      "üíæ Split 'val' salvo em data/N2A/tft_dataset_val.parquet (10478 linhas, grupos=3, max local time_idx=3506).\n",
      "üíæ Split 'val' salvo em data/treinamento/tft_dataset_val.parquet (10478 linhas, grupos=3, max local time_idx=3506).\n",
      "üíæ Split 'test' salvo em data/treinamento/tft_dataset_test.parquet (10480 linhas, grupos=3, max local time_idx=3513).\n",
      "‚úÖ Pr√©-processamento tft conclu√≠do.\n",
      "‚úÖ Finalizado treinamento - mem√≥ria liberada\n",
      "------------------------------------------------------------\n",
      "üß© Resultado treinamento: OK\n",
      "üíæ Split 'test' salvo em data/N2A/tft_dataset_test.parquet (10480 linhas, grupos=3, max local time_idx=3513).\n",
      "‚úÖ Pr√©-processamento tft conclu√≠do.\n",
      "üíæ Split 'test' salvo em data/N2C/tft_dataset_test.parquet (10480 linhas, grupos=3, max local time_idx=3513).\n",
      "‚úÖ Pr√©-processamento tft conclu√≠do.\n",
      "‚úÖ Finalizado N2A - mem√≥ria liberada\n",
      "------------------------------------------------------------\n",
      "üß© Resultado N2A: OK‚úÖ Finalizado N2C - mem√≥ria liberada\n",
      "------------------------------------------------------------\n",
      "\n",
      "üß© Resultado N2C: OK\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "\n",
    "\n",
    "preprocess_collector = {}\n",
    "def run_preprocessing(cfg):\n",
    "    \"\"\"Executa o pipeline completo de pr√©-processamento para um problema.\"\"\"\n",
    "    name = cfg[\"name\"]\n",
    "    try:\n",
    "        print(f\"\\nüöÄ Executando Preprocessamento do problema {name} ...\")\n",
    "\n",
    "        print(\"üß† Pr√©-processando dados do modelo linear/MLP\")\n",
    "        preproc_lin = linear_preproccess_pipeline(\n",
    "            data_dir=cfg[\"data_dir\"],\n",
    "            feature_cols=cfg[\"feats\"],\n",
    "            target_cols=cfg[\"tgts\"],\n",
    "            lag=cfg[\"lag\"],\n",
    "            lead=cfg[\"lead\"],\n",
    "            value_cols=cfg[\"vals\"],\n",
    "            country_list=cfg[\"countries\"],\n",
    "            reduced_dim=cfg.get(\"reduced_dim\", None),\n",
    "            mask_value=cfg.get(\"mask_value\", -999.0),\n",
    "        )\n",
    "\n",
    "        del preproc_lin\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"üß† Pr√©-processando dados do modelo LSTM\")\n",
    "        preproc_lstm = lstm_preproccess_pipeline(\n",
    "            data_dir=cfg[\"data_dir\"],\n",
    "            feature_cols=cfg[\"feats\"],\n",
    "            target_cols=cfg[\"tgts\"],\n",
    "            seq_len=cfg[\"lag\"],\n",
    "            lead=cfg[\"lead\"],\n",
    "            value_cols=cfg[\"vals\"],\n",
    "            country_list=cfg[\"countries\"],\n",
    "        )\n",
    "        del preproc_lstm\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "        # === Adicionado: pr√©-processamento TFT ===\n",
    "        print(\"üß† Pr√©-processando dados do modelo TFT\")\n",
    "        preproc_tft = tft_preproccess_pipeline(\n",
    "            data_dir=cfg[\"data_dir\"],\n",
    "            feature_cols=cfg[\"feats\"],\n",
    "            target_cols=cfg[\"tgts\"],\n",
    "            seq_len=cfg[\"lag\"],\n",
    "            lead=cfg[\"lead\"],\n",
    "            value_cols=cfg[\"vals\"],\n",
    "            country_list=cfg[\"countries\"],\n",
    "        )\n",
    "        del preproc_tft\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "        print(f\"‚úÖ Finalizado {name} - mem√≥ria liberada\\n{'-'*60}\")\n",
    "        return (name, \"OK\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro em {name}: {e}\")\n",
    "        return (name, f\"ERRO: {e}\")\n",
    "\n",
    "\n",
    "# ================================\n",
    "# Execu√ß√£o paralela\n",
    "# ================================\n",
    "MAX_WORKERS = min(8, len(problemas))  # ajuste conforme n√∫cleos / VRAM dispon√≠vel\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = [executor.submit(run_preprocessing, cfg) for cfg in problemas]\n",
    "\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        name, status = future.result()\n",
    "        print(f\"üß© Resultado {name}: {status}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e510a9f",
   "metadata": {},
   "source": [
    "# Cap√≠tulo 6: Treinamento dos modelos\n",
    "Este cap√≠tulo executa, por problema: Linear/MLP (configs_linear), MLP (configs_mlp), LSTM (configs_lstm) e TFT (config_tft), liberando mem√≥ria entre execu√ß√µes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2222b57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Iniciando treinamento dos modelos ...\n",
      "================================================================================\n",
      "Modelo LSTM...\n",
      "üì¶ Dataset Parquet carregado para treinamento.\n",
      "\n",
      "üöÄ Treinando modelo lstm_Medium...\n",
      "Epoch 1/100\n",
      "245/245 - 9s - 38ms/step - loss: 0.0620 - mae: 0.2041 - val_loss: 0.0479 - val_mae: 0.1831 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "245/245 - 6s - 24ms/step - loss: 0.0570 - mae: 0.1982 - val_loss: 0.0475 - val_mae: 0.1822 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "245/245 - 6s - 24ms/step - loss: 0.0570 - mae: 0.1983 - val_loss: 0.0480 - val_mae: 0.1834 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "245/245 - 6s - 24ms/step - loss: 0.0569 - mae: 0.1982 - val_loss: 0.0476 - val_mae: 0.1825 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "245/245 - 6s - 24ms/step - loss: 0.0569 - mae: 0.1982 - val_loss: 0.0477 - val_mae: 0.1827 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "245/245 - 6s - 24ms/step - loss: 0.0569 - mae: 0.1982 - val_loss: 0.0480 - val_mae: 0.1835 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "245/245 - 6s - 24ms/step - loss: 0.0569 - mae: 0.1982 - val_loss: 0.0471 - val_mae: 0.1813 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "245/245 - 6s - 24ms/step - loss: 0.0570 - mae: 0.1983 - val_loss: 0.0474 - val_mae: 0.1821 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "245/245 - 6s - 24ms/step - loss: 0.0569 - mae: 0.1982 - val_loss: 0.0476 - val_mae: 0.1824 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "245/245 - 6s - 24ms/step - loss: 0.0570 - mae: 0.1983 - val_loss: 0.0471 - val_mae: 0.1813 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "245/245 - 6s - 24ms/step - loss: 0.0569 - mae: 0.1981 - val_loss: 0.0470 - val_mae: 0.1811 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "245/245 - 6s - 24ms/step - loss: 0.0570 - mae: 0.1983 - val_loss: 0.0471 - val_mae: 0.1813 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "245/245 - 6s - 24ms/step - loss: 0.0570 - mae: 0.1983 - val_loss: 0.0468 - val_mae: 0.1806 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "245/245 - 6s - 24ms/step - loss: 0.0569 - mae: 0.1982 - val_loss: 0.0470 - val_mae: 0.1811 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "245/245 - 6s - 24ms/step - loss: 0.0570 - mae: 0.1983 - val_loss: 0.0470 - val_mae: 0.1811 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "245/245 - 6s - 24ms/step - loss: 0.0569 - mae: 0.1982 - val_loss: 0.0481 - val_mae: 0.1836 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "245/245 - 6s - 24ms/step - loss: 0.0569 - mae: 0.1982 - val_loss: 0.0477 - val_mae: 0.1827 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "245/245 - 6s - 24ms/step - loss: 0.0569 - mae: 0.1982 - val_loss: 0.0475 - val_mae: 0.1822 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "245/245 - 6s - 24ms/step - loss: 0.0569 - mae: 0.1982 - val_loss: 0.0472 - val_mae: 0.1815 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "245/245 - 6s - 24ms/step - loss: 0.0569 - mae: 0.1982 - val_loss: 0.0475 - val_mae: 0.1823 - learning_rate: 5.0000e-04\n",
      "Epoch 21/100\n",
      "245/245 - 6s - 23ms/step - loss: 0.0569 - mae: 0.1982 - val_loss: 0.0475 - val_mae: 0.1823 - learning_rate: 5.0000e-04\n",
      "Epoch 22/100\n",
      "245/245 - 6s - 24ms/step - loss: 0.0569 - mae: 0.1981 - val_loss: 0.0475 - val_mae: 0.1822 - learning_rate: 5.0000e-04\n",
      "Epoch 23/100\n",
      "245/245 - 6s - 25ms/step - loss: 0.0569 - mae: 0.1982 - val_loss: 0.0473 - val_mae: 0.1818 - learning_rate: 5.0000e-04\n",
      "Epoch 24/100\n",
      "245/245 - 6s - 24ms/step - loss: 0.0569 - mae: 0.1982 - val_loss: 0.0472 - val_mae: 0.1815 - learning_rate: 5.0000e-04\n",
      "Epoch 25/100\n",
      "245/245 - 6s - 24ms/step - loss: 0.0569 - mae: 0.1982 - val_loss: 0.0476 - val_mae: 0.1825 - learning_rate: 5.0000e-04\n",
      "Epoch 26/100\n",
      "245/245 - 6s - 23ms/step - loss: 0.0569 - mae: 0.1981 - val_loss: 0.0473 - val_mae: 0.1816 - learning_rate: 2.5000e-04\n",
      "Epoch 27/100\n",
      "245/245 - 6s - 24ms/step - loss: 0.0569 - mae: 0.1982 - val_loss: 0.0472 - val_mae: 0.1816 - learning_rate: 2.5000e-04\n",
      "Epoch 28/100\n",
      "245/245 - 6s - 24ms/step - loss: 0.0569 - mae: 0.1982 - val_loss: 0.0476 - val_mae: 0.1824 - learning_rate: 2.5000e-04\n",
      "Epoch 29/100\n",
      "245/245 - 6s - 24ms/step - loss: 0.0569 - mae: 0.1982 - val_loss: 0.0475 - val_mae: 0.1821 - learning_rate: 2.5000e-04\n",
      "Epoch 30/100\n",
      "245/245 - 6s - 24ms/step - loss: 0.0569 - mae: 0.1982 - val_loss: 0.0474 - val_mae: 0.1821 - learning_rate: 2.5000e-04\n",
      "Epoch 31/100\n",
      "245/245 - 6s - 24ms/step - loss: 0.0569 - mae: 0.1981 - val_loss: 0.0473 - val_mae: 0.1818 - learning_rate: 2.5000e-04\n",
      "Epoch 32/100\n",
      "245/245 - 6s - 24ms/step - loss: 0.0569 - mae: 0.1982 - val_loss: 0.0474 - val_mae: 0.1819 - learning_rate: 1.2500e-04\n",
      "Epoch 33/100\n",
      "245/245 - 6s - 24ms/step - loss: 0.0569 - mae: 0.1982 - val_loss: 0.0473 - val_mae: 0.1817 - learning_rate: 1.2500e-04\n",
      "‚úÖ lstm_Medium conclu√≠do - Val Loss: 0.046843\n",
      "[üíæ] Modelo salvo em: ./modelos/treinamento/lstm_Medium.keras\n",
      "‚úÖ Problema treinamento conclu√≠do ‚Äî mem√≥ria limpa\n",
      "------------------------------------------------------------\n",
      "‚è±Ô∏è  Tempo de treino lstm: 201.56 segundos\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gc\n",
    "# Loop de treinamento sequencial: Linear/MLP -> MLP -> LSTM -> TFT\n",
    "\n",
    "tempo_treino = {}\n",
    "\n",
    "# Carrega configura√ß√£o de treinamento\n",
    "cfg = []\n",
    "for tmp in problemas:\n",
    "    if tmp[\"name\"] == \"treinamento\":\n",
    "        cfg = tmp\n",
    "        break\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "if not cfg:\n",
    "    print(\"sem configura√ß√£o de 'treinamento' configurada na lista de problemas\")\n",
    "else:\n",
    "    name = cfg[\"name\"]\n",
    "    print(f\"\\nüöÄ Iniciando treinamento dos modelos ...\")\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"Modelo Linear...\")\n",
    "    # Treinamento Linear\n",
    "    try:\n",
    "        tempo_treino[\"linear\"] = {}\n",
    "        tempo_treino[\"linear\"][\"inicio\"] = time.time()\n",
    "        models_linear = linear_train_pipeline(\n",
    "            problem_name=name,\n",
    "            data_dir=cfg[\"data_dir\"],\n",
    "            batch_size=256,\n",
    "            configs=configs_linear,\n",
    "        )\n",
    "        del models_linear\n",
    "        tempo_treino[\"linear\"][\"fim\"] = time.time()\n",
    "        tempo_treino[\"linear\"][\"duracao\"] = tempo_treino[\"linear\"][\"fim\"] - tempo_treino[\"linear\"][\"inicio\"]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao treinar Linear para {name}: {e}\")\n",
    "    finally:\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"Modelo MLP...\")\n",
    "    # Treinamento MLP (configs_mlp)\n",
    "    try:\n",
    "        tempo_treino[\"mlp\"] = {}\n",
    "        tempo_treino[\"mlp\"][\"inicio\"] = time.time()\n",
    "        models_mlp = linear_train_pipeline(\n",
    "            problem_name=name,\n",
    "            data_dir=cfg[\"data_dir\"],\n",
    "            batch_size=256,\n",
    "            configs=configs_mlp,\n",
    "        )\n",
    "        del models_mlp\n",
    "        tempo_treino[\"mlp\"][\"fim\"] = time.time()\n",
    "        tempo_treino[\"mlp\"][\"duracao\"] = tempo_treino[\"mlp\"][\"fim\"] - tempo_treino[\"mlp\"][\"inicio\"]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao treinar MLP (configs_mlp) para {name}: {e}\")\n",
    "    finally:\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"Modelo LSTM...\")\n",
    "    # Treinamento LSTM\n",
    "    try:\n",
    "        tempo_treino[\"lstm\"] = {}\n",
    "        tempo_treino[\"lstm\"][\"inicio\"] = time.time()\n",
    "        models_lstm = lstm_train_pipeline(\n",
    "            problem_name=name,\n",
    "            data_dir=cfg[\"data_dir\"],\n",
    "            seq_len=cfg.get(\"lag\") or cfg.get(\"seq_len\"),\n",
    "            batch_size=256,\n",
    "            configs=configs_lstm,\n",
    "        )\n",
    "        del models_lstm\n",
    "        tempo_treino[\"lstm\"][\"fim\"] = time.time()\n",
    "        tempo_treino[\"lstm\"][\"duracao\"] = tempo_treino[\"lstm\"][\"fim\"] - tempo_treino[\"lstm\"][\"inicio\"]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao treinar LSTM para {name}: {e}\")\n",
    "    finally:\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"Modelo TFT...\")\n",
    "    # Treinamento TFT (Temporal Fusion Transformer)\n",
    "    try:\n",
    "        tempo_treino[\"tft\"] = {}\n",
    "        tempo_treino[\"tft\"][\"inicio\"] = time.time()\n",
    "        models_tft = tft_train_pipeline(\n",
    "            problem_name=name,\n",
    "            data_dir=cfg[\"data_dir\"],\n",
    "            feature_cols=cfg.get(\"feats\") or cfg.get(\"feature_cols\"),\n",
    "            target_cols=cfg.get(\"tgts\") or cfg.get(\"target_cols\"),\n",
    "            seq_len=cfg.get(\"lag\"),\n",
    "            lead=cfg.get(\"lead\"),\n",
    "            batch_size=256,\n",
    "            configs=config_tft,\n",
    "        )\n",
    "        # libera refer√™ncia ao retorno (modelos serializados internamente)\n",
    "        del models_tft\n",
    "        tempo_treino[\"tft\"][\"fim\"] = time.time()\n",
    "        tempo_treino[\"tft\"][\"duracao\"] = tempo_treino[\"tft\"][\"fim\"] - tempo_treino[\"tft\"][\"inicio\"]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao treinar TFT para {name}: {e}\")\n",
    "\n",
    "    finally:\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "    print(f\"‚úÖ Problema {name} conclu√≠do ‚Äî mem√≥ria limpa\\n{'-'*60}\")\n",
    "    for modelo, tempo in tempo_treino.items():\n",
    "        duracao = tempo.get(\"duracao\", 0)\n",
    "        print(f\"‚è±Ô∏è  Tempo de treino {modelo}: {duracao:.2f} segundos\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfc_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

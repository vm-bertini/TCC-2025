{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c49e9f73",
   "metadata": {},
   "source": [
    "# Introdu√ß√£o\n",
    "\n",
    "Esse Notebook ser√° respons√°vel pelo preprocessamento dos dados contidos em ./data/raw para formatos compat√≠veis e otimizados para o treinamento de cada modelo\n",
    "As defini√ß√µes dos par√¢metros do treinamento aos quais os modelos dever√£o solucionar j√° foram definidas no notebook \"Coleta de Dados\"\n",
    "\n",
    "Modelos a serem Criados:\n",
    "\n",
    "1. Modelo Linear: MLP sem fun√ß√µes de ativa√ß√£o, composta apenas de somas lineares\n",
    "2. MLP: rede neural - efetivamente identica ao modelo linear, no entanto, apresenta fun√ß√£o de ativa√ß√£o ao final do somat√≥rio de fun√ß√µes lineares\n",
    "3. LSTM: Um modelo de rede neural recorrente, com capacidade de diferencia√ß√£o de informa√ß√£o de curto e longo prazo\n",
    "4. TFT: Modelo baseado em LLMs desenvolvido pela microsoft - servir√° como um comparativo mais moderno\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1149ce94",
   "metadata": {},
   "source": [
    "## Depend√™ncias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce23fd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch=2.5.1+cu121 | cuda=True\n",
      "PyArrow dispon√≠vel: 22.0.0\n",
      "üì¶ Installing python-dotenv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-nccl-cu12 (/home/victor-bertini/Documentos/tcc_2025/TCC-2025/tfc_venv/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-nccl-cu12 (/home/victor-bertini/Documentos/tcc_2025/TCC-2025/tfc_venv/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-nccl-cu12 (/home/victor-bertini/Documentos/tcc_2025/TCC-2025/tfc_venv/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Installing scikit-learn...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-nccl-cu12 (/home/victor-bertini/Documentos/tcc_2025/TCC-2025/tfc_venv/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-nccl-cu12 (/home/victor-bertini/Documentos/tcc_2025/TCC-2025/tfc_venv/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-nccl-cu12 (/home/victor-bertini/Documentos/tcc_2025/TCC-2025/tfc_venv/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All dependencies ready\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import importlib\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "\n",
    "# ==============================================\n",
    "# TORCH INSTALL (GPU-safe)\n",
    "# ==============================================\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"torch={torch.__version__} | cuda={torch.cuda.is_available()}\")\n",
    "except Exception:\n",
    "    print(\"‚öôÔ∏è Installing PyTorch with correct CUDA runtime...\")\n",
    "    has_gpu = shutil.which(\"nvidia-smi\") is not None\n",
    "\n",
    "    # Clean any broken torch/CUDA stack\n",
    "    os.system(\"pip uninstall -y torch torchvision torchaudio nvidia-*\")\n",
    "\n",
    "    if has_gpu:\n",
    "        # CUDA 12.1 build ‚Äî safest baseline for current PyTorch\n",
    "        os.system(\n",
    "            \"pip install torch==2.5.1+cu121 torchvision==0.20.1+cu121 torchaudio==2.5.1+cu121 \"\n",
    "            \"--index-url https://download.pytorch.org/whl/cu121 --quiet\"\n",
    "        )\n",
    "    else:\n",
    "        os.system(\n",
    "            \"pip install torch torchvision torchaudio \"\n",
    "            \"--index-url https://download.pytorch.org/whl/cpu --quiet\"\n",
    "        )\n",
    "\n",
    "    import torch\n",
    "    print(f\"‚úÖ torch={torch.__version__} | cuda={torch.cuda.is_available()}\")\n",
    "\n",
    "# ==============================================\n",
    "# TFT & PIPELINE LIBS\n",
    "# ==============================================\n",
    "try:\n",
    "    import pytorch_lightning\n",
    "    import pytorch_forecasting\n",
    "except ImportError:\n",
    "    print(\"üì¶ Installing PyTorch Lightning + Forecasting...\")\n",
    "    os.system(\"pip install pytorch-lightning pytorch-forecasting --quiet\")\n",
    "\n",
    "# ==============================================\n",
    "# PARQUET / I/O LIBS\n",
    "# ==============================================\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    print(f\"PyArrow dispon√≠vel: {pa.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"üì¶ Installing PyArrow >= 18...\")\n",
    "    os.system(\"pip install --upgrade 'pyarrow>=18' --quiet\")\n",
    "    importlib.invalidate_caches()\n",
    "    import pyarrow as pa\n",
    "    print(f\"PyArrow instalado: {pa.__version__}\")\n",
    "\n",
    "try:\n",
    "    import fastparquet\n",
    "    print(\"fastparquet dispon√≠vel (opcional)\")\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# ==============================================\n",
    "# BASE LIBRARIES\n",
    "# ==============================================\n",
    "base_libs = [\n",
    "    \"numpy\",\n",
    "    \"python-dotenv\",\n",
    "    \"pandas\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"scikit-learn\",\n",
    "    \"tensorflow[and-cuda]\",\n",
    "    \"keras\",\n",
    "    \"lxml\",\n",
    "    \"pytz\",\n",
    "    \"optuna\",\n",
    "]\n",
    "\n",
    "for lib in base_libs:\n",
    "    try:\n",
    "        __import__(lib.split(\"[\")[0])\n",
    "    except ImportError:\n",
    "        print(f\"üì¶ Installing {lib}...\")\n",
    "        os.system(f\"pip install --quiet {lib}\")\n",
    "\n",
    "print(\"‚úÖ All dependencies ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cc7e78",
   "metadata": {},
   "source": [
    "## VARI√ÅVEIS NECESS√ÅRIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d89d36dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GPU detected (/physical_device:GPU:0) - using mixed precision.\n"
     ]
    }
   ],
   "source": [
    "# Imports centralizados\n",
    "import os, json, time, gc, concurrent.futures, datetime\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TensorFlow / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, mixed_precision\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping as TFEarlyStopping, ReduceLROnPlateau as TFReduceLROnPlateau\n",
    "\n",
    "# PyTorch / Lightning / TFT\n",
    "import torch\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import EarlyStopping as LGEarlyStopping, LearningRateMonitor as LGLearningRateMonitor, ModelCheckpoint as LGModelCheckpoint\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "# Parquet\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Local preprocessor classes\n",
    "from preprocessor import LinearPreprocessor, LSTMPreprocessor, TFTPreprocessor\n",
    "\n",
    "# Silenciando Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)  # last resort\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# GPU CONFIGURATION\n",
    "# ==============================================\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    mixed_precision.set_global_policy('mixed_float16')\n",
    "    print(f\"‚úÖ GPU detected ({gpus[0].name}) - using mixed precision.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected, running on CPU.\")\n",
    "\n",
    "# Carregar vari√°veis de ambiente do .env\n",
    "load_dotenv()\n",
    "# ---------------- CONFIG ---------------- #\n",
    "COUNTRY_DOMAINS = {\n",
    "    \"FR\": {\"domain\": \"10YFR-RTE------C\"},\n",
    "    \"ES\": {\"domain\": \"10YES-REE------0\"},\n",
    "    \"PT\": {\"domain\": \"10YPT-REN------W\"}\n",
    "}\n",
    "\n",
    "DATA_ITEMS = [\n",
    "    {'key': 'load_total', 'documentType': 'A65', 'processType': 'A16', 'domainParam': 'outBiddingZone_Domain', 'parser': 'load'},\n",
    "    {'key': 'market_prices', 'documentType': 'A44', 'processType': 'A07', 'domainParamIn': 'in_Domain', 'domainParamOut': 'out_Domain', 'parser': 'price'}\n",
    "]\n",
    "\n",
    "ENTSOE_TOKEN = os.environ.get(\"ENTSOE_SECURITY_TOKEN\")\n",
    "BASE_URL = \"https://web-api.tp.entsoe.eu/api\"\n",
    "MAX_WORKERS = 100\n",
    "RAW_DIR = os.path.join(\"data\", \"raw\")\n",
    "PARQUET_COMPRESSION = \"zstd\"\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "EPOCHS = 200\n",
    "\n",
    "# ==============================================\n",
    "# DICION√ÅRIO DE Treinamento\n",
    "# ==============================================\n",
    "treinamento = {\n",
    "    \"name\": \"treinamento\",\n",
    "    \"data_dir\": \"data/treinamento\",\n",
    "    \"feats\": [\"country\", \"datetime\", \"quantity_MW\"],\n",
    "    \"tgts\": [\"quantity_MW\"],\n",
    "    \"vals\": [\"quantity_MW\"],\n",
    "    \"lag\": 10 * 24,\n",
    "    \"lead\": 3 * 24,\n",
    "    \"seq_len\": 10 * 24,\n",
    "    \"countries\": list(COUNTRY_DOMAINS),\n",
    "    \"noise\": False,\n",
    "    \"train\": True\n",
    "}\n",
    "\n",
    "# Gerador do dataset CV\n",
    "cv = {\n",
    "    \"name\": \"CV\",\n",
    "    \"data_dir\": \"data/CV\",\n",
    "    \"feats\": [\"country\", \"datetime\", \"quantity_MW\"],\n",
    "    \"tgts\": [\"quantity_MW\"],\n",
    "    \"vals\": [\"quantity_MW\"],\n",
    "    \"lag\": 10 * 24,\n",
    "    \"lead\": 3 * 24,\n",
    "    \"seq_len\": 10 * 24,\n",
    "    \"countries\": list(COUNTRY_DOMAINS),\n",
    "    \"noise\": False,\n",
    "    \"size\": 0.2\n",
    "}\n",
    "\n",
    "perguntas = [\n",
    "    {\n",
    "        \"name\": \"N1A\",\n",
    "        \"data_dir\": \"data/N1A\",\n",
    "        \"feats\": [\"country\", \"datetime\", \"quantity_MW\"],\n",
    "        \"tgts\": [\"quantity_MW\"],\n",
    "        \"vals\": [\"quantity_MW\"],\n",
    "        \"lag\": 10 * 24,\n",
    "        \"lead\": 3 * 24,\n",
    "        \"seq_len\": 3 * 24,\n",
    "        \"countries\": [\"ES\"],\n",
    "        \"noise\": False\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"N1B\",\n",
    "        \"data_dir\": \"data/N1B\",\n",
    "        \"feats\": [\"country\", \"datetime\", \"quantity_MW\"],\n",
    "        \"tgts\": [\"quantity_MW\"],\n",
    "        \"vals\": [\"quantity_MW\"],\n",
    "        \"lag\": 10 * 24,\n",
    "        \"lead\": 3 * 24,\n",
    "        \"seq_len\": 7 * 24,\n",
    "        \"countries\": [\"ES\"],\n",
    "        \"noise\": False\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"N1C\",\n",
    "        \"data_dir\": \"data/N1C\",\n",
    "        \"feats\": [\"country\", \"datetime\", \"quantity_MW\"],\n",
    "        \"tgts\": [\"quantity_MW\"],\n",
    "        \"vals\": [\"quantity_MW\"],\n",
    "        \"lag\": 10 * 24,\n",
    "        \"lead\": 3 * 24,\n",
    "        \"seq_len\": 10 * 24,\n",
    "        \"countries\": [\"ES\"],\n",
    "        \"noise\": False\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"N2A\",\n",
    "        \"data_dir\": \"data/N2A\",\n",
    "        \"feats\": [\"country\", \"datetime\", \"quantity_MW\"],\n",
    "        \"tgts\": [\"quantity_MW\"],\n",
    "        \"vals\": [\"quantity_MW\"],\n",
    "        \"lag\": 10 * 24,\n",
    "        \"lead\": 3 * 24,\n",
    "        \"seq_len\": 3 * 24,\n",
    "        \"countries\": COUNTRY_DOMAINS.keys(),\n",
    "        \"noise\": False\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"N2B\",\n",
    "        \"data_dir\": \"data/N2B\",\n",
    "        \"feats\": [\"country\", \"datetime\", \"quantity_MW\"],\n",
    "        \"tgts\": [\"quantity_MW\"],\n",
    "        \"vals\": [\"quantity_MW\"],\n",
    "        \"lag\": 10 * 24,\n",
    "        \"lead\": 3 * 24,\n",
    "        \"seq_len\": 7 * 24,\n",
    "        \"countries\": COUNTRY_DOMAINS.keys(),\n",
    "        \"noise\": False\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"N2C\",\n",
    "        \"data_dir\": \"data/N2C\",\n",
    "        \"feats\": [\"country\", \"datetime\", \"quantity_MW\"],\n",
    "        \"tgts\": [\"quantity_MW\"],\n",
    "        \"vals\": [\"quantity_MW\"],\n",
    "        \"lag\": 10 * 24,\n",
    "        \"lead\": 3 * 24,\n",
    "        \"seq_len\": 10 * 24,\n",
    "        \"countries\": COUNTRY_DOMAINS.keys(),\n",
    "        \"noise\": False\n",
    "    }\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5b47ef",
   "metadata": {},
   "source": [
    "# Cap√≠tulo 1: Pr√©processamento de dados\n",
    "\n",
    "Etapa de contru√ß√£o da pipelines de pre-processamento de dados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa7c121",
   "metadata": {},
   "source": [
    "## Preprocessamento do Modelo Linear\n",
    "\n",
    "Esse modelo deve ser√° contruido a partir de lags e leads passados como par√¢metros na fun√ß√£o, resultando na contru√ß√£o de novas colunas lead lag, assim gerando uma flat matrix 2D que ser√° usada no modelo linear\n",
    "\n",
    "Observa√ß√£o importante: lag e lead s√£o inteiros e representam o m√°ximo de passos; o pipeline expande para intervalos 1..N automaticamente. Por exemplo, lag=96 gera features com defasagens de 1 a 96; lead=96 gera alvos de 1 a 96.\n",
    "\n",
    "Os arquivos do modelo ser√£o salvos em Parquet, j√° que o modelo linear ser√° constru√≠do usando TensorFlow (carregado via Parquet‚ÜíNumPy‚Üítf.data)\n",
    "\n",
    "No caso o Preprocessador do modelo linear ser√° igual ao pr√©-processador do MLP "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e82e82",
   "metadata": {},
   "source": [
    "## Preprocessamento do Modelo LSTM\n",
    "\n",
    "O preprocessador do LSTM deve ser capaz de gerar tensores de dimens√£o 3, no seguinte formato (n_batch, seq_len, features) e (n_batch, seq_len, features)  para alimenta√ß√£o do modelo e valida√ß√£o das m√©tricas do modelo\n",
    "\n",
    "Os arquivos do modelo ser√£o salvos em Parquet (colunas X e Y como listas fixas), e carregados via Parquet‚ÜíNumPy‚Üítf.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd309dd",
   "metadata": {},
   "source": [
    "## Preprocessamento do Modelo TFT (PyTorch)\n",
    "\n",
    "O preprocessador do LSTM deve ser capaz de gerar tensores de dimens√£o 3, no seguinte formato (n_batch, seq_len, features) e (n_batch, seq_len, features)  para alimenta√ß√£o do modelo e valida√ß√£o das m√©tricas do modelo\n",
    "\n",
    "Os artefatos de dados ser√£o salvos em Parquet (Keras e TFT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8ece38",
   "metadata": {},
   "source": [
    "# Cap√≠tulo 2 ‚Äî Constru√ß√£o dos Modelos\n",
    "\n",
    "A seguir, definimos construtores simples e eficientes para cada modelo (Linear, LSTM, TFT e TimesFM),\n",
    "prontos para uso em rotinas de otimiza√ß√£o de hiperpar√¢metros (por exemplo, Optuna). Cada construtor\n",
    "recebe um dicion√°rio de par√¢metros (`params`) e retorna um modelo compilado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556d5525",
   "metadata": {},
   "source": [
    "## Constru√ß√£o do Modelo Linear/MLP\n",
    "\n",
    "Objetivo: Criar um regressor simples (MLP), com capacidade de redu√ß√£o para um modelo apenas lienar - pela exclus√£o da camada de ativa√ß√£o - para prever `target_cols` a partir de `feature_cols`.\n",
    "\n",
    "Contrato r√°pido:\n",
    "- Entrada: vetor de tamanho `x_dim` (n√∫mero de features)\n",
    "- Sa√≠da: vetor de tamanho `y_dim` (n√∫mero de targets)\n",
    "- Par√¢metros (exemplos): hidden_units, activation, dropout, lr, l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "727619dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear/MLP model + loaders (Parquet only)\n",
    "# (imports centralizados na c√©lula 5)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "\n",
    "def build_linear_model(x_dim: int, y_dim: int, params: dict, linear: bool = False) -> keras.Model:\n",
    "    \"\"\"Builds either a pure linear or a simple MLP model for regression.\"\"\"\n",
    "\n",
    "    lr = params.get(\"lr\", 1e-3)\n",
    "    l2 = params.get(\"l2\", 0.0)\n",
    "    dropout = params.get(\"dropout\", 0.0)\n",
    "    hidden_units = params.get(\"hidden_units\", [128, 64])\n",
    "    activation = params.get(\"activation\", \"leaky_relu\")\n",
    "\n",
    "    inputs = keras.Input(shape=(x_dim,), name=\"features\")\n",
    "\n",
    "    if linear:\n",
    "        x = inputs\n",
    "    else:\n",
    "        x = inputs\n",
    "        for i, units in enumerate(hidden_units):\n",
    "            x = layers.Dense(units, activation=activation,\n",
    "                             kernel_regularizer=regularizers.l2(l2))(x)\n",
    "            if dropout > 0:\n",
    "                x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    outputs = layers.Dense(y_dim, activation=None, name=\"targets\")(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs, name=\"linear_model\" if linear else \"mlp_model\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=\"mse\",\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_mlp_model(x_dim: int, y_dim: int, params: Dict[str, Any]) -> keras.Model:\n",
    "\n",
    "    # --- HYPERPARAMS ---\n",
    "    hidden_units = params.get('hidden_units', [128, 64])  \n",
    "    dropout = float(params.get('dropout', 0.0))            \n",
    "    lr = float(params.get('lr', 1e-3))                     \n",
    "    l2 = float(params.get('l2', 1e-6))                     \n",
    "    activation = params.get('act', 'relu')\n",
    "\n",
    "    # --- MODEL ---\n",
    "    inputs = keras.Input(shape=(x_dim,), name='features')\n",
    "    x = inputs  # üö´ no LayerNormalization()\n",
    "\n",
    "    # hidden layers\n",
    "    for i, units in enumerate(hidden_units):\n",
    "        x = layers.Dense(\n",
    "            units,\n",
    "            activation=activation,\n",
    "            kernel_initializer='he_normal',\n",
    "            kernel_regularizer=keras.regularizers.l2(l2),\n",
    "            name=f\"dense_{i}\"\n",
    "        )(x)\n",
    "\n",
    "        if dropout > 0:\n",
    "            x = layers.Dropout(dropout, name=f\"drop_{i}\")(x)\n",
    "\n",
    "    # output layer\n",
    "    outputs = layers.Dense(y_dim, activation=None, name='targets')(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs, name='mlp_model')\n",
    "\n",
    "    # --- OPTIMIZER ---\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0c1b3c",
   "metadata": {},
   "source": [
    "## Constru√ß√£o do Modelo LSTM\n",
    "\n",
    "Objetivo: um regressor denso simples (MLP) para prever `target_cols` a partir de `feature_cols`.\n",
    "\n",
    "Contrato r√°pido:\n",
    "- Entrada: vetor de tamanho `x_dim` (n√∫mero de features)\n",
    "- Sa√≠da: vetor de tamanho `y_dim` (n√∫mero de targets)\n",
    "- Par√¢metros (exemplos): hidden_units, activation, dropout, lr, l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71b6c091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "def build_lstm_model(\n",
    "    seq_len=240,\n",
    "    x_dim_num=12,\n",
    "    y_dim=1,\n",
    "    lead=72,\n",
    "    num_countries=4,\n",
    "    emb_dim=8,\n",
    "    lstm_units=128,\n",
    "    dense_units=256,\n",
    "    dropout=0.2,\n",
    "    lr=2e-3,\n",
    "):\n",
    "    num_in = keras.Input(shape=(None, x_dim_num), name=\"num_feats\")\n",
    "    country_in = keras.Input(shape=(1,), dtype=\"int32\", name=\"country_id\")\n",
    "\n",
    "    emb = layers.Embedding(num_countries, emb_dim, name=\"country_emb\")(country_in)\n",
    "    emb = layers.Reshape((1, emb_dim))(emb)\n",
    "\n",
    "    # ‚úÖ repeat dynamically with tf.tile (preserves shape)\n",
    "    def repeat_to_seq_len(inputs):\n",
    "        emb, feats = inputs\n",
    "        seq_len = tf.shape(feats)[1]\n",
    "        emb_tiled = tf.tile(emb, [1, seq_len, 1])  # (batch, seq_len, emb_dim)\n",
    "        return emb_tiled\n",
    "\n",
    "    emb_rep = layers.Lambda(repeat_to_seq_len, name=\"emb_repeat_dyn\")([emb, num_in])\n",
    "\n",
    "    # ‚úÖ now shapes match: (None, seq_len, 12) and (None, seq_len, emb_dim)\n",
    "    x = layers.Concatenate(axis=-1, name=\"concat_feats\")([num_in, emb_rep])\n",
    "\n",
    "    x = layers.Bidirectional(\n",
    "        layers.LSTM(lstm_units, return_sequences=True, dropout=dropout),\n",
    "        name=\"bilstm_1\"\n",
    "    )(x)\n",
    "    x = layers.LSTM(lstm_units, return_sequences=True, dropout=dropout, name=\"lstm_2\")(x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(name=\"temporal_avg\")(x)\n",
    "    x = layers.Dense(dense_units, activation=\"gelu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Dense(lead * y_dim, activation=None, name=\"dense_output\")(x)\n",
    "    out = layers.Reshape((lead, y_dim), name=\"reshape_output\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=[num_in, country_in], outputs=out)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr, clipnorm=1.0),\n",
    "        loss=\"mse\",\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b72a517",
   "metadata": {},
   "source": [
    "## Constru√ß√£o do Modelo TFT (Temporal Fusion Transformer)\n",
    "\n",
    "**Objetivo:** prever `target_cols` a partir de `feature_cols` utilizando a implementa√ß√£o oficial `keras_tft`,  \n",
    "que integra **sele√ß√£o de vari√°veis din√¢micas**, **blocos LSTM**, **aten√ß√£o temporal multi-cabe√ßas** e **gating residual** em um √∫nico modelo interpretable.\n",
    "\n",
    "**Contrato r√°pido:**\n",
    "- **Entrada:** `tf.data.Dataset` com tensores no formato `(batch, seq_len, x_dim)`  \n",
    "- **Sa√≠da:** tensor cont√≠nuo de tamanho `y_dim` *(ou `dec_len √ó y_dim` para horizontes m√∫ltiplos)*\n",
    "\n",
    "**Par√¢metros (exemplos):**  \n",
    "`hidden_size` (tamanho interno das camadas GRN) ¬∑ `lstm_layers` ¬∑ `num_heads` (aten√ß√£o) ¬∑ `dropout` ¬∑ `learning_rate` ¬∑ `output_size` ¬∑ `seq_len`\n",
    "\n",
    "**Componentes internos (`keras_tft`):**  \n",
    "Variable Selection Network ‚Üí LSTM Encoder/Decoder ‚Üí Multi-Head Temporal Attention ‚Üí Gated Residual Network ‚Üí Camada de proje√ß√£o final\n",
    "\n",
    "**Compatibilidade:**  \n",
    "Totalmente compat√≠vel com o pipeline atual em Parquet do LSTM, recebendo o mesmo formato de dados  \n",
    "(`(batch, seq_len, features)`), permitindo substitui√ß√£o direta do modelo sem alterar o pr√©-processamento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ce31bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFT model builder (imports centralizados na c√©lula 5)\n",
    "\n",
    "def build_tft_model(\n",
    "    params: Dict[str, Any]\n",
    "):\n",
    "    \"\"\"\n",
    "    Constr√≥i um Temporal Fusion Transformer (TFT) com par√¢metros configur√°veis.\n",
    "\n",
    "    Args:\n",
    "        x_dim: n√∫mero de features de entrada\n",
    "        y_dim: n√∫mero de targets\n",
    "        seq_len: tamanho da sequ√™ncia temporal\n",
    "        params: dicion√°rio de hiperpar√¢metros (hidden_size, dropout, lstm_layers, etc.)\n",
    "        max_encoder_length: tamanho da janela passada (encoder)\n",
    "        max_prediction_length: tamanho do horizonte de previs√£o (decoder)\n",
    "    \"\"\"\n",
    "\n",
    "    hidden_size = int(params.get(\"hidden_size\", 64))\n",
    "    dropout = float(params.get(\"dropout\", 0.1))\n",
    "    lstm_layers = int(params.get(\"lstm_layers\", 1))\n",
    "    attention_head_size = int(params.get(\"num_heads\", 4))\n",
    "    lr = float(params.get(\"lr\", 1e-3))\n",
    "\n",
    "    model = TemporalFusionTransformer.from_dataset(\n",
    "        params[\"dataset\"],  # dataset preparado via TimeSeriesDataSet\n",
    "        learning_rate=lr,\n",
    "        hidden_size=hidden_size,\n",
    "        dropout=dropout,\n",
    "        lstm_layers=lstm_layers,\n",
    "        attention_head_size=attention_head_size,\n",
    "        loss=QuantileLoss([0.5]),\n",
    "        log_interval=10,\n",
    "        log_val_interval=1\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afbf736",
   "metadata": {},
   "source": [
    "# Cap√≠tulo 3 - Contru√ß√£o da Pipelines de dados dos modelos\n",
    "\n",
    "A fun√ß√£o de pipeline organiza o fluxo de dados para, de forma mais concisa e organizada, treinar o modelo, sendo capaz de mostrar a progress√£o das perdas a medida que as √©pocas de treinamento passam - Esse display est√© dispon√≠vel no notebook \"Resultados\"\n",
    "\n",
    "O resultado da pipeline √© um gr√°fico com a evolu√ß√£o de todas as m√©tricas e o salvamento do modelo treinado dentro da pasta ./modelo/{Nome_Problema}/{Nome_Modelo}\n",
    "\n",
    "Assim podendo ser facilmente reutilizado futuramente para um notebook comparativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b77058c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilit√°rio para salvar modelos Keras (imports centralizados na c√©lula 5)\n",
    "\n",
    "def save_model(model, path: str, format: str | None = None, include_optimizer: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Salva um modelo Keras em formato padronizado.\n",
    "\n",
    "    Regras:\n",
    "    - Se `path` terminar com .keras ou .h5, salva exatamente nesse arquivo.\n",
    "    - Se `format == 'savedmodel'`, salva no diret√≥rio indicado (SavedModel).\n",
    "    - Caso contr√°rio, adiciona sufixo .keras a `path` (arquivo √∫nico Keras v3).\n",
    "\n",
    "    Retorna o caminho final salvo (arquivo ou diret√≥rio) e grava um meta.json ao lado.\n",
    "    \"\"\"\n",
    "    # Infer√™ncia de formato por extens√£o\n",
    "    ext = None\n",
    "    lower = path.lower()\n",
    "    if lower.endswith(\".keras\"):\n",
    "        ext = \"keras\"\n",
    "    elif lower.endswith(\".h5\") or lower.endswith(\".hdf5\"):\n",
    "        ext = \"h5\"\n",
    "\n",
    "    # Normaliza√ß√£o de destino\n",
    "    if format == \"savedmodel\":\n",
    "        # Diret√≥rio SavedModel\n",
    "        save_dir = path\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        model.save(save_dir, include_optimizer=include_optimizer)\n",
    "        meta_path = os.path.join(save_dir, \"model.meta.json\")\n",
    "        final_path = save_dir\n",
    "    else:\n",
    "        if ext is None:\n",
    "            # For√ßa arquivo .keras por padr√£o\n",
    "            path = f\"{path}.keras\"\n",
    "            ext = \"keras\"\n",
    "        # Cria diret√≥rio pai\n",
    "        parent = os.path.dirname(path) or \".\"\n",
    "        os.makedirs(parent, exist_ok=True)\n",
    "        # Salva arquivo √∫nico\n",
    "        model.save(path, include_optimizer=include_optimizer)\n",
    "        meta_path = f\"{path}.meta.json\"\n",
    "        final_path = path\n",
    "\n",
    "    # Meta b√°sico ao lado do artefato\n",
    "    try:\n",
    "        meta = {\n",
    "            \"saved_at\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
    "            \"keras_version\": getattr(model, \"keras_version\", None),\n",
    "            \"model_name\": getattr(model, \"name\", None),\n",
    "            \"trainable_params\": int(getattr(model, \"count_params\", lambda: 0)()),\n",
    "            \"format\": \"savedmodel\" if format == \"savedmodel\" else ext,\n",
    "        }\n",
    "        with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Falha ao escrever meta.json: {e}\")\n",
    "\n",
    "    print(f\"[üíæ] Modelo salvo em: {final_path}\")\n",
    "    return final_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d03c22",
   "metadata": {},
   "source": [
    "## Pipeline dos Modelos Lineares\n",
    "\n",
    "Pipeline de preprocessamento e de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e97fb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines dos Modelos Lineares (imports centralizados na c√©lula 5)\n",
    "\n",
    "def linear_preproccess_pipeline(\n",
    "    preproc: LinearPreprocessor,\n",
    "    destino_dir: str,\n",
    "    save_instance: bool = True,\n",
    "    seq_len=None,\n",
    "    masked_value=None,\n",
    "    train: bool = False,\n",
    "    size: Optional[float] = 1.0\n",
    ") -> Tuple[LinearPreprocessor, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Pipeline completa de pr√©-processamento (Parquet only) e treinamento de 3 modelos\n",
    "    usando apenas a classe unificada Preprocessor.\n",
    "\n",
    "    Retorna o preprocessor (para reutilizar escalers/mapeamentos) e um dicion√°rio\n",
    "    com resultados/resumos do treinamento dos modelos lineares/MLP.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"üîÑ Carregando dados brutos ...\")\n",
    "    preproc.load_data(size=size)\n",
    "\n",
    "    print(\"üî§ Encoding de colunas categ√≥ricas/temporais ...\")\n",
    "    preproc.encode(encode_cols='datetime', encode_method='time_cycle', train=train)\n",
    "    preproc.encode(encode_cols='country', encode_method='label', train=train)\n",
    "\n",
    "    print(\"‚úÇÔ∏è Split train/val/test ...\")\n",
    "    preproc.split_train_val_test(train_size=0.8, val_size=0.1, test_size=0.1, time_col='datetime')\n",
    "\n",
    "    print(\"üî≥ Construindo matrizes planas para Linear/MLP ...\")\n",
    "    preproc.build_flat_matrices_splits(\n",
    "        dropna=True,\n",
    "        group_cols=['country'],\n",
    "        time_col='datetime',\n",
    "        seq_len=seq_len,\n",
    "        mask_value=masked_value\n",
    "    )\n",
    "\n",
    "    print(\"üìê Normaliza√ß√£o ...\")\n",
    "    preproc.normalize_splits(normalization_method='standard', train=train)\n",
    "\n",
    "    print(\"üíæ Salvando parquets LINEAR/MLP ...\")\n",
    "    preproc.save_linear_splits_parquet(basename='linear_dataset')\n",
    "\n",
    "    if save_instance and train:\n",
    "       path = os.path.join(destino_dir, \"preprocessor\")\n",
    "       preproc.save_instance(path, name=\"linear_preproc.pkl\")\n",
    "\n",
    "    print(\"üì¶ Dataset Parquet carregado para treinamento.\")\n",
    "\n",
    "    return preproc\n",
    "\n",
    "def linear_train_pipeline(\n",
    "    problem_name: str,\n",
    "    data_dir: str,\n",
    "    batch_size: int = 128,\n",
    "    configs: Dict[str, Dict[str, Any]] = None\n",
    "):\n",
    "    # ----------------------------\n",
    "    # Parquet datasets\n",
    "    # ----------------------------\n",
    "    dataset_train, meta_tr = LinearPreprocessor.load_linear_parquet_dataset(data_dir=data_dir, split='train', batch_size=batch_size, shuffle=True)\n",
    "    dataset_val, meta_va = LinearPreprocessor.load_linear_parquet_dataset(data_dir=data_dir, split='val', batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    x_dim = int(meta_tr[\"x_dim\"])\n",
    "    y_dim = int(meta_tr[\"y_dim\"])\n",
    "    print(\"üì¶ Dataset Parquet carregado para treinamento.\")\n",
    "\n",
    "    histories = {}\n",
    "    models = {}\n",
    "\n",
    "    early_stopping = TFEarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=True, verbose=0)\n",
    "    reduce_lr = TFReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=6, min_lr=1e-6, verbose=0)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Treinamento de cada modelo\n",
    "    # ----------------------------\n",
    "    for name, params in configs.items():\n",
    "        print(f\"\\nüöÄ Treinando modelo {name}...\")\n",
    "\n",
    "        linear_bool = params.get(\"linear\")\n",
    "        if linear_bool:\n",
    "            model = build_linear_model(x_dim=x_dim, y_dim=y_dim, params=params, linear=linear_bool)\n",
    "        else:\n",
    "            model = build_mlp_model(x_dim=x_dim, y_dim=y_dim, params=params)\n",
    "        \n",
    "\n",
    "        hist = model.fit(\n",
    "            dataset_train,\n",
    "            validation_data=dataset_val,\n",
    "            epochs=200,\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=2\n",
    ")\n",
    "\n",
    "        histories[name] = hist\n",
    "        models[name] = model\n",
    "        print(f\"‚úÖ {name} conclu√≠do - Val Loss: {min(hist.history['val_loss']):.6f}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Salvando modelos\n",
    "    # ----------------------------\n",
    "    for name, model in models.items():\n",
    "        # Salvando modelo no path /modelos/{nome do problema}/{nome do modelo}\n",
    "        save_model(model, path = f\"./modelos/{problem_name}/{name}\")\n",
    "    \n",
    "    \n",
    "    return models, histories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6250a234",
   "metadata": {},
   "source": [
    "## Pipelines dos modelos LSTM\n",
    "Implementa√ß√£o e uso dos preprocessors e treinadores LSTM para s√©ries temporais (janelas seq_len e lead)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44a84a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def sanity_check_prefetch(ds: tf.data.Dataset, n: int = 3, n_vals: int = 5):\n",
    "    \"\"\"\n",
    "    Quick sanity check for tf.data.Dataset (prefetched/batched).\n",
    "    Prints shape, dtype, and sample values for a few batches.\n",
    "\n",
    "    Args:\n",
    "        ds : tf.data.Dataset ‚Äî dataset to inspect\n",
    "        n  : int ‚Äî number of batches to preview\n",
    "        n_vals : int ‚Äî number of feature values to display (first & last)\n",
    "    \"\"\"\n",
    "    print(\"üîç Dataset sanity check...\")\n",
    "    for i, (x, y) in enumerate(ds.take(n)):\n",
    "        print(f\"\\n[Batch {i+1}] ===============================\")\n",
    "\n",
    "        # --- X INFO ---\n",
    "        if isinstance(x, dict):\n",
    "            print(\"X: (dict)\")\n",
    "            for k, v in x.items():\n",
    "                arr = v.numpy() if tf.is_tensor(v) else np.array(v)\n",
    "                print(f\" ‚Ä¢ {k:<20} shape={arr.shape} dtype={arr.dtype}\")\n",
    "                flat = arr.flatten()\n",
    "                if flat.size > 0:\n",
    "                    head = np.array2string(flat[:n_vals], precision=4, separator=\", \")\n",
    "                    tail = np.array2string(flat[-n_vals:], precision=4, separator=\", \")\n",
    "                    print(f\"   first: {head}\")\n",
    "                    print(f\"   last : {tail}\")\n",
    "        else:\n",
    "            arr = x.numpy() if tf.is_tensor(x) else np.array(x)\n",
    "            print(f\"X shape={arr.shape} dtype={arr.dtype}\")\n",
    "            flat = arr.flatten()\n",
    "            if flat.size > 0:\n",
    "                head = np.array2string(flat[:n_vals], precision=4, separator=\", \")\n",
    "                tail = np.array2string(flat[-n_vals:], precision=4, separator=\", \")\n",
    "                print(f\"  first: {head}\")\n",
    "                print(f\"  last : {tail}\")\n",
    "\n",
    "        # --- Y INFO ---\n",
    "        arr_y = y.numpy() if tf.is_tensor(y) else np.array(y)\n",
    "        print(f\"Y shape={arr_y.shape} dtype={arr_y.dtype}\")\n",
    "        flat_y = arr_y.flatten()\n",
    "        if flat_y.size > 0:\n",
    "            head_y = np.array2string(flat_y[:n_vals], precision=4, separator=\", \")\n",
    "            tail_y = np.array2string(flat_y[-n_vals:], precision=4, separator=\", \")\n",
    "            print(f\"  y first: {head_y}\")\n",
    "            print(f\"  y last : {tail_y}\")\n",
    "\n",
    "        if i + 1 >= n:\n",
    "            break\n",
    "    print(\"\\n‚úÖ Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39f66f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines dos modelos LSTM (imports centralizados na c√©lula 5)\n",
    "\n",
    "def lstm_preproccess_pipeline(\n",
    "    preproc: LSTMPreprocessor,\n",
    "    destino_dir: str,\n",
    "    save_instance: bool = True,\n",
    "    train: bool = False,\n",
    "    size: Optional[float] = 1.0\n",
    ") -> Tuple[LSTMPreprocessor, Dict[str, Any]]:\n",
    "    \n",
    "\n",
    "    print(\"üîÑ Carregando dados brutos ...\")\n",
    "    preproc.load_data(size=size)\n",
    "    \n",
    "    print(\"üî§ Encoding ...\")\n",
    "    preproc.encode(encode_cols=\"datetime\", encode_method=\"time_cycle\", train=train)\n",
    "    preproc.encode(encode_cols=\"country\", encode_method=\"label\", train=train)\n",
    "\n",
    "    print(\"‚úÇÔ∏è Split train/val/test ...\")\n",
    "    preproc.split_train_val_test(train_size=0.6, val_size=0.2, test_size=0.2, time_col=\"datetime\")\n",
    "    \n",
    "    print(\"üìê Normaliza√ß√£o ...\")\n",
    "    preproc.normalize_splits(normalization_method=\"standard\", train=train)\n",
    "\n",
    "    print(\"üî≥ Construindo matrizes sequenciais para LSTM ...\")\n",
    "    preproc.build_sequence_matrix_splits(\n",
    "        group_cols=['country'],\n",
    "        time_col=\"datetime\"\n",
    "    )\n",
    "\n",
    "    print(\"üíæ Salvando parquets LSTM ...\")\n",
    "    preproc.save_splits_parquet(basename=\"lstm_dataset\")\n",
    "\n",
    "    if save_instance and train:\n",
    "        path = os.path.join(destino_dir, \"preprocessor\")\n",
    "        preproc.save_instance(path, name=\"lstm_preproc.pkl\")\n",
    "\n",
    "    print(\"üì¶ Dataset LSTM Parquet carregado para treinamento.\")\n",
    "    return preproc\n",
    "\n",
    "def lstm_train_pipeline(\n",
    "    problem_name: str,\n",
    "    data_dir: str,\n",
    "    seq_len: int,\n",
    "    batch_size: int = 128,\n",
    "    configs: Dict[str, Dict[str, Any]] = None\n",
    ") -> Dict[str, keras.Model]:\n",
    "    # ----------------------------\n",
    "    # Parquet datasets\n",
    "    # ----------------------------\n",
    "    dataset_train, meta_tr = LSTMPreprocessor.load_lstm_parquet_dataset(data_dir=data_dir, split=\"train\", batch_size=batch_size, shuffle=False)\n",
    "    dataset_val, meta_va = LSTMPreprocessor.load_lstm_parquet_dataset(data_dir=data_dir, split=\"val\", batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    x_dim = int(meta_tr[\"x_dim\"]) ; y_dim = int(meta_tr[\"y_dim\"]) ; seq_len_m = int(meta_tr[\"seq_len\"]) ; lead_m = int(meta_tr[\"lead\"]) \n",
    "    if seq_len and seq_len != seq_len_m:\n",
    "        print(f\"[WARN] seq_len fornecido ({seq_len}) difere do meta ({seq_len_m}). Usando meta.\")\n",
    "    print(\"üì¶ Dataset Parquet carregado para treinamento.\")\n",
    "\n",
    "    histories = {}\n",
    "    models = {}\n",
    "\n",
    "    early_stopping = TFEarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=True, verbose=0)\n",
    "    reduce_lr = TFReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=6, min_lr=1e-6, verbose=0)\n",
    "\n",
    "    for name, params in configs.items():\n",
    "        print(f\"\\nüöÄ Treinando modelo {name}...\")\n",
    "        model = build_lstm_model(seq_len=seq_len_m, x_dim_num=x_dim, y_dim=y_dim, lead=lead_m, num_countries=4, **params)\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "        hist = model.fit(\n",
    "            dataset_train,\n",
    "            validation_data=dataset_val,\n",
    "            epochs=200,\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=2,\n",
    "        )\n",
    "\n",
    "        histories[name] = hist\n",
    "        models[name] = model\n",
    "\n",
    "        print(f\"‚úÖ {name} conclu√≠do - Val Loss: {min(hist.history['val_loss']):.6f}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Salvando modelos\n",
    "    # ----------------------------\n",
    "    for name, model in models.items():\n",
    "        save_model(model, path = f\"./modelos/{problem_name}/{name}\")\n",
    "    \n",
    "    return models, histories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be66287c",
   "metadata": {},
   "source": [
    "## Pipelines dos modelos TFT\n",
    "Pr√©-processamento em parquet e treino com PyTorch Forecasting (Temporal Fusion Transformer) via Lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87e31312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines dos modelos TFT (imports centralizados na c√©lula 5)\n",
    "from lightning.pytorch import Trainer, seed_everything, Callback\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "import torch, os\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "\n",
    "def tft_preproccess_pipeline(\n",
    "    preproc: TFTPreprocessor,\n",
    "    destino_dir: str,\n",
    "    save_instance: bool = True,\n",
    "    train: bool = False,\n",
    "    size: Optional[float] = 1.0\n",
    ") -> Tuple[TFTPreprocessor, Dict[str, Any]]:\n",
    "\n",
    "    print(\"üîÑ Carregando dados brutos ...\")\n",
    "    preproc.load_data(size=size)\n",
    "\n",
    "    print(\"üî§ Encoding ...\")\n",
    "    preproc.encode(encode_cols='datetime', encode_method='time_cycle', train=train)\n",
    "    preproc.encode(encode_cols='country', encode_method='label', train=train)\n",
    "\n",
    "    print(\"‚úÇÔ∏è Split train/val/test ...\")\n",
    "    preproc.split_train_val_test(train_size=0.8, val_size=0.1, test_size=0.1, time_col='datetime')\n",
    "\n",
    "    print(\"üìê Normaliza√ß√£o ...\")\n",
    "    preproc.normalize_splits(normalization_method='standard', train=train)\n",
    "\n",
    "\n",
    "    print(\"üß± Construindo parquets para TFT ...\")\n",
    "    preproc.build_tft_parquets(\n",
    "        group_cols=['country'],\n",
    "        time_col='datetime'\n",
    ")\n",
    "\n",
    "    if save_instance and train:\n",
    "        path = os.path.join(destino_dir, \"preprocessor\")\n",
    "        preproc.save_instance(path, name=\"tft_preproc.pkl\")\n",
    "\n",
    "    print(\"üì¶ Parquets TFT salvos. Carregando dataset ...\")\n",
    "\n",
    "    return preproc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from lightning.pytorch import Trainer, seed_everything, Callback\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import Logger\n",
    "import torch, os\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Minimal dummy logger (satisfies callbacks, no files)\n",
    "# ----------------------------------------------------\n",
    "class DummyLogger(Logger):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._version = 0\n",
    "    @property\n",
    "    def name(self): return \"dummy\"\n",
    "    @property\n",
    "    def version(self): return self._version\n",
    "    def log_metrics(self, metrics, step=None): pass\n",
    "    def log_hyperparams(self, params): pass\n",
    "    def experiment(self): return None\n",
    "\n",
    "\n",
    "def tft_train_pipeline(\n",
    "    problem_name: str,\n",
    "    data_dir: str,\n",
    "    feature_cols: List[str],\n",
    "    target_cols: List[str],\n",
    "    seq_len: int,\n",
    "    lead: int,\n",
    "    batch_size: int = 128,\n",
    "    configs: Dict[str, Dict[str, Any]] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Treina modelos TFT (Temporal Fusion Transformer) com PyTorch Lightning.\n",
    "    Retorna (models, histories) ‚Äî histories[name].history['loss'] / ['val_loss'] como em Keras.\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------\n",
    "    # 1. Carregar dados\n",
    "    # ----------------------------\n",
    "    preproc = TFTPreprocessor(\n",
    "        data_dir=data_dir,\n",
    "        model_name=\"tft_model\",\n",
    "        feature_cols=feature_cols,\n",
    "        target_cols=target_cols,\n",
    "        seq_len=seq_len,\n",
    "        lead=lead,\n",
    "        country_list=[],\n",
    "    )\n",
    "\n",
    "    df_train = preproc.load_tft_dataset(\"train\", target_col=target_cols[0])\n",
    "    df_val   = preproc.load_tft_dataset(\"val\",   target_col=target_cols[0])\n",
    "\n",
    "    train_loader = df_train.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "    val_loader   = df_val.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\n",
    "    print(f\"üì¶ Dados TFT ‚Äî batches: train={len(train_loader)} | val={len(val_loader)}\")\n",
    "\n",
    "    models, histories = {}, {}\n",
    "    seed_everything(42)\n",
    "    accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # ----------------------------\n",
    "    # 2. Custom loss collector\n",
    "    # ----------------------------\n",
    "    class LossHistoryCallback(Callback):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.history = {\"loss\": [], \"val_loss\": []}\n",
    "\n",
    "        def on_train_epoch_end(self, trainer, pl_module):\n",
    "            metrics = trainer.callback_metrics\n",
    "            if \"train_loss\" in metrics:\n",
    "                self.history[\"loss\"].append(float(metrics[\"train_loss\"]))\n",
    "            elif \"loss\" in metrics:\n",
    "                self.history[\"loss\"].append(float(metrics[\"loss\"]))\n",
    "\n",
    "        def on_validation_epoch_end(self, trainer, pl_module):\n",
    "            metrics = trainer.callback_metrics\n",
    "            if \"val_loss\" in metrics:\n",
    "                self.history[\"val_loss\"].append(float(metrics[\"val_loss\"]))\n",
    "\n",
    "    # ----------------------------\n",
    "    # 3. Loop de presets\n",
    "    # ----------------------------\n",
    "    for name, params in (configs or {}).items():\n",
    "        print(f\"\\nüöÄ Treinando TFT preset: {name} [{accelerator}]\")\n",
    "\n",
    "        model = build_tft_model(params={**params, \"dataset\": df_train})\n",
    "\n",
    "        save_dir = os.path.join(\"modelos\", problem_name, \"TFT\", name)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        loss_collector = LossHistoryCallback()\n",
    "\n",
    "        callbacks = [\n",
    "            loss_collector,\n",
    "            EarlyStopping(monitor=\"val_loss\", patience=int(params.get(\"patience\", 5)), mode=\"min\"),\n",
    "            LearningRateMonitor(logging_interval=\"epoch\"),\n",
    "            ModelCheckpoint(\n",
    "                dirpath=save_dir,\n",
    "                filename=\"best\",\n",
    "                monitor=\"val_loss\",\n",
    "                save_top_k=1,\n",
    "                mode=\"min\",\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        # ‚úÖ Use DummyLogger to satisfy LRMonitor\n",
    "        trainer = Trainer(\n",
    "            max_epochs=int(params.get(\"epochs\", 50)),\n",
    "            accelerator=accelerator,\n",
    "            devices=1,\n",
    "            callbacks=callbacks,\n",
    "            default_root_dir=save_dir,\n",
    "            log_every_n_steps=10,\n",
    "            logger=DummyLogger(),   # <‚Äî in-memory, silent\n",
    "            precision=32,\n",
    "        )\n",
    "\n",
    "        # ----------------------------\n",
    "        # 4. Treinamento\n",
    "        # ----------------------------\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        print(f\"‚úÖ {name} conclu√≠do ‚Äî melhor checkpoint salvo em {save_dir}\")\n",
    "\n",
    "        # ----------------------------\n",
    "        # 5. Hist√≥rico tipo Keras\n",
    "        # ----------------------------\n",
    "        class HistoryLike:\n",
    "            def __init__(self, hist): self.history = hist\n",
    "\n",
    "        history = HistoryLike(loss_collector.history)\n",
    "\n",
    "        print(f\"\\nüìä Hist√≥rico ({name}) ‚Äî √∫ltimas perdas:\")\n",
    "        print(\"train_loss:\", history.history[\"loss\"][-5:])\n",
    "        print(\"val_loss:\", history.history[\"val_loss\"][-5:])\n",
    "\n",
    "        models[name] = model\n",
    "        histories[name] = history\n",
    "\n",
    "    return models, histories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af6da91",
   "metadata": {},
   "source": [
    "# Cap√≠tulo 4: Preprocessamento de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bec8a927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== PIPELINE LINEAR/MLP ====\n",
      "üîÑ Carregando dados brutos ...\n",
      "üî§ Encoding de colunas categ√≥ricas/temporais ...\n",
      "‚úÇÔ∏è Split train/val/test ...\n",
      "[DIVIDIDO] train: 83,831 linhas\n",
      "[DIVIDIDO] val: 10,478 linhas\n",
      "[DIVIDIDO] test: 10,480 linhas\n",
      "üî≥ Construindo matrizes planas para Linear/MLP ...\n",
      "[INFO] Ignorando features n√£o num√©ricas/bool em build_flat_matrix: ['quantity_MW_lag1', 'quantity_MW_lag2', 'quantity_MW_lag3', 'quantity_MW_lag4', 'quantity_MW_lag5', 'quantity_MW_lag6', 'quantity_MW_lag7', 'quantity_MW_lag8', 'quantity_MW_lag9', 'quantity_MW_lag10', 'quantity_MW_lag11', 'quantity_MW_lag12', 'quantity_MW_lag13', 'quantity_MW_lag14', 'quantity_MW_lag15', 'quantity_MW_lag16', 'quantity_MW_lag17', 'quantity_MW_lag18', 'quantity_MW_lag19', 'quantity_MW_lag20', 'quantity_MW_lag21', 'quantity_MW_lag22', 'quantity_MW_lag23', 'quantity_MW_lag24', 'quantity_MW_lag25', 'quantity_MW_lag26', 'quantity_MW_lag27', 'quantity_MW_lag28', 'quantity_MW_lag29', 'quantity_MW_lag30', 'quantity_MW_lag31', 'quantity_MW_lag32', 'quantity_MW_lag33', 'quantity_MW_lag34', 'quantity_MW_lag35', 'quantity_MW_lag36', 'quantity_MW_lag37', 'quantity_MW_lag38', 'quantity_MW_lag39', 'quantity_MW_lag40', 'quantity_MW_lag41', 'quantity_MW_lag42', 'quantity_MW_lag43', 'quantity_MW_lag44', 'quantity_MW_lag45', 'quantity_MW_lag46', 'quantity_MW_lag47', 'quantity_MW_lag48', 'quantity_MW_lag49', 'quantity_MW_lag50', 'quantity_MW_lag51', 'quantity_MW_lag52', 'quantity_MW_lag53', 'quantity_MW_lag54', 'quantity_MW_lag55', 'quantity_MW_lag56', 'quantity_MW_lag57', 'quantity_MW_lag58', 'quantity_MW_lag59', 'quantity_MW_lag60', 'quantity_MW_lag61', 'quantity_MW_lag62', 'quantity_MW_lag63', 'quantity_MW_lag64', 'quantity_MW_lag65', 'quantity_MW_lag66', 'quantity_MW_lag67', 'quantity_MW_lag68', 'quantity_MW_lag69', 'quantity_MW_lag70', 'quantity_MW_lag71', 'quantity_MW_lag72', 'quantity_MW_lag73', 'quantity_MW_lag74', 'quantity_MW_lag75', 'quantity_MW_lag76', 'quantity_MW_lag77', 'quantity_MW_lag78', 'quantity_MW_lag79', 'quantity_MW_lag80', 'quantity_MW_lag81', 'quantity_MW_lag82', 'quantity_MW_lag83', 'quantity_MW_lag84', 'quantity_MW_lag85', 'quantity_MW_lag86', 'quantity_MW_lag87', 'quantity_MW_lag88', 'quantity_MW_lag89', 'quantity_MW_lag90', 'quantity_MW_lag91', 'quantity_MW_lag92', 'quantity_MW_lag93', 'quantity_MW_lag94', 'quantity_MW_lag95', 'quantity_MW_lag96', 'quantity_MW_lag97', 'quantity_MW_lag98', 'quantity_MW_lag99', 'quantity_MW_lag100', 'quantity_MW_lag101', 'quantity_MW_lag102', 'quantity_MW_lag103', 'quantity_MW_lag104', 'quantity_MW_lag105', 'quantity_MW_lag106', 'quantity_MW_lag107', 'quantity_MW_lag108', 'quantity_MW_lag109', 'quantity_MW_lag110', 'quantity_MW_lag111', 'quantity_MW_lag112', 'quantity_MW_lag113', 'quantity_MW_lag114', 'quantity_MW_lag115', 'quantity_MW_lag116', 'quantity_MW_lag117', 'quantity_MW_lag118', 'quantity_MW_lag119', 'quantity_MW_lag120', 'quantity_MW_lag121', 'quantity_MW_lag122', 'quantity_MW_lag123', 'quantity_MW_lag124', 'quantity_MW_lag125', 'quantity_MW_lag126', 'quantity_MW_lag127', 'quantity_MW_lag128', 'quantity_MW_lag129', 'quantity_MW_lag130', 'quantity_MW_lag131', 'quantity_MW_lag132', 'quantity_MW_lag133', 'quantity_MW_lag134', 'quantity_MW_lag135', 'quantity_MW_lag136', 'quantity_MW_lag137', 'quantity_MW_lag138', 'quantity_MW_lag139', 'quantity_MW_lag140', 'quantity_MW_lag141', 'quantity_MW_lag142', 'quantity_MW_lag143', 'quantity_MW_lag144', 'quantity_MW_lag145', 'quantity_MW_lag146', 'quantity_MW_lag147', 'quantity_MW_lag148', 'quantity_MW_lag149', 'quantity_MW_lag150', 'quantity_MW_lag151', 'quantity_MW_lag152', 'quantity_MW_lag153', 'quantity_MW_lag154', 'quantity_MW_lag155', 'quantity_MW_lag156', 'quantity_MW_lag157', 'quantity_MW_lag158', 'quantity_MW_lag159', 'quantity_MW_lag160', 'quantity_MW_lag161', 'quantity_MW_lag162', 'quantity_MW_lag163', 'quantity_MW_lag164', 'quantity_MW_lag165', 'quantity_MW_lag166', 'quantity_MW_lag167', 'quantity_MW_lag168', 'quantity_MW_lag169', 'quantity_MW_lag170', 'quantity_MW_lag171', 'quantity_MW_lag172', 'quantity_MW_lag173', 'quantity_MW_lag174', 'quantity_MW_lag175', 'quantity_MW_lag176', 'quantity_MW_lag177', 'quantity_MW_lag178', 'quantity_MW_lag179', 'quantity_MW_lag180', 'quantity_MW_lag181', 'quantity_MW_lag182', 'quantity_MW_lag183', 'quantity_MW_lag184', 'quantity_MW_lag185', 'quantity_MW_lag186', 'quantity_MW_lag187', 'quantity_MW_lag188', 'quantity_MW_lag189', 'quantity_MW_lag190', 'quantity_MW_lag191', 'quantity_MW_lag192', 'quantity_MW_lag193', 'quantity_MW_lag194', 'quantity_MW_lag195', 'quantity_MW_lag196', 'quantity_MW_lag197', 'quantity_MW_lag198', 'quantity_MW_lag199', 'quantity_MW_lag200', 'quantity_MW_lag201', 'quantity_MW_lag202', 'quantity_MW_lag203', 'quantity_MW_lag204', 'quantity_MW_lag205', 'quantity_MW_lag206', 'quantity_MW_lag207', 'quantity_MW_lag208', 'quantity_MW_lag209', 'quantity_MW_lag210', 'quantity_MW_lag211', 'quantity_MW_lag212', 'quantity_MW_lag213', 'quantity_MW_lag214', 'quantity_MW_lag215', 'quantity_MW_lag216', 'quantity_MW_lag217', 'quantity_MW_lag218', 'quantity_MW_lag219', 'quantity_MW_lag220', 'quantity_MW_lag221', 'quantity_MW_lag222', 'quantity_MW_lag223', 'quantity_MW_lag224', 'quantity_MW_lag225', 'quantity_MW_lag226', 'quantity_MW_lag227', 'quantity_MW_lag228', 'quantity_MW_lag229', 'quantity_MW_lag230', 'quantity_MW_lag231', 'quantity_MW_lag232', 'quantity_MW_lag233', 'quantity_MW_lag234', 'quantity_MW_lag235', 'quantity_MW_lag236', 'quantity_MW_lag237', 'quantity_MW_lag238', 'quantity_MW_lag239', 'quantity_MW_lag240']\n",
      "[INFO] Ignorando targets n√£o num√©ricos/bool em build_flat_matrix: ['quantity_MW_lead1', 'quantity_MW_lead2', 'quantity_MW_lead3', 'quantity_MW_lead4', 'quantity_MW_lead5', 'quantity_MW_lead6', 'quantity_MW_lead7', 'quantity_MW_lead8', 'quantity_MW_lead9', 'quantity_MW_lead10', 'quantity_MW_lead11', 'quantity_MW_lead12', 'quantity_MW_lead13', 'quantity_MW_lead14', 'quantity_MW_lead15', 'quantity_MW_lead16', 'quantity_MW_lead17', 'quantity_MW_lead18', 'quantity_MW_lead19', 'quantity_MW_lead20', 'quantity_MW_lead21', 'quantity_MW_lead22', 'quantity_MW_lead23', 'quantity_MW_lead24', 'quantity_MW_lead25', 'quantity_MW_lead26', 'quantity_MW_lead27', 'quantity_MW_lead28', 'quantity_MW_lead29', 'quantity_MW_lead30', 'quantity_MW_lead31', 'quantity_MW_lead32', 'quantity_MW_lead33', 'quantity_MW_lead34', 'quantity_MW_lead35', 'quantity_MW_lead36', 'quantity_MW_lead37', 'quantity_MW_lead38', 'quantity_MW_lead39', 'quantity_MW_lead40', 'quantity_MW_lead41', 'quantity_MW_lead42', 'quantity_MW_lead43', 'quantity_MW_lead44', 'quantity_MW_lead45', 'quantity_MW_lead46', 'quantity_MW_lead47', 'quantity_MW_lead48', 'quantity_MW_lead49', 'quantity_MW_lead50', 'quantity_MW_lead51', 'quantity_MW_lead52', 'quantity_MW_lead53', 'quantity_MW_lead54', 'quantity_MW_lead55', 'quantity_MW_lead56', 'quantity_MW_lead57', 'quantity_MW_lead58', 'quantity_MW_lead59', 'quantity_MW_lead60', 'quantity_MW_lead61', 'quantity_MW_lead62', 'quantity_MW_lead63', 'quantity_MW_lead64', 'quantity_MW_lead65', 'quantity_MW_lead66', 'quantity_MW_lead67', 'quantity_MW_lead68', 'quantity_MW_lead69', 'quantity_MW_lead70', 'quantity_MW_lead71', 'quantity_MW_lead72']\n",
      "[INFO] Ignorando features n√£o num√©ricas/bool em build_flat_matrix: ['quantity_MW_lag1', 'quantity_MW_lag2', 'quantity_MW_lag3', 'quantity_MW_lag4', 'quantity_MW_lag5', 'quantity_MW_lag6', 'quantity_MW_lag7', 'quantity_MW_lag8', 'quantity_MW_lag9', 'quantity_MW_lag10', 'quantity_MW_lag11', 'quantity_MW_lag12', 'quantity_MW_lag13', 'quantity_MW_lag14', 'quantity_MW_lag15', 'quantity_MW_lag16', 'quantity_MW_lag17', 'quantity_MW_lag18', 'quantity_MW_lag19', 'quantity_MW_lag20', 'quantity_MW_lag21', 'quantity_MW_lag22', 'quantity_MW_lag23', 'quantity_MW_lag24', 'quantity_MW_lag25', 'quantity_MW_lag26', 'quantity_MW_lag27', 'quantity_MW_lag28', 'quantity_MW_lag29', 'quantity_MW_lag30', 'quantity_MW_lag31', 'quantity_MW_lag32', 'quantity_MW_lag33', 'quantity_MW_lag34', 'quantity_MW_lag35', 'quantity_MW_lag36', 'quantity_MW_lag37', 'quantity_MW_lag38', 'quantity_MW_lag39', 'quantity_MW_lag40', 'quantity_MW_lag41', 'quantity_MW_lag42', 'quantity_MW_lag43', 'quantity_MW_lag44', 'quantity_MW_lag45', 'quantity_MW_lag46', 'quantity_MW_lag47', 'quantity_MW_lag48', 'quantity_MW_lag49', 'quantity_MW_lag50', 'quantity_MW_lag51', 'quantity_MW_lag52', 'quantity_MW_lag53', 'quantity_MW_lag54', 'quantity_MW_lag55', 'quantity_MW_lag56', 'quantity_MW_lag57', 'quantity_MW_lag58', 'quantity_MW_lag59', 'quantity_MW_lag60', 'quantity_MW_lag61', 'quantity_MW_lag62', 'quantity_MW_lag63', 'quantity_MW_lag64', 'quantity_MW_lag65', 'quantity_MW_lag66', 'quantity_MW_lag67', 'quantity_MW_lag68', 'quantity_MW_lag69', 'quantity_MW_lag70', 'quantity_MW_lag71', 'quantity_MW_lag72', 'quantity_MW_lag73', 'quantity_MW_lag74', 'quantity_MW_lag75', 'quantity_MW_lag76', 'quantity_MW_lag77', 'quantity_MW_lag78', 'quantity_MW_lag79', 'quantity_MW_lag80', 'quantity_MW_lag81', 'quantity_MW_lag82', 'quantity_MW_lag83', 'quantity_MW_lag84', 'quantity_MW_lag85', 'quantity_MW_lag86', 'quantity_MW_lag87', 'quantity_MW_lag88', 'quantity_MW_lag89', 'quantity_MW_lag90', 'quantity_MW_lag91', 'quantity_MW_lag92', 'quantity_MW_lag93', 'quantity_MW_lag94', 'quantity_MW_lag95', 'quantity_MW_lag96', 'quantity_MW_lag97', 'quantity_MW_lag98', 'quantity_MW_lag99', 'quantity_MW_lag100', 'quantity_MW_lag101', 'quantity_MW_lag102', 'quantity_MW_lag103', 'quantity_MW_lag104', 'quantity_MW_lag105', 'quantity_MW_lag106', 'quantity_MW_lag107', 'quantity_MW_lag108', 'quantity_MW_lag109', 'quantity_MW_lag110', 'quantity_MW_lag111', 'quantity_MW_lag112', 'quantity_MW_lag113', 'quantity_MW_lag114', 'quantity_MW_lag115', 'quantity_MW_lag116', 'quantity_MW_lag117', 'quantity_MW_lag118', 'quantity_MW_lag119', 'quantity_MW_lag120', 'quantity_MW_lag121', 'quantity_MW_lag122', 'quantity_MW_lag123', 'quantity_MW_lag124', 'quantity_MW_lag125', 'quantity_MW_lag126', 'quantity_MW_lag127', 'quantity_MW_lag128', 'quantity_MW_lag129', 'quantity_MW_lag130', 'quantity_MW_lag131', 'quantity_MW_lag132', 'quantity_MW_lag133', 'quantity_MW_lag134', 'quantity_MW_lag135', 'quantity_MW_lag136', 'quantity_MW_lag137', 'quantity_MW_lag138', 'quantity_MW_lag139', 'quantity_MW_lag140', 'quantity_MW_lag141', 'quantity_MW_lag142', 'quantity_MW_lag143', 'quantity_MW_lag144', 'quantity_MW_lag145', 'quantity_MW_lag146', 'quantity_MW_lag147', 'quantity_MW_lag148', 'quantity_MW_lag149', 'quantity_MW_lag150', 'quantity_MW_lag151', 'quantity_MW_lag152', 'quantity_MW_lag153', 'quantity_MW_lag154', 'quantity_MW_lag155', 'quantity_MW_lag156', 'quantity_MW_lag157', 'quantity_MW_lag158', 'quantity_MW_lag159', 'quantity_MW_lag160', 'quantity_MW_lag161', 'quantity_MW_lag162', 'quantity_MW_lag163', 'quantity_MW_lag164', 'quantity_MW_lag165', 'quantity_MW_lag166', 'quantity_MW_lag167', 'quantity_MW_lag168', 'quantity_MW_lag169', 'quantity_MW_lag170', 'quantity_MW_lag171', 'quantity_MW_lag172', 'quantity_MW_lag173', 'quantity_MW_lag174', 'quantity_MW_lag175', 'quantity_MW_lag176', 'quantity_MW_lag177', 'quantity_MW_lag178', 'quantity_MW_lag179', 'quantity_MW_lag180', 'quantity_MW_lag181', 'quantity_MW_lag182', 'quantity_MW_lag183', 'quantity_MW_lag184', 'quantity_MW_lag185', 'quantity_MW_lag186', 'quantity_MW_lag187', 'quantity_MW_lag188', 'quantity_MW_lag189', 'quantity_MW_lag190', 'quantity_MW_lag191', 'quantity_MW_lag192', 'quantity_MW_lag193', 'quantity_MW_lag194', 'quantity_MW_lag195', 'quantity_MW_lag196', 'quantity_MW_lag197', 'quantity_MW_lag198', 'quantity_MW_lag199', 'quantity_MW_lag200', 'quantity_MW_lag201', 'quantity_MW_lag202', 'quantity_MW_lag203', 'quantity_MW_lag204', 'quantity_MW_lag205', 'quantity_MW_lag206', 'quantity_MW_lag207', 'quantity_MW_lag208', 'quantity_MW_lag209', 'quantity_MW_lag210', 'quantity_MW_lag211', 'quantity_MW_lag212', 'quantity_MW_lag213', 'quantity_MW_lag214', 'quantity_MW_lag215', 'quantity_MW_lag216', 'quantity_MW_lag217', 'quantity_MW_lag218', 'quantity_MW_lag219', 'quantity_MW_lag220', 'quantity_MW_lag221', 'quantity_MW_lag222', 'quantity_MW_lag223', 'quantity_MW_lag224', 'quantity_MW_lag225', 'quantity_MW_lag226', 'quantity_MW_lag227', 'quantity_MW_lag228', 'quantity_MW_lag229', 'quantity_MW_lag230', 'quantity_MW_lag231', 'quantity_MW_lag232', 'quantity_MW_lag233', 'quantity_MW_lag234', 'quantity_MW_lag235', 'quantity_MW_lag236', 'quantity_MW_lag237', 'quantity_MW_lag238', 'quantity_MW_lag239', 'quantity_MW_lag240']\n",
      "[INFO] Ignorando targets n√£o num√©ricos/bool em build_flat_matrix: ['quantity_MW_lead1', 'quantity_MW_lead2', 'quantity_MW_lead3', 'quantity_MW_lead4', 'quantity_MW_lead5', 'quantity_MW_lead6', 'quantity_MW_lead7', 'quantity_MW_lead8', 'quantity_MW_lead9', 'quantity_MW_lead10', 'quantity_MW_lead11', 'quantity_MW_lead12', 'quantity_MW_lead13', 'quantity_MW_lead14', 'quantity_MW_lead15', 'quantity_MW_lead16', 'quantity_MW_lead17', 'quantity_MW_lead18', 'quantity_MW_lead19', 'quantity_MW_lead20', 'quantity_MW_lead21', 'quantity_MW_lead22', 'quantity_MW_lead23', 'quantity_MW_lead24', 'quantity_MW_lead25', 'quantity_MW_lead26', 'quantity_MW_lead27', 'quantity_MW_lead28', 'quantity_MW_lead29', 'quantity_MW_lead30', 'quantity_MW_lead31', 'quantity_MW_lead32', 'quantity_MW_lead33', 'quantity_MW_lead34', 'quantity_MW_lead35', 'quantity_MW_lead36', 'quantity_MW_lead37', 'quantity_MW_lead38', 'quantity_MW_lead39', 'quantity_MW_lead40', 'quantity_MW_lead41', 'quantity_MW_lead42', 'quantity_MW_lead43', 'quantity_MW_lead44', 'quantity_MW_lead45', 'quantity_MW_lead46', 'quantity_MW_lead47', 'quantity_MW_lead48', 'quantity_MW_lead49', 'quantity_MW_lead50', 'quantity_MW_lead51', 'quantity_MW_lead52', 'quantity_MW_lead53', 'quantity_MW_lead54', 'quantity_MW_lead55', 'quantity_MW_lead56', 'quantity_MW_lead57', 'quantity_MW_lead58', 'quantity_MW_lead59', 'quantity_MW_lead60', 'quantity_MW_lead61', 'quantity_MW_lead62', 'quantity_MW_lead63', 'quantity_MW_lead64', 'quantity_MW_lead65', 'quantity_MW_lead66', 'quantity_MW_lead67', 'quantity_MW_lead68', 'quantity_MW_lead69', 'quantity_MW_lead70', 'quantity_MW_lead71', 'quantity_MW_lead72']\n",
      "üìê Normaliza√ß√£o ...\n",
      "üíæ Salvando parquets LINEAR/MLP ...\n",
      "[Linear:train] linhas=82,895  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[üíæ] Parquet salvo: data/treinamento/linear_dataset_train.parquet\n",
      "[Linear:val] linhas=9,542  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[üíæ] Parquet salvo: data/treinamento/linear_dataset_val.parquet\n",
      "[Linear:test] linhas=9,544  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[üíæ] Parquet salvo: data/treinamento/linear_dataset_test.parquet\n",
      "‚úÖ Instance saved at data/treinamento/preprocessor/linear_preproc.pkl\n",
      "üì¶ Dataset Parquet carregado para treinamento.\n",
      "==== PIPELINE LSTM ====\n",
      "üîÑ Carregando dados brutos ...\n",
      "üî§ Encoding ...\n",
      "‚úÇÔ∏è Split train/val/test ...\n",
      "[DIVIDIDO] train: 62,873 linhas\n",
      "[DIVIDIDO] val: 20,957 linhas\n",
      "[DIVIDIDO] test: 20,959 linhas\n",
      "üìê Normaliza√ß√£o ...\n",
      "üî≥ Construindo matrizes sequenciais para LSTM ...\n",
      "[INFO] Ignorando features n√£o num√©ricas/bool em build_sequence_matrix: ['datetime']\n",
      "[JANELAS] X=(61940, 240, 12), Y=(61940, 72, 1), seq_len=240, lead=72\n",
      "[JANELAS] X=(20024, 240, 12), Y=(20024, 72, 1), seq_len=240, lead=72\n",
      "[JANELAS] X=(20026, 240, 12), Y=(20026, 72, 1), seq_len=240, lead=72\n",
      "üíæ Salvando parquets LSTM ...\n",
      "[üíæ] Parquet salvo: data/treinamento/lstm_dataset_train.parquet (61,940 linhas)\n",
      "[üíæ] Parquet salvo: data/treinamento/lstm_dataset_val.parquet (20,024 linhas)\n",
      "[üíæ] Parquet salvo: data/treinamento/lstm_dataset_test.parquet (20,026 linhas)\n",
      "‚úÖ Instance saved at data/treinamento/preprocessor/lstm_preproc.pkl\n",
      "üì¶ Dataset LSTM Parquet carregado para treinamento.\n",
      "==== PIPELINE TFT ====\n",
      "üîÑ Carregando dados brutos ...\n",
      "üî§ Encoding ...\n",
      "‚úÇÔ∏è Split train/val/test ...\n",
      "[DIVIDIDO] train: 83,831 linhas\n",
      "[DIVIDIDO] val: 10,478 linhas\n",
      "[DIVIDIDO] test: 10,480 linhas\n",
      "üìê Normaliza√ß√£o ...\n",
      "üß± Construindo parquets para TFT ...\n",
      "üíæ Split 'train' salvo em data/treinamento/tft_dataset_train.parquet (83831 linhas, grupos=3, max local time_idx=28018).\n",
      "üíæ Split 'val' salvo em data/treinamento/tft_dataset_val.parquet (10478 linhas, grupos=3, max local time_idx=3506).\n",
      "üíæ Split 'test' salvo em data/treinamento/tft_dataset_test.parquet (10480 linhas, grupos=3, max local time_idx=3513).\n",
      "‚úÖ Instance saved at data/treinamento/preprocessor/tft_preproc.pkl\n",
      "üì¶ Parquets TFT salvos. Carregando dataset ...\n",
      "üöÄ Executando 7 tarefas em 7 workers...\n",
      "==== PIPELINE LINEAR/MLP ====\n",
      "üîÑ Carregando dados brutos ...\n",
      "==== PIPELINE LINEAR/MLP ====\n",
      "üîÑ Carregando dados brutos ...\n",
      "==== PIPELINE LINEAR/MLP ====\n",
      "üîÑ Carregando dados brutos ...\n",
      "==== PIPELINE LINEAR/MLP ====\n",
      "üîÑ Carregando dados brutos ...\n",
      "==== PIPELINE LINEAR/MLP ====\n",
      "üîÑ Carregando dados brutos ...\n",
      "==== PIPELINE LINEAR/MLP ====\n",
      "üîÑ Carregando dados brutos ...\n",
      "==== PIPELINE LINEAR/MLP ====\n",
      "üîÑ Carregando dados brutos ...\n",
      "üî§ Encoding de colunas categ√≥ricas/temporais ...\n",
      "üî§ Encoding de colunas categ√≥ricas/temporais ...\n",
      "üî§ Encoding de colunas categ√≥ricas/temporais ...\n",
      "üî§ Encoding de colunas categ√≥ricas/temporais ...\n",
      "üî§ Encoding de colunas categ√≥ricas/temporais ...\n",
      "üî§ Encoding de colunas categ√≥ricas/temporais ...\n",
      "[INFO] Carregado 20,958 linhas (20.0%) de data/raw/raw_dataset.parquet [balanceado por 'country']\n",
      "üî§ Encoding de colunas categ√≥ricas/temporais ...\n",
      "‚úÇÔ∏è Split train/val/test ...\n",
      "‚úÇÔ∏è Split train/val/test ...\n",
      "‚úÇÔ∏è Split train/val/test ...\n",
      "[DIVIDIDO] train: 27,931 linhas\n",
      "[DIVIDIDO] val: 3,491 linhas\n",
      "[DIVIDIDO] test: 3,492 linhas\n",
      "üî≥ Construindo matrizes planas para Linear/MLP ...\n",
      "[DIVIDIDO] train: 27,931 linhas\n",
      "[DIVIDIDO] val: 3,491 linhas\n",
      "[DIVIDIDO] test: 3,492 linhas\n",
      "üî≥ Construindo matrizes planas para Linear/MLP ...\n",
      "[DIVIDIDO] train: 27,931 linhas\n",
      "[DIVIDIDO] val: 3,491 linhas\n",
      "[DIVIDIDO] test: 3,492 linhas\n",
      "üî≥ Construindo matrizes planas para Linear/MLP ...\n",
      "‚úÇÔ∏è Split train/val/test ...\n",
      "‚úÇÔ∏è Split train/val/test ...\n",
      "[‚ÑπÔ∏è] Reduzindo lags: usando 168 e mascarando 72 restantes com -999.0\n",
      "[‚ÑπÔ∏è] Reduzindo lags: usando 72 e mascarando 168 restantes com -999.0\n",
      "‚úÇÔ∏è Split train/val/test ...\n",
      "‚úÇÔ∏è Split train/val/test ...\n",
      "[DIVIDIDO] train: 16,766 linhas\n",
      "[DIVIDIDO] val: 2,095 linhas\n",
      "[DIVIDIDO] test: 2,097 linhas\n",
      "üî≥ Construindo matrizes planas para Linear/MLP ...\n",
      "[DIVIDIDO] train: 83,831 linhas\n",
      "[DIVIDIDO] val: 10,478 linhas\n",
      "[DIVIDIDO] test: 10,480 linhas\n",
      "üî≥ Construindo matrizes planas para Linear/MLP ...\n",
      "[DIVIDIDO] train: 83,831 linhas\n",
      "[DIVIDIDO] val: 10,478 linhas\n",
      "[DIVIDIDO] test: 10,480 linhas\n",
      "üî≥ Construindo matrizes planas para Linear/MLP ...\n",
      "[DIVIDIDO] train: 83,831 linhas\n",
      "[DIVIDIDO] val: 10,478 linhas\n",
      "[DIVIDIDO] test: 10,480 linhas\n",
      "üî≥ Construindo matrizes planas para Linear/MLP ...\n",
      "[‚ÑπÔ∏è] Reduzindo lags: usando 72 e mascarando 168 restantes com -999.0\n",
      "[‚ÑπÔ∏è] Reduzindo lags: usando 168 e mascarando 72 restantes com -999.0\n",
      "[INFO] Ignorando features n√£o num√©ricas/bool em build_flat_matrix: ['quantity_MW_lag1', 'quantity_MW_lag2', 'quantity_MW_lag3', 'quantity_MW_lag4', 'quantity_MW_lag5', 'quantity_MW_lag6', 'quantity_MW_lag7', 'quantity_MW_lag8', 'quantity_MW_lag9', 'quantity_MW_lag10', 'quantity_MW_lag11', 'quantity_MW_lag12', 'quantity_MW_lag13', 'quantity_MW_lag14', 'quantity_MW_lag15', 'quantity_MW_lag16', 'quantity_MW_lag17', 'quantity_MW_lag18', 'quantity_MW_lag19', 'quantity_MW_lag20', 'quantity_MW_lag21', 'quantity_MW_lag22', 'quantity_MW_lag23', 'quantity_MW_lag24', 'quantity_MW_lag25', 'quantity_MW_lag26', 'quantity_MW_lag27', 'quantity_MW_lag28', 'quantity_MW_lag29', 'quantity_MW_lag30', 'quantity_MW_lag31', 'quantity_MW_lag32', 'quantity_MW_lag33', 'quantity_MW_lag34', 'quantity_MW_lag35', 'quantity_MW_lag36', 'quantity_MW_lag37', 'quantity_MW_lag38', 'quantity_MW_lag39', 'quantity_MW_lag40', 'quantity_MW_lag41', 'quantity_MW_lag42', 'quantity_MW_lag43', 'quantity_MW_lag44', 'quantity_MW_lag45', 'quantity_MW_lag46', 'quantity_MW_lag47', 'quantity_MW_lag48', 'quantity_MW_lag49', 'quantity_MW_lag50', 'quantity_MW_lag51', 'quantity_MW_lag52', 'quantity_MW_lag53', 'quantity_MW_lag54', 'quantity_MW_lag55', 'quantity_MW_lag56', 'quantity_MW_lag57', 'quantity_MW_lag58', 'quantity_MW_lag59', 'quantity_MW_lag60', 'quantity_MW_lag61', 'quantity_MW_lag62', 'quantity_MW_lag63', 'quantity_MW_lag64', 'quantity_MW_lag65', 'quantity_MW_lag66', 'quantity_MW_lag67', 'quantity_MW_lag68', 'quantity_MW_lag69', 'quantity_MW_lag70', 'quantity_MW_lag71', 'quantity_MW_lag72', 'quantity_MW_lag73', 'quantity_MW_lag74', 'quantity_MW_lag75', 'quantity_MW_lag76', 'quantity_MW_lag77', 'quantity_MW_lag78', 'quantity_MW_lag79', 'quantity_MW_lag80', 'quantity_MW_lag81', 'quantity_MW_lag82', 'quantity_MW_lag83', 'quantity_MW_lag84', 'quantity_MW_lag85', 'quantity_MW_lag86', 'quantity_MW_lag87', 'quantity_MW_lag88', 'quantity_MW_lag89', 'quantity_MW_lag90', 'quantity_MW_lag91', 'quantity_MW_lag92', 'quantity_MW_lag93', 'quantity_MW_lag94', 'quantity_MW_lag95', 'quantity_MW_lag96', 'quantity_MW_lag97', 'quantity_MW_lag98', 'quantity_MW_lag99', 'quantity_MW_lag100', 'quantity_MW_lag101', 'quantity_MW_lag102', 'quantity_MW_lag103', 'quantity_MW_lag104', 'quantity_MW_lag105', 'quantity_MW_lag106', 'quantity_MW_lag107', 'quantity_MW_lag108', 'quantity_MW_lag109', 'quantity_MW_lag110', 'quantity_MW_lag111', 'quantity_MW_lag112', 'quantity_MW_lag113', 'quantity_MW_lag114', 'quantity_MW_lag115', 'quantity_MW_lag116', 'quantity_MW_lag117', 'quantity_MW_lag118', 'quantity_MW_lag119', 'quantity_MW_lag120', 'quantity_MW_lag121', 'quantity_MW_lag122', 'quantity_MW_lag123', 'quantity_MW_lag124', 'quantity_MW_lag125', 'quantity_MW_lag126', 'quantity_MW_lag127', 'quantity_MW_lag128', 'quantity_MW_lag129', 'quantity_MW_lag130', 'quantity_MW_lag131', 'quantity_MW_lag132', 'quantity_MW_lag133', 'quantity_MW_lag134', 'quantity_MW_lag135', 'quantity_MW_lag136', 'quantity_MW_lag137', 'quantity_MW_lag138', 'quantity_MW_lag139', 'quantity_MW_lag140', 'quantity_MW_lag141', 'quantity_MW_lag142', 'quantity_MW_lag143', 'quantity_MW_lag144', 'quantity_MW_lag145', 'quantity_MW_lag146', 'quantity_MW_lag147', 'quantity_MW_lag148', 'quantity_MW_lag149', 'quantity_MW_lag150', 'quantity_MW_lag151', 'quantity_MW_lag152', 'quantity_MW_lag153', 'quantity_MW_lag154', 'quantity_MW_lag155', 'quantity_MW_lag156', 'quantity_MW_lag157', 'quantity_MW_lag158', 'quantity_MW_lag159', 'quantity_MW_lag160', 'quantity_MW_lag161', 'quantity_MW_lag162', 'quantity_MW_lag163', 'quantity_MW_lag164', 'quantity_MW_lag165', 'quantity_MW_lag166', 'quantity_MW_lag167', 'quantity_MW_lag168', 'quantity_MW_lag169', 'quantity_MW_lag170', 'quantity_MW_lag171', 'quantity_MW_lag172', 'quantity_MW_lag173', 'quantity_MW_lag174', 'quantity_MW_lag175', 'quantity_MW_lag176', 'quantity_MW_lag177', 'quantity_MW_lag178', 'quantity_MW_lag179', 'quantity_MW_lag180', 'quantity_MW_lag181', 'quantity_MW_lag182', 'quantity_MW_lag183', 'quantity_MW_lag184', 'quantity_MW_lag185', 'quantity_MW_lag186', 'quantity_MW_lag187', 'quantity_MW_lag188', 'quantity_MW_lag189', 'quantity_MW_lag190', 'quantity_MW_lag191', 'quantity_MW_lag192', 'quantity_MW_lag193', 'quantity_MW_lag194', 'quantity_MW_lag195', 'quantity_MW_lag196', 'quantity_MW_lag197', 'quantity_MW_lag198', 'quantity_MW_lag199', 'quantity_MW_lag200', 'quantity_MW_lag201', 'quantity_MW_lag202', 'quantity_MW_lag203', 'quantity_MW_lag204', 'quantity_MW_lag205', 'quantity_MW_lag206', 'quantity_MW_lag207', 'quantity_MW_lag208', 'quantity_MW_lag209', 'quantity_MW_lag210', 'quantity_MW_lag211', 'quantity_MW_lag212', 'quantity_MW_lag213', 'quantity_MW_lag214', 'quantity_MW_lag215', 'quantity_MW_lag216', 'quantity_MW_lag217', 'quantity_MW_lag218', 'quantity_MW_lag219', 'quantity_MW_lag220', 'quantity_MW_lag221', 'quantity_MW_lag222', 'quantity_MW_lag223', 'quantity_MW_lag224', 'quantity_MW_lag225', 'quantity_MW_lag226', 'quantity_MW_lag227', 'quantity_MW_lag228', 'quantity_MW_lag229', 'quantity_MW_lag230', 'quantity_MW_lag231', 'quantity_MW_lag232', 'quantity_MW_lag233', 'quantity_MW_lag234', 'quantity_MW_lag235', 'quantity_MW_lag236', 'quantity_MW_lag237', 'quantity_MW_lag238', 'quantity_MW_lag239', 'quantity_MW_lag240']\n",
      "[INFO] Ignorando targets n√£o num√©ricos/bool em build_flat_matrix: ['quantity_MW_lead1', 'quantity_MW_lead2', 'quantity_MW_lead3', 'quantity_MW_lead4', 'quantity_MW_lead5', 'quantity_MW_lead6', 'quantity_MW_lead7', 'quantity_MW_lead8', 'quantity_MW_lead9', 'quantity_MW_lead10', 'quantity_MW_lead11', 'quantity_MW_lead12', 'quantity_MW_lead13', 'quantity_MW_lead14', 'quantity_MW_lead15', 'quantity_MW_lead16', 'quantity_MW_lead17', 'quantity_MW_lead18', 'quantity_MW_lead19', 'quantity_MW_lead20', 'quantity_MW_lead21', 'quantity_MW_lead22', 'quantity_MW_lead23', 'quantity_MW_lead24', 'quantity_MW_lead25', 'quantity_MW_lead26', 'quantity_MW_lead27', 'quantity_MW_lead28', 'quantity_MW_lead29', 'quantity_MW_lead30', 'quantity_MW_lead31', 'quantity_MW_lead32', 'quantity_MW_lead33', 'quantity_MW_lead34', 'quantity_MW_lead35', 'quantity_MW_lead36', 'quantity_MW_lead37', 'quantity_MW_lead38', 'quantity_MW_lead39', 'quantity_MW_lead40', 'quantity_MW_lead41', 'quantity_MW_lead42', 'quantity_MW_lead43', 'quantity_MW_lead44', 'quantity_MW_lead45', 'quantity_MW_lead46', 'quantity_MW_lead47', 'quantity_MW_lead48', 'quantity_MW_lead49', 'quantity_MW_lead50', 'quantity_MW_lead51', 'quantity_MW_lead52', 'quantity_MW_lead53', 'quantity_MW_lead54', 'quantity_MW_lead55', 'quantity_MW_lead56', 'quantity_MW_lead57', 'quantity_MW_lead58', 'quantity_MW_lead59', 'quantity_MW_lead60', 'quantity_MW_lead61', 'quantity_MW_lead62', 'quantity_MW_lead63', 'quantity_MW_lead64', 'quantity_MW_lead65', 'quantity_MW_lead66', 'quantity_MW_lead67', 'quantity_MW_lead68', 'quantity_MW_lead69', 'quantity_MW_lead70', 'quantity_MW_lead71', 'quantity_MW_lead72']\n",
      "[‚ÑπÔ∏è] Reduzindo lags: usando 72 e mascarando 168 restantes com -999.0\n",
      "[INFO] Ignorando features n√£o num√©ricas/bool em build_flat_matrix: ['quantity_MW_lag1', 'quantity_MW_lag2', 'quantity_MW_lag3', 'quantity_MW_lag4', 'quantity_MW_lag5', 'quantity_MW_lag6', 'quantity_MW_lag7', 'quantity_MW_lag8', 'quantity_MW_lag9', 'quantity_MW_lag10', 'quantity_MW_lag11', 'quantity_MW_lag12', 'quantity_MW_lag13', 'quantity_MW_lag14', 'quantity_MW_lag15', 'quantity_MW_lag16', 'quantity_MW_lag17', 'quantity_MW_lag18', 'quantity_MW_lag19', 'quantity_MW_lag20', 'quantity_MW_lag21', 'quantity_MW_lag22', 'quantity_MW_lag23', 'quantity_MW_lag24', 'quantity_MW_lag25', 'quantity_MW_lag26', 'quantity_MW_lag27', 'quantity_MW_lag28', 'quantity_MW_lag29', 'quantity_MW_lag30', 'quantity_MW_lag31', 'quantity_MW_lag32', 'quantity_MW_lag33', 'quantity_MW_lag34', 'quantity_MW_lag35', 'quantity_MW_lag36', 'quantity_MW_lag37', 'quantity_MW_lag38', 'quantity_MW_lag39', 'quantity_MW_lag40', 'quantity_MW_lag41', 'quantity_MW_lag42', 'quantity_MW_lag43', 'quantity_MW_lag44', 'quantity_MW_lag45', 'quantity_MW_lag46', 'quantity_MW_lag47', 'quantity_MW_lag48', 'quantity_MW_lag49', 'quantity_MW_lag50', 'quantity_MW_lag51', 'quantity_MW_lag52', 'quantity_MW_lag53', 'quantity_MW_lag54', 'quantity_MW_lag55', 'quantity_MW_lag56', 'quantity_MW_lag57', 'quantity_MW_lag58', 'quantity_MW_lag59', 'quantity_MW_lag60', 'quantity_MW_lag61', 'quantity_MW_lag62', 'quantity_MW_lag63', 'quantity_MW_lag64', 'quantity_MW_lag65', 'quantity_MW_lag66', 'quantity_MW_lag67', 'quantity_MW_lag68', 'quantity_MW_lag69', 'quantity_MW_lag70', 'quantity_MW_lag71', 'quantity_MW_lag72', 'quantity_MW_lag73', 'quantity_MW_lag74', 'quantity_MW_lag75', 'quantity_MW_lag76', 'quantity_MW_lag77', 'quantity_MW_lag78', 'quantity_MW_lag79', 'quantity_MW_lag80', 'quantity_MW_lag81', 'quantity_MW_lag82', 'quantity_MW_lag83', 'quantity_MW_lag84', 'quantity_MW_lag85', 'quantity_MW_lag86', 'quantity_MW_lag87', 'quantity_MW_lag88', 'quantity_MW_lag89', 'quantity_MW_lag90', 'quantity_MW_lag91', 'quantity_MW_lag92', 'quantity_MW_lag93', 'quantity_MW_lag94', 'quantity_MW_lag95', 'quantity_MW_lag96', 'quantity_MW_lag97', 'quantity_MW_lag98', 'quantity_MW_lag99', 'quantity_MW_lag100', 'quantity_MW_lag101', 'quantity_MW_lag102', 'quantity_MW_lag103', 'quantity_MW_lag104', 'quantity_MW_lag105', 'quantity_MW_lag106', 'quantity_MW_lag107', 'quantity_MW_lag108', 'quantity_MW_lag109', 'quantity_MW_lag110', 'quantity_MW_lag111', 'quantity_MW_lag112', 'quantity_MW_lag113', 'quantity_MW_lag114', 'quantity_MW_lag115', 'quantity_MW_lag116', 'quantity_MW_lag117', 'quantity_MW_lag118', 'quantity_MW_lag119', 'quantity_MW_lag120', 'quantity_MW_lag121', 'quantity_MW_lag122', 'quantity_MW_lag123', 'quantity_MW_lag124', 'quantity_MW_lag125', 'quantity_MW_lag126', 'quantity_MW_lag127', 'quantity_MW_lag128', 'quantity_MW_lag129', 'quantity_MW_lag130', 'quantity_MW_lag131', 'quantity_MW_lag132', 'quantity_MW_lag133', 'quantity_MW_lag134', 'quantity_MW_lag135', 'quantity_MW_lag136', 'quantity_MW_lag137', 'quantity_MW_lag138', 'quantity_MW_lag139', 'quantity_MW_lag140', 'quantity_MW_lag141', 'quantity_MW_lag142', 'quantity_MW_lag143', 'quantity_MW_lag144', 'quantity_MW_lag145', 'quantity_MW_lag146', 'quantity_MW_lag147', 'quantity_MW_lag148', 'quantity_MW_lag149', 'quantity_MW_lag150', 'quantity_MW_lag151', 'quantity_MW_lag152', 'quantity_MW_lag153', 'quantity_MW_lag154', 'quantity_MW_lag155', 'quantity_MW_lag156', 'quantity_MW_lag157', 'quantity_MW_lag158', 'quantity_MW_lag159', 'quantity_MW_lag160', 'quantity_MW_lag161', 'quantity_MW_lag162', 'quantity_MW_lag163', 'quantity_MW_lag164', 'quantity_MW_lag165', 'quantity_MW_lag166', 'quantity_MW_lag167', 'quantity_MW_lag168', 'quantity_MW_lag169', 'quantity_MW_lag170', 'quantity_MW_lag171', 'quantity_MW_lag172', 'quantity_MW_lag173', 'quantity_MW_lag174', 'quantity_MW_lag175', 'quantity_MW_lag176', 'quantity_MW_lag177', 'quantity_MW_lag178', 'quantity_MW_lag179', 'quantity_MW_lag180', 'quantity_MW_lag181', 'quantity_MW_lag182', 'quantity_MW_lag183', 'quantity_MW_lag184', 'quantity_MW_lag185', 'quantity_MW_lag186', 'quantity_MW_lag187', 'quantity_MW_lag188', 'quantity_MW_lag189', 'quantity_MW_lag190', 'quantity_MW_lag191', 'quantity_MW_lag192', 'quantity_MW_lag193', 'quantity_MW_lag194', 'quantity_MW_lag195', 'quantity_MW_lag196', 'quantity_MW_lag197', 'quantity_MW_lag198', 'quantity_MW_lag199', 'quantity_MW_lag200', 'quantity_MW_lag201', 'quantity_MW_lag202', 'quantity_MW_lag203', 'quantity_MW_lag204', 'quantity_MW_lag205', 'quantity_MW_lag206', 'quantity_MW_lag207', 'quantity_MW_lag208', 'quantity_MW_lag209', 'quantity_MW_lag210', 'quantity_MW_lag211', 'quantity_MW_lag212', 'quantity_MW_lag213', 'quantity_MW_lag214', 'quantity_MW_lag215', 'quantity_MW_lag216', 'quantity_MW_lag217', 'quantity_MW_lag218', 'quantity_MW_lag219', 'quantity_MW_lag220', 'quantity_MW_lag221', 'quantity_MW_lag222', 'quantity_MW_lag223', 'quantity_MW_lag224', 'quantity_MW_lag225', 'quantity_MW_lag226', 'quantity_MW_lag227', 'quantity_MW_lag228', 'quantity_MW_lag229', 'quantity_MW_lag230', 'quantity_MW_lag231', 'quantity_MW_lag232', 'quantity_MW_lag233', 'quantity_MW_lag234', 'quantity_MW_lag235', 'quantity_MW_lag236', 'quantity_MW_lag237', 'quantity_MW_lag238', 'quantity_MW_lag239', 'quantity_MW_lag240']\n",
      "[INFO] Ignorando targets n√£o num√©ricos/bool em build_flat_matrix: ['quantity_MW_lead1', 'quantity_MW_lead2', 'quantity_MW_lead3', 'quantity_MW_lead4', 'quantity_MW_lead5', 'quantity_MW_lead6', 'quantity_MW_lead7', 'quantity_MW_lead8', 'quantity_MW_lead9', 'quantity_MW_lead10', 'quantity_MW_lead11', 'quantity_MW_lead12', 'quantity_MW_lead13', 'quantity_MW_lead14', 'quantity_MW_lead15', 'quantity_MW_lead16', 'quantity_MW_lead17', 'quantity_MW_lead18', 'quantity_MW_lead19', 'quantity_MW_lead20', 'quantity_MW_lead21', 'quantity_MW_lead22', 'quantity_MW_lead23', 'quantity_MW_lead24', 'quantity_MW_lead25', 'quantity_MW_lead26', 'quantity_MW_lead27', 'quantity_MW_lead28', 'quantity_MW_lead29', 'quantity_MW_lead30', 'quantity_MW_lead31', 'quantity_MW_lead32', 'quantity_MW_lead33', 'quantity_MW_lead34', 'quantity_MW_lead35', 'quantity_MW_lead36', 'quantity_MW_lead37', 'quantity_MW_lead38', 'quantity_MW_lead39', 'quantity_MW_lead40', 'quantity_MW_lead41', 'quantity_MW_lead42', 'quantity_MW_lead43', 'quantity_MW_lead44', 'quantity_MW_lead45', 'quantity_MW_lead46', 'quantity_MW_lead47', 'quantity_MW_lead48', 'quantity_MW_lead49', 'quantity_MW_lead50', 'quantity_MW_lead51', 'quantity_MW_lead52', 'quantity_MW_lead53', 'quantity_MW_lead54', 'quantity_MW_lead55', 'quantity_MW_lead56', 'quantity_MW_lead57', 'quantity_MW_lead58', 'quantity_MW_lead59', 'quantity_MW_lead60', 'quantity_MW_lead61', 'quantity_MW_lead62', 'quantity_MW_lead63', 'quantity_MW_lead64', 'quantity_MW_lead65', 'quantity_MW_lead66', 'quantity_MW_lead67', 'quantity_MW_lead68', 'quantity_MW_lead69', 'quantity_MW_lead70', 'quantity_MW_lead71', 'quantity_MW_lead72']\n",
      "[‚ÑπÔ∏è] Reduzindo lags: usando 168 e mascarando 72 restantes com -999.0\n",
      "[INFO] Ignorando features n√£o num√©ricas/bool em build_flat_matrix: ['quantity_MW_lag1', 'quantity_MW_lag2', 'quantity_MW_lag3', 'quantity_MW_lag4', 'quantity_MW_lag5', 'quantity_MW_lag6', 'quantity_MW_lag7', 'quantity_MW_lag8', 'quantity_MW_lag9', 'quantity_MW_lag10', 'quantity_MW_lag11', 'quantity_MW_lag12', 'quantity_MW_lag13', 'quantity_MW_lag14', 'quantity_MW_lag15', 'quantity_MW_lag16', 'quantity_MW_lag17', 'quantity_MW_lag18', 'quantity_MW_lag19', 'quantity_MW_lag20', 'quantity_MW_lag21', 'quantity_MW_lag22', 'quantity_MW_lag23', 'quantity_MW_lag24', 'quantity_MW_lag25', 'quantity_MW_lag26', 'quantity_MW_lag27', 'quantity_MW_lag28', 'quantity_MW_lag29', 'quantity_MW_lag30', 'quantity_MW_lag31', 'quantity_MW_lag32', 'quantity_MW_lag33', 'quantity_MW_lag34', 'quantity_MW_lag35', 'quantity_MW_lag36', 'quantity_MW_lag37', 'quantity_MW_lag38', 'quantity_MW_lag39', 'quantity_MW_lag40', 'quantity_MW_lag41', 'quantity_MW_lag42', 'quantity_MW_lag43', 'quantity_MW_lag44', 'quantity_MW_lag45', 'quantity_MW_lag46', 'quantity_MW_lag47', 'quantity_MW_lag48', 'quantity_MW_lag49', 'quantity_MW_lag50', 'quantity_MW_lag51', 'quantity_MW_lag52', 'quantity_MW_lag53', 'quantity_MW_lag54', 'quantity_MW_lag55', 'quantity_MW_lag56', 'quantity_MW_lag57', 'quantity_MW_lag58', 'quantity_MW_lag59', 'quantity_MW_lag60', 'quantity_MW_lag61', 'quantity_MW_lag62', 'quantity_MW_lag63', 'quantity_MW_lag64', 'quantity_MW_lag65', 'quantity_MW_lag66', 'quantity_MW_lag67', 'quantity_MW_lag68', 'quantity_MW_lag69', 'quantity_MW_lag70', 'quantity_MW_lag71', 'quantity_MW_lag72', 'quantity_MW_lag73', 'quantity_MW_lag74', 'quantity_MW_lag75', 'quantity_MW_lag76', 'quantity_MW_lag77', 'quantity_MW_lag78', 'quantity_MW_lag79', 'quantity_MW_lag80', 'quantity_MW_lag81', 'quantity_MW_lag82', 'quantity_MW_lag83', 'quantity_MW_lag84', 'quantity_MW_lag85', 'quantity_MW_lag86', 'quantity_MW_lag87', 'quantity_MW_lag88', 'quantity_MW_lag89', 'quantity_MW_lag90', 'quantity_MW_lag91', 'quantity_MW_lag92', 'quantity_MW_lag93', 'quantity_MW_lag94', 'quantity_MW_lag95', 'quantity_MW_lag96', 'quantity_MW_lag97', 'quantity_MW_lag98', 'quantity_MW_lag99', 'quantity_MW_lag100', 'quantity_MW_lag101', 'quantity_MW_lag102', 'quantity_MW_lag103', 'quantity_MW_lag104', 'quantity_MW_lag105', 'quantity_MW_lag106', 'quantity_MW_lag107', 'quantity_MW_lag108', 'quantity_MW_lag109', 'quantity_MW_lag110', 'quantity_MW_lag111', 'quantity_MW_lag112', 'quantity_MW_lag113', 'quantity_MW_lag114', 'quantity_MW_lag115', 'quantity_MW_lag116', 'quantity_MW_lag117', 'quantity_MW_lag118', 'quantity_MW_lag119', 'quantity_MW_lag120', 'quantity_MW_lag121', 'quantity_MW_lag122', 'quantity_MW_lag123', 'quantity_MW_lag124', 'quantity_MW_lag125', 'quantity_MW_lag126', 'quantity_MW_lag127', 'quantity_MW_lag128', 'quantity_MW_lag129', 'quantity_MW_lag130', 'quantity_MW_lag131', 'quantity_MW_lag132', 'quantity_MW_lag133', 'quantity_MW_lag134', 'quantity_MW_lag135', 'quantity_MW_lag136', 'quantity_MW_lag137', 'quantity_MW_lag138', 'quantity_MW_lag139', 'quantity_MW_lag140', 'quantity_MW_lag141', 'quantity_MW_lag142', 'quantity_MW_lag143', 'quantity_MW_lag144', 'quantity_MW_lag145', 'quantity_MW_lag146', 'quantity_MW_lag147', 'quantity_MW_lag148', 'quantity_MW_lag149', 'quantity_MW_lag150', 'quantity_MW_lag151', 'quantity_MW_lag152', 'quantity_MW_lag153', 'quantity_MW_lag154', 'quantity_MW_lag155', 'quantity_MW_lag156', 'quantity_MW_lag157', 'quantity_MW_lag158', 'quantity_MW_lag159', 'quantity_MW_lag160', 'quantity_MW_lag161', 'quantity_MW_lag162', 'quantity_MW_lag163', 'quantity_MW_lag164', 'quantity_MW_lag165', 'quantity_MW_lag166', 'quantity_MW_lag167', 'quantity_MW_lag168', 'quantity_MW_lag169', 'quantity_MW_lag170', 'quantity_MW_lag171', 'quantity_MW_lag172', 'quantity_MW_lag173', 'quantity_MW_lag174', 'quantity_MW_lag175', 'quantity_MW_lag176', 'quantity_MW_lag177', 'quantity_MW_lag178', 'quantity_MW_lag179', 'quantity_MW_lag180', 'quantity_MW_lag181', 'quantity_MW_lag182', 'quantity_MW_lag183', 'quantity_MW_lag184', 'quantity_MW_lag185', 'quantity_MW_lag186', 'quantity_MW_lag187', 'quantity_MW_lag188', 'quantity_MW_lag189', 'quantity_MW_lag190', 'quantity_MW_lag191', 'quantity_MW_lag192', 'quantity_MW_lag193', 'quantity_MW_lag194', 'quantity_MW_lag195', 'quantity_MW_lag196', 'quantity_MW_lag197', 'quantity_MW_lag198', 'quantity_MW_lag199', 'quantity_MW_lag200', 'quantity_MW_lag201', 'quantity_MW_lag202', 'quantity_MW_lag203', 'quantity_MW_lag204', 'quantity_MW_lag205', 'quantity_MW_lag206', 'quantity_MW_lag207', 'quantity_MW_lag208', 'quantity_MW_lag209', 'quantity_MW_lag210', 'quantity_MW_lag211', 'quantity_MW_lag212', 'quantity_MW_lag213', 'quantity_MW_lag214', 'quantity_MW_lag215', 'quantity_MW_lag216', 'quantity_MW_lag217', 'quantity_MW_lag218', 'quantity_MW_lag219', 'quantity_MW_lag220', 'quantity_MW_lag221', 'quantity_MW_lag222', 'quantity_MW_lag223', 'quantity_MW_lag224', 'quantity_MW_lag225', 'quantity_MW_lag226', 'quantity_MW_lag227', 'quantity_MW_lag228', 'quantity_MW_lag229', 'quantity_MW_lag230', 'quantity_MW_lag231', 'quantity_MW_lag232', 'quantity_MW_lag233', 'quantity_MW_lag234', 'quantity_MW_lag235', 'quantity_MW_lag236', 'quantity_MW_lag237', 'quantity_MW_lag238', 'quantity_MW_lag239', 'quantity_MW_lag240']\n",
      "[INFO] Ignorando targets n√£o num√©ricos/bool em build_flat_matrix: ['quantity_MW_lead1', 'quantity_MW_lead2', 'quantity_MW_lead3', 'quantity_MW_lead4', 'quantity_MW_lead5', 'quantity_MW_lead6', 'quantity_MW_lead7', 'quantity_MW_lead8', 'quantity_MW_lead9', 'quantity_MW_lead10', 'quantity_MW_lead11', 'quantity_MW_lead12', 'quantity_MW_lead13', 'quantity_MW_lead14', 'quantity_MW_lead15', 'quantity_MW_lead16', 'quantity_MW_lead17', 'quantity_MW_lead18', 'quantity_MW_lead19', 'quantity_MW_lead20', 'quantity_MW_lead21', 'quantity_MW_lead22', 'quantity_MW_lead23', 'quantity_MW_lead24', 'quantity_MW_lead25', 'quantity_MW_lead26', 'quantity_MW_lead27', 'quantity_MW_lead28', 'quantity_MW_lead29', 'quantity_MW_lead30', 'quantity_MW_lead31', 'quantity_MW_lead32', 'quantity_MW_lead33', 'quantity_MW_lead34', 'quantity_MW_lead35', 'quantity_MW_lead36', 'quantity_MW_lead37', 'quantity_MW_lead38', 'quantity_MW_lead39', 'quantity_MW_lead40', 'quantity_MW_lead41', 'quantity_MW_lead42', 'quantity_MW_lead43', 'quantity_MW_lead44', 'quantity_MW_lead45', 'quantity_MW_lead46', 'quantity_MW_lead47', 'quantity_MW_lead48', 'quantity_MW_lead49', 'quantity_MW_lead50', 'quantity_MW_lead51', 'quantity_MW_lead52', 'quantity_MW_lead53', 'quantity_MW_lead54', 'quantity_MW_lead55', 'quantity_MW_lead56', 'quantity_MW_lead57', 'quantity_MW_lead58', 'quantity_MW_lead59', 'quantity_MW_lead60', 'quantity_MW_lead61', 'quantity_MW_lead62', 'quantity_MW_lead63', 'quantity_MW_lead64', 'quantity_MW_lead65', 'quantity_MW_lead66', 'quantity_MW_lead67', 'quantity_MW_lead68', 'quantity_MW_lead69', 'quantity_MW_lead70', 'quantity_MW_lead71', 'quantity_MW_lead72']\n",
      "[INFO] Ignorando features n√£o num√©ricas/bool em build_flat_matrix: ['quantity_MW_lag1', 'quantity_MW_lag2', 'quantity_MW_lag3', 'quantity_MW_lag4', 'quantity_MW_lag5', 'quantity_MW_lag6', 'quantity_MW_lag7', 'quantity_MW_lag8', 'quantity_MW_lag9', 'quantity_MW_lag10', 'quantity_MW_lag11', 'quantity_MW_lag12', 'quantity_MW_lag13', 'quantity_MW_lag14', 'quantity_MW_lag15', 'quantity_MW_lag16', 'quantity_MW_lag17', 'quantity_MW_lag18', 'quantity_MW_lag19', 'quantity_MW_lag20', 'quantity_MW_lag21', 'quantity_MW_lag22', 'quantity_MW_lag23', 'quantity_MW_lag24', 'quantity_MW_lag25', 'quantity_MW_lag26', 'quantity_MW_lag27', 'quantity_MW_lag28', 'quantity_MW_lag29', 'quantity_MW_lag30', 'quantity_MW_lag31', 'quantity_MW_lag32', 'quantity_MW_lag33', 'quantity_MW_lag34', 'quantity_MW_lag35', 'quantity_MW_lag36', 'quantity_MW_lag37', 'quantity_MW_lag38', 'quantity_MW_lag39', 'quantity_MW_lag40', 'quantity_MW_lag41', 'quantity_MW_lag42', 'quantity_MW_lag43', 'quantity_MW_lag44', 'quantity_MW_lag45', 'quantity_MW_lag46', 'quantity_MW_lag47', 'quantity_MW_lag48', 'quantity_MW_lag49', 'quantity_MW_lag50', 'quantity_MW_lag51', 'quantity_MW_lag52', 'quantity_MW_lag53', 'quantity_MW_lag54', 'quantity_MW_lag55', 'quantity_MW_lag56', 'quantity_MW_lag57', 'quantity_MW_lag58', 'quantity_MW_lag59', 'quantity_MW_lag60', 'quantity_MW_lag61', 'quantity_MW_lag62', 'quantity_MW_lag63', 'quantity_MW_lag64', 'quantity_MW_lag65', 'quantity_MW_lag66', 'quantity_MW_lag67', 'quantity_MW_lag68', 'quantity_MW_lag69', 'quantity_MW_lag70', 'quantity_MW_lag71', 'quantity_MW_lag72', 'quantity_MW_lag73', 'quantity_MW_lag74', 'quantity_MW_lag75', 'quantity_MW_lag76', 'quantity_MW_lag77', 'quantity_MW_lag78', 'quantity_MW_lag79', 'quantity_MW_lag80', 'quantity_MW_lag81', 'quantity_MW_lag82', 'quantity_MW_lag83', 'quantity_MW_lag84', 'quantity_MW_lag85', 'quantity_MW_lag86', 'quantity_MW_lag87', 'quantity_MW_lag88', 'quantity_MW_lag89', 'quantity_MW_lag90', 'quantity_MW_lag91', 'quantity_MW_lag92', 'quantity_MW_lag93', 'quantity_MW_lag94', 'quantity_MW_lag95', 'quantity_MW_lag96', 'quantity_MW_lag97', 'quantity_MW_lag98', 'quantity_MW_lag99', 'quantity_MW_lag100', 'quantity_MW_lag101', 'quantity_MW_lag102', 'quantity_MW_lag103', 'quantity_MW_lag104', 'quantity_MW_lag105', 'quantity_MW_lag106', 'quantity_MW_lag107', 'quantity_MW_lag108', 'quantity_MW_lag109', 'quantity_MW_lag110', 'quantity_MW_lag111', 'quantity_MW_lag112', 'quantity_MW_lag113', 'quantity_MW_lag114', 'quantity_MW_lag115', 'quantity_MW_lag116', 'quantity_MW_lag117', 'quantity_MW_lag118', 'quantity_MW_lag119', 'quantity_MW_lag120', 'quantity_MW_lag121', 'quantity_MW_lag122', 'quantity_MW_lag123', 'quantity_MW_lag124', 'quantity_MW_lag125', 'quantity_MW_lag126', 'quantity_MW_lag127', 'quantity_MW_lag128', 'quantity_MW_lag129', 'quantity_MW_lag130', 'quantity_MW_lag131', 'quantity_MW_lag132', 'quantity_MW_lag133', 'quantity_MW_lag134', 'quantity_MW_lag135', 'quantity_MW_lag136', 'quantity_MW_lag137', 'quantity_MW_lag138', 'quantity_MW_lag139', 'quantity_MW_lag140', 'quantity_MW_lag141', 'quantity_MW_lag142', 'quantity_MW_lag143', 'quantity_MW_lag144', 'quantity_MW_lag145', 'quantity_MW_lag146', 'quantity_MW_lag147', 'quantity_MW_lag148', 'quantity_MW_lag149', 'quantity_MW_lag150', 'quantity_MW_lag151', 'quantity_MW_lag152', 'quantity_MW_lag153', 'quantity_MW_lag154', 'quantity_MW_lag155', 'quantity_MW_lag156', 'quantity_MW_lag157', 'quantity_MW_lag158', 'quantity_MW_lag159', 'quantity_MW_lag160', 'quantity_MW_lag161', 'quantity_MW_lag162', 'quantity_MW_lag163', 'quantity_MW_lag164', 'quantity_MW_lag165', 'quantity_MW_lag166', 'quantity_MW_lag167', 'quantity_MW_lag168', 'quantity_MW_lag169', 'quantity_MW_lag170', 'quantity_MW_lag171', 'quantity_MW_lag172', 'quantity_MW_lag173', 'quantity_MW_lag174', 'quantity_MW_lag175', 'quantity_MW_lag176', 'quantity_MW_lag177', 'quantity_MW_lag178', 'quantity_MW_lag179', 'quantity_MW_lag180', 'quantity_MW_lag181', 'quantity_MW_lag182', 'quantity_MW_lag183', 'quantity_MW_lag184', 'quantity_MW_lag185', 'quantity_MW_lag186', 'quantity_MW_lag187', 'quantity_MW_lag188', 'quantity_MW_lag189', 'quantity_MW_lag190', 'quantity_MW_lag191', 'quantity_MW_lag192', 'quantity_MW_lag193', 'quantity_MW_lag194', 'quantity_MW_lag195', 'quantity_MW_lag196', 'quantity_MW_lag197', 'quantity_MW_lag198', 'quantity_MW_lag199', 'quantity_MW_lag200', 'quantity_MW_lag201', 'quantity_MW_lag202', 'quantity_MW_lag203', 'quantity_MW_lag204', 'quantity_MW_lag205', 'quantity_MW_lag206', 'quantity_MW_lag207', 'quantity_MW_lag208', 'quantity_MW_lag209', 'quantity_MW_lag210', 'quantity_MW_lag211', 'quantity_MW_lag212', 'quantity_MW_lag213', 'quantity_MW_lag214', 'quantity_MW_lag215', 'quantity_MW_lag216', 'quantity_MW_lag217', 'quantity_MW_lag218', 'quantity_MW_lag219', 'quantity_MW_lag220', 'quantity_MW_lag221', 'quantity_MW_lag222', 'quantity_MW_lag223', 'quantity_MW_lag224', 'quantity_MW_lag225', 'quantity_MW_lag226', 'quantity_MW_lag227', 'quantity_MW_lag228', 'quantity_MW_lag229', 'quantity_MW_lag230', 'quantity_MW_lag231', 'quantity_MW_lag232', 'quantity_MW_lag233', 'quantity_MW_lag234', 'quantity_MW_lag235', 'quantity_MW_lag236', 'quantity_MW_lag237', 'quantity_MW_lag238', 'quantity_MW_lag239', 'quantity_MW_lag240']\n",
      "[INFO] Ignorando targets n√£o num√©ricos/bool em build_flat_matrix: ['quantity_MW_lead1', 'quantity_MW_lead2', 'quantity_MW_lead3', 'quantity_MW_lead4', 'quantity_MW_lead5', 'quantity_MW_lead6', 'quantity_MW_lead7', 'quantity_MW_lead8', 'quantity_MW_lead9', 'quantity_MW_lead10', 'quantity_MW_lead11', 'quantity_MW_lead12', 'quantity_MW_lead13', 'quantity_MW_lead14', 'quantity_MW_lead15', 'quantity_MW_lead16', 'quantity_MW_lead17', 'quantity_MW_lead18', 'quantity_MW_lead19', 'quantity_MW_lead20', 'quantity_MW_lead21', 'quantity_MW_lead22', 'quantity_MW_lead23', 'quantity_MW_lead24', 'quantity_MW_lead25', 'quantity_MW_lead26', 'quantity_MW_lead27', 'quantity_MW_lead28', 'quantity_MW_lead29', 'quantity_MW_lead30', 'quantity_MW_lead31', 'quantity_MW_lead32', 'quantity_MW_lead33', 'quantity_MW_lead34', 'quantity_MW_lead35', 'quantity_MW_lead36', 'quantity_MW_lead37', 'quantity_MW_lead38', 'quantity_MW_lead39', 'quantity_MW_lead40', 'quantity_MW_lead41', 'quantity_MW_lead42', 'quantity_MW_lead43', 'quantity_MW_lead44', 'quantity_MW_lead45', 'quantity_MW_lead46', 'quantity_MW_lead47', 'quantity_MW_lead48', 'quantity_MW_lead49', 'quantity_MW_lead50', 'quantity_MW_lead51', 'quantity_MW_lead52', 'quantity_MW_lead53', 'quantity_MW_lead54', 'quantity_MW_lead55', 'quantity_MW_lead56', 'quantity_MW_lead57', 'quantity_MW_lead58', 'quantity_MW_lead59', 'quantity_MW_lead60', 'quantity_MW_lead61', 'quantity_MW_lead62', 'quantity_MW_lead63', 'quantity_MW_lead64', 'quantity_MW_lead65', 'quantity_MW_lead66', 'quantity_MW_lead67', 'quantity_MW_lead68', 'quantity_MW_lead69', 'quantity_MW_lead70', 'quantity_MW_lead71', 'quantity_MW_lead72']\n",
      "[INFO] Ignorando features n√£o num√©ricas/bool em build_flat_matrix: ['quantity_MW_lag1', 'quantity_MW_lag2', 'quantity_MW_lag3', 'quantity_MW_lag4', 'quantity_MW_lag5', 'quantity_MW_lag6', 'quantity_MW_lag7', 'quantity_MW_lag8', 'quantity_MW_lag9', 'quantity_MW_lag10', 'quantity_MW_lag11', 'quantity_MW_lag12', 'quantity_MW_lag13', 'quantity_MW_lag14', 'quantity_MW_lag15', 'quantity_MW_lag16', 'quantity_MW_lag17', 'quantity_MW_lag18', 'quantity_MW_lag19', 'quantity_MW_lag20', 'quantity_MW_lag21', 'quantity_MW_lag22', 'quantity_MW_lag23', 'quantity_MW_lag24', 'quantity_MW_lag25', 'quantity_MW_lag26', 'quantity_MW_lag27', 'quantity_MW_lag28', 'quantity_MW_lag29', 'quantity_MW_lag30', 'quantity_MW_lag31', 'quantity_MW_lag32', 'quantity_MW_lag33', 'quantity_MW_lag34', 'quantity_MW_lag35', 'quantity_MW_lag36', 'quantity_MW_lag37', 'quantity_MW_lag38', 'quantity_MW_lag39', 'quantity_MW_lag40', 'quantity_MW_lag41', 'quantity_MW_lag42', 'quantity_MW_lag43', 'quantity_MW_lag44', 'quantity_MW_lag45', 'quantity_MW_lag46', 'quantity_MW_lag47', 'quantity_MW_lag48', 'quantity_MW_lag49', 'quantity_MW_lag50', 'quantity_MW_lag51', 'quantity_MW_lag52', 'quantity_MW_lag53', 'quantity_MW_lag54', 'quantity_MW_lag55', 'quantity_MW_lag56', 'quantity_MW_lag57', 'quantity_MW_lag58', 'quantity_MW_lag59', 'quantity_MW_lag60', 'quantity_MW_lag61', 'quantity_MW_lag62', 'quantity_MW_lag63', 'quantity_MW_lag64', 'quantity_MW_lag65', 'quantity_MW_lag66', 'quantity_MW_lag67', 'quantity_MW_lag68', 'quantity_MW_lag69', 'quantity_MW_lag70', 'quantity_MW_lag71', 'quantity_MW_lag72', 'quantity_MW_lag73', 'quantity_MW_lag74', 'quantity_MW_lag75', 'quantity_MW_lag76', 'quantity_MW_lag77', 'quantity_MW_lag78', 'quantity_MW_lag79', 'quantity_MW_lag80', 'quantity_MW_lag81', 'quantity_MW_lag82', 'quantity_MW_lag83', 'quantity_MW_lag84', 'quantity_MW_lag85', 'quantity_MW_lag86', 'quantity_MW_lag87', 'quantity_MW_lag88', 'quantity_MW_lag89', 'quantity_MW_lag90', 'quantity_MW_lag91', 'quantity_MW_lag92', 'quantity_MW_lag93', 'quantity_MW_lag94', 'quantity_MW_lag95', 'quantity_MW_lag96', 'quantity_MW_lag97', 'quantity_MW_lag98', 'quantity_MW_lag99', 'quantity_MW_lag100', 'quantity_MW_lag101', 'quantity_MW_lag102', 'quantity_MW_lag103', 'quantity_MW_lag104', 'quantity_MW_lag105', 'quantity_MW_lag106', 'quantity_MW_lag107', 'quantity_MW_lag108', 'quantity_MW_lag109', 'quantity_MW_lag110', 'quantity_MW_lag111', 'quantity_MW_lag112', 'quantity_MW_lag113', 'quantity_MW_lag114', 'quantity_MW_lag115', 'quantity_MW_lag116', 'quantity_MW_lag117', 'quantity_MW_lag118', 'quantity_MW_lag119', 'quantity_MW_lag120', 'quantity_MW_lag121', 'quantity_MW_lag122', 'quantity_MW_lag123', 'quantity_MW_lag124', 'quantity_MW_lag125', 'quantity_MW_lag126', 'quantity_MW_lag127', 'quantity_MW_lag128', 'quantity_MW_lag129', 'quantity_MW_lag130', 'quantity_MW_lag131', 'quantity_MW_lag132', 'quantity_MW_lag133', 'quantity_MW_lag134', 'quantity_MW_lag135', 'quantity_MW_lag136', 'quantity_MW_lag137', 'quantity_MW_lag138', 'quantity_MW_lag139', 'quantity_MW_lag140', 'quantity_MW_lag141', 'quantity_MW_lag142', 'quantity_MW_lag143', 'quantity_MW_lag144', 'quantity_MW_lag145', 'quantity_MW_lag146', 'quantity_MW_lag147', 'quantity_MW_lag148', 'quantity_MW_lag149', 'quantity_MW_lag150', 'quantity_MW_lag151', 'quantity_MW_lag152', 'quantity_MW_lag153', 'quantity_MW_lag154', 'quantity_MW_lag155', 'quantity_MW_lag156', 'quantity_MW_lag157', 'quantity_MW_lag158', 'quantity_MW_lag159', 'quantity_MW_lag160', 'quantity_MW_lag161', 'quantity_MW_lag162', 'quantity_MW_lag163', 'quantity_MW_lag164', 'quantity_MW_lag165', 'quantity_MW_lag166', 'quantity_MW_lag167', 'quantity_MW_lag168', 'quantity_MW_lag169', 'quantity_MW_lag170', 'quantity_MW_lag171', 'quantity_MW_lag172', 'quantity_MW_lag173', 'quantity_MW_lag174', 'quantity_MW_lag175', 'quantity_MW_lag176', 'quantity_MW_lag177', 'quantity_MW_lag178', 'quantity_MW_lag179', 'quantity_MW_lag180', 'quantity_MW_lag181', 'quantity_MW_lag182', 'quantity_MW_lag183', 'quantity_MW_lag184', 'quantity_MW_lag185', 'quantity_MW_lag186', 'quantity_MW_lag187', 'quantity_MW_lag188', 'quantity_MW_lag189', 'quantity_MW_lag190', 'quantity_MW_lag191', 'quantity_MW_lag192', 'quantity_MW_lag193', 'quantity_MW_lag194', 'quantity_MW_lag195', 'quantity_MW_lag196', 'quantity_MW_lag197', 'quantity_MW_lag198', 'quantity_MW_lag199', 'quantity_MW_lag200', 'quantity_MW_lag201', 'quantity_MW_lag202', 'quantity_MW_lag203', 'quantity_MW_lag204', 'quantity_MW_lag205', 'quantity_MW_lag206', 'quantity_MW_lag207', 'quantity_MW_lag208', 'quantity_MW_lag209', 'quantity_MW_lag210', 'quantity_MW_lag211', 'quantity_MW_lag212', 'quantity_MW_lag213', 'quantity_MW_lag214', 'quantity_MW_lag215', 'quantity_MW_lag216', 'quantity_MW_lag217', 'quantity_MW_lag218', 'quantity_MW_lag219', 'quantity_MW_lag220', 'quantity_MW_lag221', 'quantity_MW_lag222', 'quantity_MW_lag223', 'quantity_MW_lag224', 'quantity_MW_lag225', 'quantity_MW_lag226', 'quantity_MW_lag227', 'quantity_MW_lag228', 'quantity_MW_lag229', 'quantity_MW_lag230', 'quantity_MW_lag231', 'quantity_MW_lag232', 'quantity_MW_lag233', 'quantity_MW_lag234', 'quantity_MW_lag235', 'quantity_MW_lag236', 'quantity_MW_lag237', 'quantity_MW_lag238', 'quantity_MW_lag239', 'quantity_MW_lag240']\n",
      "[INFO] Ignorando targets n√£o num√©ricos/bool em build_flat_matrix: ['quantity_MW_lead1', 'quantity_MW_lead2', 'quantity_MW_lead3', 'quantity_MW_lead4', 'quantity_MW_lead5', 'quantity_MW_lead6', 'quantity_MW_lead7', 'quantity_MW_lead8', 'quantity_MW_lead9', 'quantity_MW_lead10', 'quantity_MW_lead11', 'quantity_MW_lead12', 'quantity_MW_lead13', 'quantity_MW_lead14', 'quantity_MW_lead15', 'quantity_MW_lead16', 'quantity_MW_lead17', 'quantity_MW_lead18', 'quantity_MW_lead19', 'quantity_MW_lead20', 'quantity_MW_lead21', 'quantity_MW_lead22', 'quantity_MW_lead23', 'quantity_MW_lead24', 'quantity_MW_lead25', 'quantity_MW_lead26', 'quantity_MW_lead27', 'quantity_MW_lead28', 'quantity_MW_lead29', 'quantity_MW_lead30', 'quantity_MW_lead31', 'quantity_MW_lead32', 'quantity_MW_lead33', 'quantity_MW_lead34', 'quantity_MW_lead35', 'quantity_MW_lead36', 'quantity_MW_lead37', 'quantity_MW_lead38', 'quantity_MW_lead39', 'quantity_MW_lead40', 'quantity_MW_lead41', 'quantity_MW_lead42', 'quantity_MW_lead43', 'quantity_MW_lead44', 'quantity_MW_lead45', 'quantity_MW_lead46', 'quantity_MW_lead47', 'quantity_MW_lead48', 'quantity_MW_lead49', 'quantity_MW_lead50', 'quantity_MW_lead51', 'quantity_MW_lead52', 'quantity_MW_lead53', 'quantity_MW_lead54', 'quantity_MW_lead55', 'quantity_MW_lead56', 'quantity_MW_lead57', 'quantity_MW_lead58', 'quantity_MW_lead59', 'quantity_MW_lead60', 'quantity_MW_lead61', 'quantity_MW_lead62', 'quantity_MW_lead63', 'quantity_MW_lead64', 'quantity_MW_lead65', 'quantity_MW_lead66', 'quantity_MW_lead67', 'quantity_MW_lead68', 'quantity_MW_lead69', 'quantity_MW_lead70', 'quantity_MW_lead71', 'quantity_MW_lead72']\n",
      "[‚ÑπÔ∏è] Reduzindo lags: usando 72 e mascarando 168 restantes com -999.0\n",
      "[INFO] Ignorando features n√£o num√©ricas/bool em build_flat_matrix: ['quantity_MW_lag1', 'quantity_MW_lag2', 'quantity_MW_lag3', 'quantity_MW_lag4', 'quantity_MW_lag5', 'quantity_MW_lag6', 'quantity_MW_lag7', 'quantity_MW_lag8', 'quantity_MW_lag9', 'quantity_MW_lag10', 'quantity_MW_lag11', 'quantity_MW_lag12', 'quantity_MW_lag13', 'quantity_MW_lag14', 'quantity_MW_lag15', 'quantity_MW_lag16', 'quantity_MW_lag17', 'quantity_MW_lag18', 'quantity_MW_lag19', 'quantity_MW_lag20', 'quantity_MW_lag21', 'quantity_MW_lag22', 'quantity_MW_lag23', 'quantity_MW_lag24', 'quantity_MW_lag25', 'quantity_MW_lag26', 'quantity_MW_lag27', 'quantity_MW_lag28', 'quantity_MW_lag29', 'quantity_MW_lag30', 'quantity_MW_lag31', 'quantity_MW_lag32', 'quantity_MW_lag33', 'quantity_MW_lag34', 'quantity_MW_lag35', 'quantity_MW_lag36', 'quantity_MW_lag37', 'quantity_MW_lag38', 'quantity_MW_lag39', 'quantity_MW_lag40', 'quantity_MW_lag41', 'quantity_MW_lag42', 'quantity_MW_lag43', 'quantity_MW_lag44', 'quantity_MW_lag45', 'quantity_MW_lag46', 'quantity_MW_lag47', 'quantity_MW_lag48', 'quantity_MW_lag49', 'quantity_MW_lag50', 'quantity_MW_lag51', 'quantity_MW_lag52', 'quantity_MW_lag53', 'quantity_MW_lag54', 'quantity_MW_lag55', 'quantity_MW_lag56', 'quantity_MW_lag57', 'quantity_MW_lag58', 'quantity_MW_lag59', 'quantity_MW_lag60', 'quantity_MW_lag61', 'quantity_MW_lag62', 'quantity_MW_lag63', 'quantity_MW_lag64', 'quantity_MW_lag65', 'quantity_MW_lag66', 'quantity_MW_lag67', 'quantity_MW_lag68', 'quantity_MW_lag69', 'quantity_MW_lag70', 'quantity_MW_lag71', 'quantity_MW_lag72', 'quantity_MW_lag73', 'quantity_MW_lag74', 'quantity_MW_lag75', 'quantity_MW_lag76', 'quantity_MW_lag77', 'quantity_MW_lag78', 'quantity_MW_lag79', 'quantity_MW_lag80', 'quantity_MW_lag81', 'quantity_MW_lag82', 'quantity_MW_lag83', 'quantity_MW_lag84', 'quantity_MW_lag85', 'quantity_MW_lag86', 'quantity_MW_lag87', 'quantity_MW_lag88', 'quantity_MW_lag89', 'quantity_MW_lag90', 'quantity_MW_lag91', 'quantity_MW_lag92', 'quantity_MW_lag93', 'quantity_MW_lag94', 'quantity_MW_lag95', 'quantity_MW_lag96', 'quantity_MW_lag97', 'quantity_MW_lag98', 'quantity_MW_lag99', 'quantity_MW_lag100', 'quantity_MW_lag101', 'quantity_MW_lag102', 'quantity_MW_lag103', 'quantity_MW_lag104', 'quantity_MW_lag105', 'quantity_MW_lag106', 'quantity_MW_lag107', 'quantity_MW_lag108', 'quantity_MW_lag109', 'quantity_MW_lag110', 'quantity_MW_lag111', 'quantity_MW_lag112', 'quantity_MW_lag113', 'quantity_MW_lag114', 'quantity_MW_lag115', 'quantity_MW_lag116', 'quantity_MW_lag117', 'quantity_MW_lag118', 'quantity_MW_lag119', 'quantity_MW_lag120', 'quantity_MW_lag121', 'quantity_MW_lag122', 'quantity_MW_lag123', 'quantity_MW_lag124', 'quantity_MW_lag125', 'quantity_MW_lag126', 'quantity_MW_lag127', 'quantity_MW_lag128', 'quantity_MW_lag129', 'quantity_MW_lag130', 'quantity_MW_lag131', 'quantity_MW_lag132', 'quantity_MW_lag133', 'quantity_MW_lag134', 'quantity_MW_lag135', 'quantity_MW_lag136', 'quantity_MW_lag137', 'quantity_MW_lag138', 'quantity_MW_lag139', 'quantity_MW_lag140', 'quantity_MW_lag141', 'quantity_MW_lag142', 'quantity_MW_lag143', 'quantity_MW_lag144', 'quantity_MW_lag145', 'quantity_MW_lag146', 'quantity_MW_lag147', 'quantity_MW_lag148', 'quantity_MW_lag149', 'quantity_MW_lag150', 'quantity_MW_lag151', 'quantity_MW_lag152', 'quantity_MW_lag153', 'quantity_MW_lag154', 'quantity_MW_lag155', 'quantity_MW_lag156', 'quantity_MW_lag157', 'quantity_MW_lag158', 'quantity_MW_lag159', 'quantity_MW_lag160', 'quantity_MW_lag161', 'quantity_MW_lag162', 'quantity_MW_lag163', 'quantity_MW_lag164', 'quantity_MW_lag165', 'quantity_MW_lag166', 'quantity_MW_lag167', 'quantity_MW_lag168', 'quantity_MW_lag169', 'quantity_MW_lag170', 'quantity_MW_lag171', 'quantity_MW_lag172', 'quantity_MW_lag173', 'quantity_MW_lag174', 'quantity_MW_lag175', 'quantity_MW_lag176', 'quantity_MW_lag177', 'quantity_MW_lag178', 'quantity_MW_lag179', 'quantity_MW_lag180', 'quantity_MW_lag181', 'quantity_MW_lag182', 'quantity_MW_lag183', 'quantity_MW_lag184', 'quantity_MW_lag185', 'quantity_MW_lag186', 'quantity_MW_lag187', 'quantity_MW_lag188', 'quantity_MW_lag189', 'quantity_MW_lag190', 'quantity_MW_lag191', 'quantity_MW_lag192', 'quantity_MW_lag193', 'quantity_MW_lag194', 'quantity_MW_lag195', 'quantity_MW_lag196', 'quantity_MW_lag197', 'quantity_MW_lag198', 'quantity_MW_lag199', 'quantity_MW_lag200', 'quantity_MW_lag201', 'quantity_MW_lag202', 'quantity_MW_lag203', 'quantity_MW_lag204', 'quantity_MW_lag205', 'quantity_MW_lag206', 'quantity_MW_lag207', 'quantity_MW_lag208', 'quantity_MW_lag209', 'quantity_MW_lag210', 'quantity_MW_lag211', 'quantity_MW_lag212', 'quantity_MW_lag213', 'quantity_MW_lag214', 'quantity_MW_lag215', 'quantity_MW_lag216', 'quantity_MW_lag217', 'quantity_MW_lag218', 'quantity_MW_lag219', 'quantity_MW_lag220', 'quantity_MW_lag221', 'quantity_MW_lag222', 'quantity_MW_lag223', 'quantity_MW_lag224', 'quantity_MW_lag225', 'quantity_MW_lag226', 'quantity_MW_lag227', 'quantity_MW_lag228', 'quantity_MW_lag229', 'quantity_MW_lag230', 'quantity_MW_lag231', 'quantity_MW_lag232', 'quantity_MW_lag233', 'quantity_MW_lag234', 'quantity_MW_lag235', 'quantity_MW_lag236', 'quantity_MW_lag237', 'quantity_MW_lag238', 'quantity_MW_lag239', 'quantity_MW_lag240']\n",
      "[INFO] Ignorando targets n√£o num√©ricos/bool em build_flat_matrix: ['quantity_MW_lead1', 'quantity_MW_lead2', 'quantity_MW_lead3', 'quantity_MW_lead4', 'quantity_MW_lead5', 'quantity_MW_lead6', 'quantity_MW_lead7', 'quantity_MW_lead8', 'quantity_MW_lead9', 'quantity_MW_lead10', 'quantity_MW_lead11', 'quantity_MW_lead12', 'quantity_MW_lead13', 'quantity_MW_lead14', 'quantity_MW_lead15', 'quantity_MW_lead16', 'quantity_MW_lead17', 'quantity_MW_lead18', 'quantity_MW_lead19', 'quantity_MW_lead20', 'quantity_MW_lead21', 'quantity_MW_lead22', 'quantity_MW_lead23', 'quantity_MW_lead24', 'quantity_MW_lead25', 'quantity_MW_lead26', 'quantity_MW_lead27', 'quantity_MW_lead28', 'quantity_MW_lead29', 'quantity_MW_lead30', 'quantity_MW_lead31', 'quantity_MW_lead32', 'quantity_MW_lead33', 'quantity_MW_lead34', 'quantity_MW_lead35', 'quantity_MW_lead36', 'quantity_MW_lead37', 'quantity_MW_lead38', 'quantity_MW_lead39', 'quantity_MW_lead40', 'quantity_MW_lead41', 'quantity_MW_lead42', 'quantity_MW_lead43', 'quantity_MW_lead44', 'quantity_MW_lead45', 'quantity_MW_lead46', 'quantity_MW_lead47', 'quantity_MW_lead48', 'quantity_MW_lead49', 'quantity_MW_lead50', 'quantity_MW_lead51', 'quantity_MW_lead52', 'quantity_MW_lead53', 'quantity_MW_lead54', 'quantity_MW_lead55', 'quantity_MW_lead56', 'quantity_MW_lead57', 'quantity_MW_lead58', 'quantity_MW_lead59', 'quantity_MW_lead60', 'quantity_MW_lead61', 'quantity_MW_lead62', 'quantity_MW_lead63', 'quantity_MW_lead64', 'quantity_MW_lead65', 'quantity_MW_lead66', 'quantity_MW_lead67', 'quantity_MW_lead68', 'quantity_MW_lead69', 'quantity_MW_lead70', 'quantity_MW_lead71', 'quantity_MW_lead72']\n",
      "[‚ÑπÔ∏è] Reduzindo lags: usando 72 e mascarando 168 restantes com -999.0\n",
      "[INFO] Ignorando features n√£o num√©ricas/bool em build_flat_matrix: ['quantity_MW_lag1', 'quantity_MW_lag2', 'quantity_MW_lag3', 'quantity_MW_lag4', 'quantity_MW_lag5', 'quantity_MW_lag6', 'quantity_MW_lag7', 'quantity_MW_lag8', 'quantity_MW_lag9', 'quantity_MW_lag10', 'quantity_MW_lag11', 'quantity_MW_lag12', 'quantity_MW_lag13', 'quantity_MW_lag14', 'quantity_MW_lag15', 'quantity_MW_lag16', 'quantity_MW_lag17', 'quantity_MW_lag18', 'quantity_MW_lag19', 'quantity_MW_lag20', 'quantity_MW_lag21', 'quantity_MW_lag22', 'quantity_MW_lag23', 'quantity_MW_lag24', 'quantity_MW_lag25', 'quantity_MW_lag26', 'quantity_MW_lag27', 'quantity_MW_lag28', 'quantity_MW_lag29', 'quantity_MW_lag30', 'quantity_MW_lag31', 'quantity_MW_lag32', 'quantity_MW_lag33', 'quantity_MW_lag34', 'quantity_MW_lag35', 'quantity_MW_lag36', 'quantity_MW_lag37', 'quantity_MW_lag38', 'quantity_MW_lag39', 'quantity_MW_lag40', 'quantity_MW_lag41', 'quantity_MW_lag42', 'quantity_MW_lag43', 'quantity_MW_lag44', 'quantity_MW_lag45', 'quantity_MW_lag46', 'quantity_MW_lag47', 'quantity_MW_lag48', 'quantity_MW_lag49', 'quantity_MW_lag50', 'quantity_MW_lag51', 'quantity_MW_lag52', 'quantity_MW_lag53', 'quantity_MW_lag54', 'quantity_MW_lag55', 'quantity_MW_lag56', 'quantity_MW_lag57', 'quantity_MW_lag58', 'quantity_MW_lag59', 'quantity_MW_lag60', 'quantity_MW_lag61', 'quantity_MW_lag62', 'quantity_MW_lag63', 'quantity_MW_lag64', 'quantity_MW_lag65', 'quantity_MW_lag66', 'quantity_MW_lag67', 'quantity_MW_lag68', 'quantity_MW_lag69', 'quantity_MW_lag70', 'quantity_MW_lag71', 'quantity_MW_lag72', 'quantity_MW_lag73', 'quantity_MW_lag74', 'quantity_MW_lag75', 'quantity_MW_lag76', 'quantity_MW_lag77', 'quantity_MW_lag78', 'quantity_MW_lag79', 'quantity_MW_lag80', 'quantity_MW_lag81', 'quantity_MW_lag82', 'quantity_MW_lag83', 'quantity_MW_lag84', 'quantity_MW_lag85', 'quantity_MW_lag86', 'quantity_MW_lag87', 'quantity_MW_lag88', 'quantity_MW_lag89', 'quantity_MW_lag90', 'quantity_MW_lag91', 'quantity_MW_lag92', 'quantity_MW_lag93', 'quantity_MW_lag94', 'quantity_MW_lag95', 'quantity_MW_lag96', 'quantity_MW_lag97', 'quantity_MW_lag98', 'quantity_MW_lag99', 'quantity_MW_lag100', 'quantity_MW_lag101', 'quantity_MW_lag102', 'quantity_MW_lag103', 'quantity_MW_lag104', 'quantity_MW_lag105', 'quantity_MW_lag106', 'quantity_MW_lag107', 'quantity_MW_lag108', 'quantity_MW_lag109', 'quantity_MW_lag110', 'quantity_MW_lag111', 'quantity_MW_lag112', 'quantity_MW_lag113', 'quantity_MW_lag114', 'quantity_MW_lag115', 'quantity_MW_lag116', 'quantity_MW_lag117', 'quantity_MW_lag118', 'quantity_MW_lag119', 'quantity_MW_lag120', 'quantity_MW_lag121', 'quantity_MW_lag122', 'quantity_MW_lag123', 'quantity_MW_lag124', 'quantity_MW_lag125', 'quantity_MW_lag126', 'quantity_MW_lag127', 'quantity_MW_lag128', 'quantity_MW_lag129', 'quantity_MW_lag130', 'quantity_MW_lag131', 'quantity_MW_lag132', 'quantity_MW_lag133', 'quantity_MW_lag134', 'quantity_MW_lag135', 'quantity_MW_lag136', 'quantity_MW_lag137', 'quantity_MW_lag138', 'quantity_MW_lag139', 'quantity_MW_lag140', 'quantity_MW_lag141', 'quantity_MW_lag142', 'quantity_MW_lag143', 'quantity_MW_lag144', 'quantity_MW_lag145', 'quantity_MW_lag146', 'quantity_MW_lag147', 'quantity_MW_lag148', 'quantity_MW_lag149', 'quantity_MW_lag150', 'quantity_MW_lag151', 'quantity_MW_lag152', 'quantity_MW_lag153', 'quantity_MW_lag154', 'quantity_MW_lag155', 'quantity_MW_lag156', 'quantity_MW_lag157', 'quantity_MW_lag158', 'quantity_MW_lag159', 'quantity_MW_lag160', 'quantity_MW_lag161', 'quantity_MW_lag162', 'quantity_MW_lag163', 'quantity_MW_lag164', 'quantity_MW_lag165', 'quantity_MW_lag166', 'quantity_MW_lag167', 'quantity_MW_lag168', 'quantity_MW_lag169', 'quantity_MW_lag170', 'quantity_MW_lag171', 'quantity_MW_lag172', 'quantity_MW_lag173', 'quantity_MW_lag174', 'quantity_MW_lag175', 'quantity_MW_lag176', 'quantity_MW_lag177', 'quantity_MW_lag178', 'quantity_MW_lag179', 'quantity_MW_lag180', 'quantity_MW_lag181', 'quantity_MW_lag182', 'quantity_MW_lag183', 'quantity_MW_lag184', 'quantity_MW_lag185', 'quantity_MW_lag186', 'quantity_MW_lag187', 'quantity_MW_lag188', 'quantity_MW_lag189', 'quantity_MW_lag190', 'quantity_MW_lag191', 'quantity_MW_lag192', 'quantity_MW_lag193', 'quantity_MW_lag194', 'quantity_MW_lag195', 'quantity_MW_lag196', 'quantity_MW_lag197', 'quantity_MW_lag198', 'quantity_MW_lag199', 'quantity_MW_lag200', 'quantity_MW_lag201', 'quantity_MW_lag202', 'quantity_MW_lag203', 'quantity_MW_lag204', 'quantity_MW_lag205', 'quantity_MW_lag206', 'quantity_MW_lag207', 'quantity_MW_lag208', 'quantity_MW_lag209', 'quantity_MW_lag210', 'quantity_MW_lag211', 'quantity_MW_lag212', 'quantity_MW_lag213', 'quantity_MW_lag214', 'quantity_MW_lag215', 'quantity_MW_lag216', 'quantity_MW_lag217', 'quantity_MW_lag218', 'quantity_MW_lag219', 'quantity_MW_lag220', 'quantity_MW_lag221', 'quantity_MW_lag222', 'quantity_MW_lag223', 'quantity_MW_lag224', 'quantity_MW_lag225', 'quantity_MW_lag226', 'quantity_MW_lag227', 'quantity_MW_lag228', 'quantity_MW_lag229', 'quantity_MW_lag230', 'quantity_MW_lag231', 'quantity_MW_lag232', 'quantity_MW_lag233', 'quantity_MW_lag234', 'quantity_MW_lag235', 'quantity_MW_lag236', 'quantity_MW_lag237', 'quantity_MW_lag238', 'quantity_MW_lag239', 'quantity_MW_lag240']\n",
      "[INFO] Ignorando targets n√£o num√©ricos/bool em build_flat_matrix: ['quantity_MW_lead1', 'quantity_MW_lead2', 'quantity_MW_lead3', 'quantity_MW_lead4', 'quantity_MW_lead5', 'quantity_MW_lead6', 'quantity_MW_lead7', 'quantity_MW_lead8', 'quantity_MW_lead9', 'quantity_MW_lead10', 'quantity_MW_lead11', 'quantity_MW_lead12', 'quantity_MW_lead13', 'quantity_MW_lead14', 'quantity_MW_lead15', 'quantity_MW_lead16', 'quantity_MW_lead17', 'quantity_MW_lead18', 'quantity_MW_lead19', 'quantity_MW_lead20', 'quantity_MW_lead21', 'quantity_MW_lead22', 'quantity_MW_lead23', 'quantity_MW_lead24', 'quantity_MW_lead25', 'quantity_MW_lead26', 'quantity_MW_lead27', 'quantity_MW_lead28', 'quantity_MW_lead29', 'quantity_MW_lead30', 'quantity_MW_lead31', 'quantity_MW_lead32', 'quantity_MW_lead33', 'quantity_MW_lead34', 'quantity_MW_lead35', 'quantity_MW_lead36', 'quantity_MW_lead37', 'quantity_MW_lead38', 'quantity_MW_lead39', 'quantity_MW_lead40', 'quantity_MW_lead41', 'quantity_MW_lead42', 'quantity_MW_lead43', 'quantity_MW_lead44', 'quantity_MW_lead45', 'quantity_MW_lead46', 'quantity_MW_lead47', 'quantity_MW_lead48', 'quantity_MW_lead49', 'quantity_MW_lead50', 'quantity_MW_lead51', 'quantity_MW_lead52', 'quantity_MW_lead53', 'quantity_MW_lead54', 'quantity_MW_lead55', 'quantity_MW_lead56', 'quantity_MW_lead57', 'quantity_MW_lead58', 'quantity_MW_lead59', 'quantity_MW_lead60', 'quantity_MW_lead61', 'quantity_MW_lead62', 'quantity_MW_lead63', 'quantity_MW_lead64', 'quantity_MW_lead65', 'quantity_MW_lead66', 'quantity_MW_lead67', 'quantity_MW_lead68', 'quantity_MW_lead69', 'quantity_MW_lead70', 'quantity_MW_lead71', 'quantity_MW_lead72']\n",
      "[INFO] Ignorando features n√£o num√©ricas/bool em build_flat_matrix: ['quantity_MW_lag1', 'quantity_MW_lag2', 'quantity_MW_lag3', 'quantity_MW_lag4', 'quantity_MW_lag5', 'quantity_MW_lag6', 'quantity_MW_lag7', 'quantity_MW_lag8', 'quantity_MW_lag9', 'quantity_MW_lag10', 'quantity_MW_lag11', 'quantity_MW_lag12', 'quantity_MW_lag13', 'quantity_MW_lag14', 'quantity_MW_lag15', 'quantity_MW_lag16', 'quantity_MW_lag17', 'quantity_MW_lag18', 'quantity_MW_lag19', 'quantity_MW_lag20', 'quantity_MW_lag21', 'quantity_MW_lag22', 'quantity_MW_lag23', 'quantity_MW_lag24', 'quantity_MW_lag25', 'quantity_MW_lag26', 'quantity_MW_lag27', 'quantity_MW_lag28', 'quantity_MW_lag29', 'quantity_MW_lag30', 'quantity_MW_lag31', 'quantity_MW_lag32', 'quantity_MW_lag33', 'quantity_MW_lag34', 'quantity_MW_lag35', 'quantity_MW_lag36', 'quantity_MW_lag37', 'quantity_MW_lag38', 'quantity_MW_lag39', 'quantity_MW_lag40', 'quantity_MW_lag41', 'quantity_MW_lag42', 'quantity_MW_lag43', 'quantity_MW_lag44', 'quantity_MW_lag45', 'quantity_MW_lag46', 'quantity_MW_lag47', 'quantity_MW_lag48', 'quantity_MW_lag49', 'quantity_MW_lag50', 'quantity_MW_lag51', 'quantity_MW_lag52', 'quantity_MW_lag53', 'quantity_MW_lag54', 'quantity_MW_lag55', 'quantity_MW_lag56', 'quantity_MW_lag57', 'quantity_MW_lag58', 'quantity_MW_lag59', 'quantity_MW_lag60', 'quantity_MW_lag61', 'quantity_MW_lag62', 'quantity_MW_lag63', 'quantity_MW_lag64', 'quantity_MW_lag65', 'quantity_MW_lag66', 'quantity_MW_lag67', 'quantity_MW_lag68', 'quantity_MW_lag69', 'quantity_MW_lag70', 'quantity_MW_lag71', 'quantity_MW_lag72', 'quantity_MW_lag73', 'quantity_MW_lag74', 'quantity_MW_lag75', 'quantity_MW_lag76', 'quantity_MW_lag77', 'quantity_MW_lag78', 'quantity_MW_lag79', 'quantity_MW_lag80', 'quantity_MW_lag81', 'quantity_MW_lag82', 'quantity_MW_lag83', 'quantity_MW_lag84', 'quantity_MW_lag85', 'quantity_MW_lag86', 'quantity_MW_lag87', 'quantity_MW_lag88', 'quantity_MW_lag89', 'quantity_MW_lag90', 'quantity_MW_lag91', 'quantity_MW_lag92', 'quantity_MW_lag93', 'quantity_MW_lag94', 'quantity_MW_lag95', 'quantity_MW_lag96', 'quantity_MW_lag97', 'quantity_MW_lag98', 'quantity_MW_lag99', 'quantity_MW_lag100', 'quantity_MW_lag101', 'quantity_MW_lag102', 'quantity_MW_lag103', 'quantity_MW_lag104', 'quantity_MW_lag105', 'quantity_MW_lag106', 'quantity_MW_lag107', 'quantity_MW_lag108', 'quantity_MW_lag109', 'quantity_MW_lag110', 'quantity_MW_lag111', 'quantity_MW_lag112', 'quantity_MW_lag113', 'quantity_MW_lag114', 'quantity_MW_lag115', 'quantity_MW_lag116', 'quantity_MW_lag117', 'quantity_MW_lag118', 'quantity_MW_lag119', 'quantity_MW_lag120', 'quantity_MW_lag121', 'quantity_MW_lag122', 'quantity_MW_lag123', 'quantity_MW_lag124', 'quantity_MW_lag125', 'quantity_MW_lag126', 'quantity_MW_lag127', 'quantity_MW_lag128', 'quantity_MW_lag129', 'quantity_MW_lag130', 'quantity_MW_lag131', 'quantity_MW_lag132', 'quantity_MW_lag133', 'quantity_MW_lag134', 'quantity_MW_lag135', 'quantity_MW_lag136', 'quantity_MW_lag137', 'quantity_MW_lag138', 'quantity_MW_lag139', 'quantity_MW_lag140', 'quantity_MW_lag141', 'quantity_MW_lag142', 'quantity_MW_lag143', 'quantity_MW_lag144', 'quantity_MW_lag145', 'quantity_MW_lag146', 'quantity_MW_lag147', 'quantity_MW_lag148', 'quantity_MW_lag149', 'quantity_MW_lag150', 'quantity_MW_lag151', 'quantity_MW_lag152', 'quantity_MW_lag153', 'quantity_MW_lag154', 'quantity_MW_lag155', 'quantity_MW_lag156', 'quantity_MW_lag157', 'quantity_MW_lag158', 'quantity_MW_lag159', 'quantity_MW_lag160', 'quantity_MW_lag161', 'quantity_MW_lag162', 'quantity_MW_lag163', 'quantity_MW_lag164', 'quantity_MW_lag165', 'quantity_MW_lag166', 'quantity_MW_lag167', 'quantity_MW_lag168', 'quantity_MW_lag169', 'quantity_MW_lag170', 'quantity_MW_lag171', 'quantity_MW_lag172', 'quantity_MW_lag173', 'quantity_MW_lag174', 'quantity_MW_lag175', 'quantity_MW_lag176', 'quantity_MW_lag177', 'quantity_MW_lag178', 'quantity_MW_lag179', 'quantity_MW_lag180', 'quantity_MW_lag181', 'quantity_MW_lag182', 'quantity_MW_lag183', 'quantity_MW_lag184', 'quantity_MW_lag185', 'quantity_MW_lag186', 'quantity_MW_lag187', 'quantity_MW_lag188', 'quantity_MW_lag189', 'quantity_MW_lag190', 'quantity_MW_lag191', 'quantity_MW_lag192', 'quantity_MW_lag193', 'quantity_MW_lag194', 'quantity_MW_lag195', 'quantity_MW_lag196', 'quantity_MW_lag197', 'quantity_MW_lag198', 'quantity_MW_lag199', 'quantity_MW_lag200', 'quantity_MW_lag201', 'quantity_MW_lag202', 'quantity_MW_lag203', 'quantity_MW_lag204', 'quantity_MW_lag205', 'quantity_MW_lag206', 'quantity_MW_lag207', 'quantity_MW_lag208', 'quantity_MW_lag209', 'quantity_MW_lag210', 'quantity_MW_lag211', 'quantity_MW_lag212', 'quantity_MW_lag213', 'quantity_MW_lag214', 'quantity_MW_lag215', 'quantity_MW_lag216', 'quantity_MW_lag217', 'quantity_MW_lag218', 'quantity_MW_lag219', 'quantity_MW_lag220', 'quantity_MW_lag221', 'quantity_MW_lag222', 'quantity_MW_lag223', 'quantity_MW_lag224', 'quantity_MW_lag225', 'quantity_MW_lag226', 'quantity_MW_lag227', 'quantity_MW_lag228', 'quantity_MW_lag229', 'quantity_MW_lag230', 'quantity_MW_lag231', 'quantity_MW_lag232', 'quantity_MW_lag233', 'quantity_MW_lag234', 'quantity_MW_lag235', 'quantity_MW_lag236', 'quantity_MW_lag237', 'quantity_MW_lag238', 'quantity_MW_lag239', 'quantity_MW_lag240']\n",
      "[INFO] Ignorando targets n√£o num√©ricos/bool em build_flat_matrix: ['quantity_MW_lead1', 'quantity_MW_lead2', 'quantity_MW_lead3', 'quantity_MW_lead4', 'quantity_MW_lead5', 'quantity_MW_lead6', 'quantity_MW_lead7', 'quantity_MW_lead8', 'quantity_MW_lead9', 'quantity_MW_lead10', 'quantity_MW_lead11', 'quantity_MW_lead12', 'quantity_MW_lead13', 'quantity_MW_lead14', 'quantity_MW_lead15', 'quantity_MW_lead16', 'quantity_MW_lead17', 'quantity_MW_lead18', 'quantity_MW_lead19', 'quantity_MW_lead20', 'quantity_MW_lead21', 'quantity_MW_lead22', 'quantity_MW_lead23', 'quantity_MW_lead24', 'quantity_MW_lead25', 'quantity_MW_lead26', 'quantity_MW_lead27', 'quantity_MW_lead28', 'quantity_MW_lead29', 'quantity_MW_lead30', 'quantity_MW_lead31', 'quantity_MW_lead32', 'quantity_MW_lead33', 'quantity_MW_lead34', 'quantity_MW_lead35', 'quantity_MW_lead36', 'quantity_MW_lead37', 'quantity_MW_lead38', 'quantity_MW_lead39', 'quantity_MW_lead40', 'quantity_MW_lead41', 'quantity_MW_lead42', 'quantity_MW_lead43', 'quantity_MW_lead44', 'quantity_MW_lead45', 'quantity_MW_lead46', 'quantity_MW_lead47', 'quantity_MW_lead48', 'quantity_MW_lead49', 'quantity_MW_lead50', 'quantity_MW_lead51', 'quantity_MW_lead52', 'quantity_MW_lead53', 'quantity_MW_lead54', 'quantity_MW_lead55', 'quantity_MW_lead56', 'quantity_MW_lead57', 'quantity_MW_lead58', 'quantity_MW_lead59', 'quantity_MW_lead60', 'quantity_MW_lead61', 'quantity_MW_lead62', 'quantity_MW_lead63', 'quantity_MW_lead64', 'quantity_MW_lead65', 'quantity_MW_lead66', 'quantity_MW_lead67', 'quantity_MW_lead68', 'quantity_MW_lead69', 'quantity_MW_lead70', 'quantity_MW_lead71', 'quantity_MW_lead72']\n",
      "[‚ÑπÔ∏è] Reduzindo lags: usando 168 e mascarando 72 restantes com -999.0\n",
      "[INFO] Ignorando features n√£o num√©ricas/bool em build_flat_matrix: ['quantity_MW_lag1', 'quantity_MW_lag2', 'quantity_MW_lag3', 'quantity_MW_lag4', 'quantity_MW_lag5', 'quantity_MW_lag6', 'quantity_MW_lag7', 'quantity_MW_lag8', 'quantity_MW_lag9', 'quantity_MW_lag10', 'quantity_MW_lag11', 'quantity_MW_lag12', 'quantity_MW_lag13', 'quantity_MW_lag14', 'quantity_MW_lag15', 'quantity_MW_lag16', 'quantity_MW_lag17', 'quantity_MW_lag18', 'quantity_MW_lag19', 'quantity_MW_lag20', 'quantity_MW_lag21', 'quantity_MW_lag22', 'quantity_MW_lag23', 'quantity_MW_lag24', 'quantity_MW_lag25', 'quantity_MW_lag26', 'quantity_MW_lag27', 'quantity_MW_lag28', 'quantity_MW_lag29', 'quantity_MW_lag30', 'quantity_MW_lag31', 'quantity_MW_lag32', 'quantity_MW_lag33', 'quantity_MW_lag34', 'quantity_MW_lag35', 'quantity_MW_lag36', 'quantity_MW_lag37', 'quantity_MW_lag38', 'quantity_MW_lag39', 'quantity_MW_lag40', 'quantity_MW_lag41', 'quantity_MW_lag42', 'quantity_MW_lag43', 'quantity_MW_lag44', 'quantity_MW_lag45', 'quantity_MW_lag46', 'quantity_MW_lag47', 'quantity_MW_lag48', 'quantity_MW_lag49', 'quantity_MW_lag50', 'quantity_MW_lag51', 'quantity_MW_lag52', 'quantity_MW_lag53', 'quantity_MW_lag54', 'quantity_MW_lag55', 'quantity_MW_lag56', 'quantity_MW_lag57', 'quantity_MW_lag58', 'quantity_MW_lag59', 'quantity_MW_lag60', 'quantity_MW_lag61', 'quantity_MW_lag62', 'quantity_MW_lag63', 'quantity_MW_lag64', 'quantity_MW_lag65', 'quantity_MW_lag66', 'quantity_MW_lag67', 'quantity_MW_lag68', 'quantity_MW_lag69', 'quantity_MW_lag70', 'quantity_MW_lag71', 'quantity_MW_lag72', 'quantity_MW_lag73', 'quantity_MW_lag74', 'quantity_MW_lag75', 'quantity_MW_lag76', 'quantity_MW_lag77', 'quantity_MW_lag78', 'quantity_MW_lag79', 'quantity_MW_lag80', 'quantity_MW_lag81', 'quantity_MW_lag82', 'quantity_MW_lag83', 'quantity_MW_lag84', 'quantity_MW_lag85', 'quantity_MW_lag86', 'quantity_MW_lag87', 'quantity_MW_lag88', 'quantity_MW_lag89', 'quantity_MW_lag90', 'quantity_MW_lag91', 'quantity_MW_lag92', 'quantity_MW_lag93', 'quantity_MW_lag94', 'quantity_MW_lag95', 'quantity_MW_lag96', 'quantity_MW_lag97', 'quantity_MW_lag98', 'quantity_MW_lag99', 'quantity_MW_lag100', 'quantity_MW_lag101', 'quantity_MW_lag102', 'quantity_MW_lag103', 'quantity_MW_lag104', 'quantity_MW_lag105', 'quantity_MW_lag106', 'quantity_MW_lag107', 'quantity_MW_lag108', 'quantity_MW_lag109', 'quantity_MW_lag110', 'quantity_MW_lag111', 'quantity_MW_lag112', 'quantity_MW_lag113', 'quantity_MW_lag114', 'quantity_MW_lag115', 'quantity_MW_lag116', 'quantity_MW_lag117', 'quantity_MW_lag118', 'quantity_MW_lag119', 'quantity_MW_lag120', 'quantity_MW_lag121', 'quantity_MW_lag122', 'quantity_MW_lag123', 'quantity_MW_lag124', 'quantity_MW_lag125', 'quantity_MW_lag126', 'quantity_MW_lag127', 'quantity_MW_lag128', 'quantity_MW_lag129', 'quantity_MW_lag130', 'quantity_MW_lag131', 'quantity_MW_lag132', 'quantity_MW_lag133', 'quantity_MW_lag134', 'quantity_MW_lag135', 'quantity_MW_lag136', 'quantity_MW_lag137', 'quantity_MW_lag138', 'quantity_MW_lag139', 'quantity_MW_lag140', 'quantity_MW_lag141', 'quantity_MW_lag142', 'quantity_MW_lag143', 'quantity_MW_lag144', 'quantity_MW_lag145', 'quantity_MW_lag146', 'quantity_MW_lag147', 'quantity_MW_lag148', 'quantity_MW_lag149', 'quantity_MW_lag150', 'quantity_MW_lag151', 'quantity_MW_lag152', 'quantity_MW_lag153', 'quantity_MW_lag154', 'quantity_MW_lag155', 'quantity_MW_lag156', 'quantity_MW_lag157', 'quantity_MW_lag158', 'quantity_MW_lag159', 'quantity_MW_lag160', 'quantity_MW_lag161', 'quantity_MW_lag162', 'quantity_MW_lag163', 'quantity_MW_lag164', 'quantity_MW_lag165', 'quantity_MW_lag166', 'quantity_MW_lag167', 'quantity_MW_lag168', 'quantity_MW_lag169', 'quantity_MW_lag170', 'quantity_MW_lag171', 'quantity_MW_lag172', 'quantity_MW_lag173', 'quantity_MW_lag174', 'quantity_MW_lag175', 'quantity_MW_lag176', 'quantity_MW_lag177', 'quantity_MW_lag178', 'quantity_MW_lag179', 'quantity_MW_lag180', 'quantity_MW_lag181', 'quantity_MW_lag182', 'quantity_MW_lag183', 'quantity_MW_lag184', 'quantity_MW_lag185', 'quantity_MW_lag186', 'quantity_MW_lag187', 'quantity_MW_lag188', 'quantity_MW_lag189', 'quantity_MW_lag190', 'quantity_MW_lag191', 'quantity_MW_lag192', 'quantity_MW_lag193', 'quantity_MW_lag194', 'quantity_MW_lag195', 'quantity_MW_lag196', 'quantity_MW_lag197', 'quantity_MW_lag198', 'quantity_MW_lag199', 'quantity_MW_lag200', 'quantity_MW_lag201', 'quantity_MW_lag202', 'quantity_MW_lag203', 'quantity_MW_lag204', 'quantity_MW_lag205', 'quantity_MW_lag206', 'quantity_MW_lag207', 'quantity_MW_lag208', 'quantity_MW_lag209', 'quantity_MW_lag210', 'quantity_MW_lag211', 'quantity_MW_lag212', 'quantity_MW_lag213', 'quantity_MW_lag214', 'quantity_MW_lag215', 'quantity_MW_lag216', 'quantity_MW_lag217', 'quantity_MW_lag218', 'quantity_MW_lag219', 'quantity_MW_lag220', 'quantity_MW_lag221', 'quantity_MW_lag222', 'quantity_MW_lag223', 'quantity_MW_lag224', 'quantity_MW_lag225', 'quantity_MW_lag226', 'quantity_MW_lag227', 'quantity_MW_lag228', 'quantity_MW_lag229', 'quantity_MW_lag230', 'quantity_MW_lag231', 'quantity_MW_lag232', 'quantity_MW_lag233', 'quantity_MW_lag234', 'quantity_MW_lag235', 'quantity_MW_lag236', 'quantity_MW_lag237', 'quantity_MW_lag238', 'quantity_MW_lag239', 'quantity_MW_lag240']\n",
      "[INFO] Ignorando targets n√£o num√©ricos/bool em build_flat_matrix: ['quantity_MW_lead1', 'quantity_MW_lead2', 'quantity_MW_lead3', 'quantity_MW_lead4', 'quantity_MW_lead5', 'quantity_MW_lead6', 'quantity_MW_lead7', 'quantity_MW_lead8', 'quantity_MW_lead9', 'quantity_MW_lead10', 'quantity_MW_lead11', 'quantity_MW_lead12', 'quantity_MW_lead13', 'quantity_MW_lead14', 'quantity_MW_lead15', 'quantity_MW_lead16', 'quantity_MW_lead17', 'quantity_MW_lead18', 'quantity_MW_lead19', 'quantity_MW_lead20', 'quantity_MW_lead21', 'quantity_MW_lead22', 'quantity_MW_lead23', 'quantity_MW_lead24', 'quantity_MW_lead25', 'quantity_MW_lead26', 'quantity_MW_lead27', 'quantity_MW_lead28', 'quantity_MW_lead29', 'quantity_MW_lead30', 'quantity_MW_lead31', 'quantity_MW_lead32', 'quantity_MW_lead33', 'quantity_MW_lead34', 'quantity_MW_lead35', 'quantity_MW_lead36', 'quantity_MW_lead37', 'quantity_MW_lead38', 'quantity_MW_lead39', 'quantity_MW_lead40', 'quantity_MW_lead41', 'quantity_MW_lead42', 'quantity_MW_lead43', 'quantity_MW_lead44', 'quantity_MW_lead45', 'quantity_MW_lead46', 'quantity_MW_lead47', 'quantity_MW_lead48', 'quantity_MW_lead49', 'quantity_MW_lead50', 'quantity_MW_lead51', 'quantity_MW_lead52', 'quantity_MW_lead53', 'quantity_MW_lead54', 'quantity_MW_lead55', 'quantity_MW_lead56', 'quantity_MW_lead57', 'quantity_MW_lead58', 'quantity_MW_lead59', 'quantity_MW_lead60', 'quantity_MW_lead61', 'quantity_MW_lead62', 'quantity_MW_lead63', 'quantity_MW_lead64', 'quantity_MW_lead65', 'quantity_MW_lead66', 'quantity_MW_lead67', 'quantity_MW_lead68', 'quantity_MW_lead69', 'quantity_MW_lead70', 'quantity_MW_lead71', 'quantity_MW_lead72']\n",
      "[‚ÑπÔ∏è] Reduzindo lags: usando 168 e mascarando 72 restantes com -999.0\n",
      "üìê Normaliza√ß√£o ...\n",
      "üìê Normaliza√ß√£o ...\n",
      "[INFO] Ignorando features n√£o num√©ricas/bool em build_flat_matrix: ['quantity_MW_lag1', 'quantity_MW_lag2', 'quantity_MW_lag3', 'quantity_MW_lag4', 'quantity_MW_lag5', 'quantity_MW_lag6', 'quantity_MW_lag7', 'quantity_MW_lag8', 'quantity_MW_lag9', 'quantity_MW_lag10', 'quantity_MW_lag11', 'quantity_MW_lag12', 'quantity_MW_lag13', 'quantity_MW_lag14', 'quantity_MW_lag15', 'quantity_MW_lag16', 'quantity_MW_lag17', 'quantity_MW_lag18', 'quantity_MW_lag19', 'quantity_MW_lag20', 'quantity_MW_lag21', 'quantity_MW_lag22', 'quantity_MW_lag23', 'quantity_MW_lag24', 'quantity_MW_lag25', 'quantity_MW_lag26', 'quantity_MW_lag27', 'quantity_MW_lag28', 'quantity_MW_lag29', 'quantity_MW_lag30', 'quantity_MW_lag31', 'quantity_MW_lag32', 'quantity_MW_lag33', 'quantity_MW_lag34', 'quantity_MW_lag35', 'quantity_MW_lag36', 'quantity_MW_lag37', 'quantity_MW_lag38', 'quantity_MW_lag39', 'quantity_MW_lag40', 'quantity_MW_lag41', 'quantity_MW_lag42', 'quantity_MW_lag43', 'quantity_MW_lag44', 'quantity_MW_lag45', 'quantity_MW_lag46', 'quantity_MW_lag47', 'quantity_MW_lag48', 'quantity_MW_lag49', 'quantity_MW_lag50', 'quantity_MW_lag51', 'quantity_MW_lag52', 'quantity_MW_lag53', 'quantity_MW_lag54', 'quantity_MW_lag55', 'quantity_MW_lag56', 'quantity_MW_lag57', 'quantity_MW_lag58', 'quantity_MW_lag59', 'quantity_MW_lag60', 'quantity_MW_lag61', 'quantity_MW_lag62', 'quantity_MW_lag63', 'quantity_MW_lag64', 'quantity_MW_lag65', 'quantity_MW_lag66', 'quantity_MW_lag67', 'quantity_MW_lag68', 'quantity_MW_lag69', 'quantity_MW_lag70', 'quantity_MW_lag71', 'quantity_MW_lag72', 'quantity_MW_lag73', 'quantity_MW_lag74', 'quantity_MW_lag75', 'quantity_MW_lag76', 'quantity_MW_lag77', 'quantity_MW_lag78', 'quantity_MW_lag79', 'quantity_MW_lag80', 'quantity_MW_lag81', 'quantity_MW_lag82', 'quantity_MW_lag83', 'quantity_MW_lag84', 'quantity_MW_lag85', 'quantity_MW_lag86', 'quantity_MW_lag87', 'quantity_MW_lag88', 'quantity_MW_lag89', 'quantity_MW_lag90', 'quantity_MW_lag91', 'quantity_MW_lag92', 'quantity_MW_lag93', 'quantity_MW_lag94', 'quantity_MW_lag95', 'quantity_MW_lag96', 'quantity_MW_lag97', 'quantity_MW_lag98', 'quantity_MW_lag99', 'quantity_MW_lag100', 'quantity_MW_lag101', 'quantity_MW_lag102', 'quantity_MW_lag103', 'quantity_MW_lag104', 'quantity_MW_lag105', 'quantity_MW_lag106', 'quantity_MW_lag107', 'quantity_MW_lag108', 'quantity_MW_lag109', 'quantity_MW_lag110', 'quantity_MW_lag111', 'quantity_MW_lag112', 'quantity_MW_lag113', 'quantity_MW_lag114', 'quantity_MW_lag115', 'quantity_MW_lag116', 'quantity_MW_lag117', 'quantity_MW_lag118', 'quantity_MW_lag119', 'quantity_MW_lag120', 'quantity_MW_lag121', 'quantity_MW_lag122', 'quantity_MW_lag123', 'quantity_MW_lag124', 'quantity_MW_lag125', 'quantity_MW_lag126', 'quantity_MW_lag127', 'quantity_MW_lag128', 'quantity_MW_lag129', 'quantity_MW_lag130', 'quantity_MW_lag131', 'quantity_MW_lag132', 'quantity_MW_lag133', 'quantity_MW_lag134', 'quantity_MW_lag135', 'quantity_MW_lag136', 'quantity_MW_lag137', 'quantity_MW_lag138', 'quantity_MW_lag139', 'quantity_MW_lag140', 'quantity_MW_lag141', 'quantity_MW_lag142', 'quantity_MW_lag143', 'quantity_MW_lag144', 'quantity_MW_lag145', 'quantity_MW_lag146', 'quantity_MW_lag147', 'quantity_MW_lag148', 'quantity_MW_lag149', 'quantity_MW_lag150', 'quantity_MW_lag151', 'quantity_MW_lag152', 'quantity_MW_lag153', 'quantity_MW_lag154', 'quantity_MW_lag155', 'quantity_MW_lag156', 'quantity_MW_lag157', 'quantity_MW_lag158', 'quantity_MW_lag159', 'quantity_MW_lag160', 'quantity_MW_lag161', 'quantity_MW_lag162', 'quantity_MW_lag163', 'quantity_MW_lag164', 'quantity_MW_lag165', 'quantity_MW_lag166', 'quantity_MW_lag167', 'quantity_MW_lag168', 'quantity_MW_lag169', 'quantity_MW_lag170', 'quantity_MW_lag171', 'quantity_MW_lag172', 'quantity_MW_lag173', 'quantity_MW_lag174', 'quantity_MW_lag175', 'quantity_MW_lag176', 'quantity_MW_lag177', 'quantity_MW_lag178', 'quantity_MW_lag179', 'quantity_MW_lag180', 'quantity_MW_lag181', 'quantity_MW_lag182', 'quantity_MW_lag183', 'quantity_MW_lag184', 'quantity_MW_lag185', 'quantity_MW_lag186', 'quantity_MW_lag187', 'quantity_MW_lag188', 'quantity_MW_lag189', 'quantity_MW_lag190', 'quantity_MW_lag191', 'quantity_MW_lag192', 'quantity_MW_lag193', 'quantity_MW_lag194', 'quantity_MW_lag195', 'quantity_MW_lag196', 'quantity_MW_lag197', 'quantity_MW_lag198', 'quantity_MW_lag199', 'quantity_MW_lag200', 'quantity_MW_lag201', 'quantity_MW_lag202', 'quantity_MW_lag203', 'quantity_MW_lag204', 'quantity_MW_lag205', 'quantity_MW_lag206', 'quantity_MW_lag207', 'quantity_MW_lag208', 'quantity_MW_lag209', 'quantity_MW_lag210', 'quantity_MW_lag211', 'quantity_MW_lag212', 'quantity_MW_lag213', 'quantity_MW_lag214', 'quantity_MW_lag215', 'quantity_MW_lag216', 'quantity_MW_lag217', 'quantity_MW_lag218', 'quantity_MW_lag219', 'quantity_MW_lag220', 'quantity_MW_lag221', 'quantity_MW_lag222', 'quantity_MW_lag223', 'quantity_MW_lag224', 'quantity_MW_lag225', 'quantity_MW_lag226', 'quantity_MW_lag227', 'quantity_MW_lag228', 'quantity_MW_lag229', 'quantity_MW_lag230', 'quantity_MW_lag231', 'quantity_MW_lag232', 'quantity_MW_lag233', 'quantity_MW_lag234', 'quantity_MW_lag235', 'quantity_MW_lag236', 'quantity_MW_lag237', 'quantity_MW_lag238', 'quantity_MW_lag239', 'quantity_MW_lag240']\n",
      "[INFO] Ignorando targets n√£o num√©ricos/bool em build_flat_matrix: ['quantity_MW_lead1', 'quantity_MW_lead2', 'quantity_MW_lead3', 'quantity_MW_lead4', 'quantity_MW_lead5', 'quantity_MW_lead6', 'quantity_MW_lead7', 'quantity_MW_lead8', 'quantity_MW_lead9', 'quantity_MW_lead10', 'quantity_MW_lead11', 'quantity_MW_lead12', 'quantity_MW_lead13', 'quantity_MW_lead14', 'quantity_MW_lead15', 'quantity_MW_lead16', 'quantity_MW_lead17', 'quantity_MW_lead18', 'quantity_MW_lead19', 'quantity_MW_lead20', 'quantity_MW_lead21', 'quantity_MW_lead22', 'quantity_MW_lead23', 'quantity_MW_lead24', 'quantity_MW_lead25', 'quantity_MW_lead26', 'quantity_MW_lead27', 'quantity_MW_lead28', 'quantity_MW_lead29', 'quantity_MW_lead30', 'quantity_MW_lead31', 'quantity_MW_lead32', 'quantity_MW_lead33', 'quantity_MW_lead34', 'quantity_MW_lead35', 'quantity_MW_lead36', 'quantity_MW_lead37', 'quantity_MW_lead38', 'quantity_MW_lead39', 'quantity_MW_lead40', 'quantity_MW_lead41', 'quantity_MW_lead42', 'quantity_MW_lead43', 'quantity_MW_lead44', 'quantity_MW_lead45', 'quantity_MW_lead46', 'quantity_MW_lead47', 'quantity_MW_lead48', 'quantity_MW_lead49', 'quantity_MW_lead50', 'quantity_MW_lead51', 'quantity_MW_lead52', 'quantity_MW_lead53', 'quantity_MW_lead54', 'quantity_MW_lead55', 'quantity_MW_lead56', 'quantity_MW_lead57', 'quantity_MW_lead58', 'quantity_MW_lead59', 'quantity_MW_lead60', 'quantity_MW_lead61', 'quantity_MW_lead62', 'quantity_MW_lead63', 'quantity_MW_lead64', 'quantity_MW_lead65', 'quantity_MW_lead66', 'quantity_MW_lead67', 'quantity_MW_lead68', 'quantity_MW_lead69', 'quantity_MW_lead70', 'quantity_MW_lead71', 'quantity_MW_lead72']\n",
      "[INFO] Ignorando features n√£o num√©ricas/bool em build_flat_matrix: ['quantity_MW_lag1', 'quantity_MW_lag2', 'quantity_MW_lag3', 'quantity_MW_lag4', 'quantity_MW_lag5', 'quantity_MW_lag6', 'quantity_MW_lag7', 'quantity_MW_lag8', 'quantity_MW_lag9', 'quantity_MW_lag10', 'quantity_MW_lag11', 'quantity_MW_lag12', 'quantity_MW_lag13', 'quantity_MW_lag14', 'quantity_MW_lag15', 'quantity_MW_lag16', 'quantity_MW_lag17', 'quantity_MW_lag18', 'quantity_MW_lag19', 'quantity_MW_lag20', 'quantity_MW_lag21', 'quantity_MW_lag22', 'quantity_MW_lag23', 'quantity_MW_lag24', 'quantity_MW_lag25', 'quantity_MW_lag26', 'quantity_MW_lag27', 'quantity_MW_lag28', 'quantity_MW_lag29', 'quantity_MW_lag30', 'quantity_MW_lag31', 'quantity_MW_lag32', 'quantity_MW_lag33', 'quantity_MW_lag34', 'quantity_MW_lag35', 'quantity_MW_lag36', 'quantity_MW_lag37', 'quantity_MW_lag38', 'quantity_MW_lag39', 'quantity_MW_lag40', 'quantity_MW_lag41', 'quantity_MW_lag42', 'quantity_MW_lag43', 'quantity_MW_lag44', 'quantity_MW_lag45', 'quantity_MW_lag46', 'quantity_MW_lag47', 'quantity_MW_lag48', 'quantity_MW_lag49', 'quantity_MW_lag50', 'quantity_MW_lag51', 'quantity_MW_lag52', 'quantity_MW_lag53', 'quantity_MW_lag54', 'quantity_MW_lag55', 'quantity_MW_lag56', 'quantity_MW_lag57', 'quantity_MW_lag58', 'quantity_MW_lag59', 'quantity_MW_lag60', 'quantity_MW_lag61', 'quantity_MW_lag62', 'quantity_MW_lag63', 'quantity_MW_lag64', 'quantity_MW_lag65', 'quantity_MW_lag66', 'quantity_MW_lag67', 'quantity_MW_lag68', 'quantity_MW_lag69', 'quantity_MW_lag70', 'quantity_MW_lag71', 'quantity_MW_lag72', 'quantity_MW_lag73', 'quantity_MW_lag74', 'quantity_MW_lag75', 'quantity_MW_lag76', 'quantity_MW_lag77', 'quantity_MW_lag78', 'quantity_MW_lag79', 'quantity_MW_lag80', 'quantity_MW_lag81', 'quantity_MW_lag82', 'quantity_MW_lag83', 'quantity_MW_lag84', 'quantity_MW_lag85', 'quantity_MW_lag86', 'quantity_MW_lag87', 'quantity_MW_lag88', 'quantity_MW_lag89', 'quantity_MW_lag90', 'quantity_MW_lag91', 'quantity_MW_lag92', 'quantity_MW_lag93', 'quantity_MW_lag94', 'quantity_MW_lag95', 'quantity_MW_lag96', 'quantity_MW_lag97', 'quantity_MW_lag98', 'quantity_MW_lag99', 'quantity_MW_lag100', 'quantity_MW_lag101', 'quantity_MW_lag102', 'quantity_MW_lag103', 'quantity_MW_lag104', 'quantity_MW_lag105', 'quantity_MW_lag106', 'quantity_MW_lag107', 'quantity_MW_lag108', 'quantity_MW_lag109', 'quantity_MW_lag110', 'quantity_MW_lag111', 'quantity_MW_lag112', 'quantity_MW_lag113', 'quantity_MW_lag114', 'quantity_MW_lag115', 'quantity_MW_lag116', 'quantity_MW_lag117', 'quantity_MW_lag118', 'quantity_MW_lag119', 'quantity_MW_lag120', 'quantity_MW_lag121', 'quantity_MW_lag122', 'quantity_MW_lag123', 'quantity_MW_lag124', 'quantity_MW_lag125', 'quantity_MW_lag126', 'quantity_MW_lag127', 'quantity_MW_lag128', 'quantity_MW_lag129', 'quantity_MW_lag130', 'quantity_MW_lag131', 'quantity_MW_lag132', 'quantity_MW_lag133', 'quantity_MW_lag134', 'quantity_MW_lag135', 'quantity_MW_lag136', 'quantity_MW_lag137', 'quantity_MW_lag138', 'quantity_MW_lag139', 'quantity_MW_lag140', 'quantity_MW_lag141', 'quantity_MW_lag142', 'quantity_MW_lag143', 'quantity_MW_lag144', 'quantity_MW_lag145', 'quantity_MW_lag146', 'quantity_MW_lag147', 'quantity_MW_lag148', 'quantity_MW_lag149', 'quantity_MW_lag150', 'quantity_MW_lag151', 'quantity_MW_lag152', 'quantity_MW_lag153', 'quantity_MW_lag154', 'quantity_MW_lag155', 'quantity_MW_lag156', 'quantity_MW_lag157', 'quantity_MW_lag158', 'quantity_MW_lag159', 'quantity_MW_lag160', 'quantity_MW_lag161', 'quantity_MW_lag162', 'quantity_MW_lag163', 'quantity_MW_lag164', 'quantity_MW_lag165', 'quantity_MW_lag166', 'quantity_MW_lag167', 'quantity_MW_lag168', 'quantity_MW_lag169', 'quantity_MW_lag170', 'quantity_MW_lag171', 'quantity_MW_lag172', 'quantity_MW_lag173', 'quantity_MW_lag174', 'quantity_MW_lag175', 'quantity_MW_lag176', 'quantity_MW_lag177', 'quantity_MW_lag178', 'quantity_MW_lag179', 'quantity_MW_lag180', 'quantity_MW_lag181', 'quantity_MW_lag182', 'quantity_MW_lag183', 'quantity_MW_lag184', 'quantity_MW_lag185', 'quantity_MW_lag186', 'quantity_MW_lag187', 'quantity_MW_lag188', 'quantity_MW_lag189', 'quantity_MW_lag190', 'quantity_MW_lag191', 'quantity_MW_lag192', 'quantity_MW_lag193', 'quantity_MW_lag194', 'quantity_MW_lag195', 'quantity_MW_lag196', 'quantity_MW_lag197', 'quantity_MW_lag198', 'quantity_MW_lag199', 'quantity_MW_lag200', 'quantity_MW_lag201', 'quantity_MW_lag202', 'quantity_MW_lag203', 'quantity_MW_lag204', 'quantity_MW_lag205', 'quantity_MW_lag206', 'quantity_MW_lag207', 'quantity_MW_lag208', 'quantity_MW_lag209', 'quantity_MW_lag210', 'quantity_MW_lag211', 'quantity_MW_lag212', 'quantity_MW_lag213', 'quantity_MW_lag214', 'quantity_MW_lag215', 'quantity_MW_lag216', 'quantity_MW_lag217', 'quantity_MW_lag218', 'quantity_MW_lag219', 'quantity_MW_lag220', 'quantity_MW_lag221', 'quantity_MW_lag222', 'quantity_MW_lag223', 'quantity_MW_lag224', 'quantity_MW_lag225', 'quantity_MW_lag226', 'quantity_MW_lag227', 'quantity_MW_lag228', 'quantity_MW_lag229', 'quantity_MW_lag230', 'quantity_MW_lag231', 'quantity_MW_lag232', 'quantity_MW_lag233', 'quantity_MW_lag234', 'quantity_MW_lag235', 'quantity_MW_lag236', 'quantity_MW_lag237', 'quantity_MW_lag238', 'quantity_MW_lag239', 'quantity_MW_lag240']\n",
      "[INFO] Ignorando targets n√£o num√©ricos/bool em build_flat_matrix: ['quantity_MW_lead1', 'quantity_MW_lead2', 'quantity_MW_lead3', 'quantity_MW_lead4', 'quantity_MW_lead5', 'quantity_MW_lead6', 'quantity_MW_lead7', 'quantity_MW_lead8', 'quantity_MW_lead9', 'quantity_MW_lead10', 'quantity_MW_lead11', 'quantity_MW_lead12', 'quantity_MW_lead13', 'quantity_MW_lead14', 'quantity_MW_lead15', 'quantity_MW_lead16', 'quantity_MW_lead17', 'quantity_MW_lead18', 'quantity_MW_lead19', 'quantity_MW_lead20', 'quantity_MW_lead21', 'quantity_MW_lead22', 'quantity_MW_lead23', 'quantity_MW_lead24', 'quantity_MW_lead25', 'quantity_MW_lead26', 'quantity_MW_lead27', 'quantity_MW_lead28', 'quantity_MW_lead29', 'quantity_MW_lead30', 'quantity_MW_lead31', 'quantity_MW_lead32', 'quantity_MW_lead33', 'quantity_MW_lead34', 'quantity_MW_lead35', 'quantity_MW_lead36', 'quantity_MW_lead37', 'quantity_MW_lead38', 'quantity_MW_lead39', 'quantity_MW_lead40', 'quantity_MW_lead41', 'quantity_MW_lead42', 'quantity_MW_lead43', 'quantity_MW_lead44', 'quantity_MW_lead45', 'quantity_MW_lead46', 'quantity_MW_lead47', 'quantity_MW_lead48', 'quantity_MW_lead49', 'quantity_MW_lead50', 'quantity_MW_lead51', 'quantity_MW_lead52', 'quantity_MW_lead53', 'quantity_MW_lead54', 'quantity_MW_lead55', 'quantity_MW_lead56', 'quantity_MW_lead57', 'quantity_MW_lead58', 'quantity_MW_lead59', 'quantity_MW_lead60', 'quantity_MW_lead61', 'quantity_MW_lead62', 'quantity_MW_lead63', 'quantity_MW_lead64', 'quantity_MW_lead65', 'quantity_MW_lead66', 'quantity_MW_lead67', 'quantity_MW_lead68', 'quantity_MW_lead69', 'quantity_MW_lead70', 'quantity_MW_lead71', 'quantity_MW_lead72']\n",
      "[INFO] Ignorando features n√£o num√©ricas/bool em build_flat_matrix: ['quantity_MW_lag1', 'quantity_MW_lag2', 'quantity_MW_lag3', 'quantity_MW_lag4', 'quantity_MW_lag5', 'quantity_MW_lag6', 'quantity_MW_lag7', 'quantity_MW_lag8', 'quantity_MW_lag9', 'quantity_MW_lag10', 'quantity_MW_lag11', 'quantity_MW_lag12', 'quantity_MW_lag13', 'quantity_MW_lag14', 'quantity_MW_lag15', 'quantity_MW_lag16', 'quantity_MW_lag17', 'quantity_MW_lag18', 'quantity_MW_lag19', 'quantity_MW_lag20', 'quantity_MW_lag21', 'quantity_MW_lag22', 'quantity_MW_lag23', 'quantity_MW_lag24', 'quantity_MW_lag25', 'quantity_MW_lag26', 'quantity_MW_lag27', 'quantity_MW_lag28', 'quantity_MW_lag29', 'quantity_MW_lag30', 'quantity_MW_lag31', 'quantity_MW_lag32', 'quantity_MW_lag33', 'quantity_MW_lag34', 'quantity_MW_lag35', 'quantity_MW_lag36', 'quantity_MW_lag37', 'quantity_MW_lag38', 'quantity_MW_lag39', 'quantity_MW_lag40', 'quantity_MW_lag41', 'quantity_MW_lag42', 'quantity_MW_lag43', 'quantity_MW_lag44', 'quantity_MW_lag45', 'quantity_MW_lag46', 'quantity_MW_lag47', 'quantity_MW_lag48', 'quantity_MW_lag49', 'quantity_MW_lag50', 'quantity_MW_lag51', 'quantity_MW_lag52', 'quantity_MW_lag53', 'quantity_MW_lag54', 'quantity_MW_lag55', 'quantity_MW_lag56', 'quantity_MW_lag57', 'quantity_MW_lag58', 'quantity_MW_lag59', 'quantity_MW_lag60', 'quantity_MW_lag61', 'quantity_MW_lag62', 'quantity_MW_lag63', 'quantity_MW_lag64', 'quantity_MW_lag65', 'quantity_MW_lag66', 'quantity_MW_lag67', 'quantity_MW_lag68', 'quantity_MW_lag69', 'quantity_MW_lag70', 'quantity_MW_lag71', 'quantity_MW_lag72', 'quantity_MW_lag73', 'quantity_MW_lag74', 'quantity_MW_lag75', 'quantity_MW_lag76', 'quantity_MW_lag77', 'quantity_MW_lag78', 'quantity_MW_lag79', 'quantity_MW_lag80', 'quantity_MW_lag81', 'quantity_MW_lag82', 'quantity_MW_lag83', 'quantity_MW_lag84', 'quantity_MW_lag85', 'quantity_MW_lag86', 'quantity_MW_lag87', 'quantity_MW_lag88', 'quantity_MW_lag89', 'quantity_MW_lag90', 'quantity_MW_lag91', 'quantity_MW_lag92', 'quantity_MW_lag93', 'quantity_MW_lag94', 'quantity_MW_lag95', 'quantity_MW_lag96', 'quantity_MW_lag97', 'quantity_MW_lag98', 'quantity_MW_lag99', 'quantity_MW_lag100', 'quantity_MW_lag101', 'quantity_MW_lag102', 'quantity_MW_lag103', 'quantity_MW_lag104', 'quantity_MW_lag105', 'quantity_MW_lag106', 'quantity_MW_lag107', 'quantity_MW_lag108', 'quantity_MW_lag109', 'quantity_MW_lag110', 'quantity_MW_lag111', 'quantity_MW_lag112', 'quantity_MW_lag113', 'quantity_MW_lag114', 'quantity_MW_lag115', 'quantity_MW_lag116', 'quantity_MW_lag117', 'quantity_MW_lag118', 'quantity_MW_lag119', 'quantity_MW_lag120', 'quantity_MW_lag121', 'quantity_MW_lag122', 'quantity_MW_lag123', 'quantity_MW_lag124', 'quantity_MW_lag125', 'quantity_MW_lag126', 'quantity_MW_lag127', 'quantity_MW_lag128', 'quantity_MW_lag129', 'quantity_MW_lag130', 'quantity_MW_lag131', 'quantity_MW_lag132', 'quantity_MW_lag133', 'quantity_MW_lag134', 'quantity_MW_lag135', 'quantity_MW_lag136', 'quantity_MW_lag137', 'quantity_MW_lag138', 'quantity_MW_lag139', 'quantity_MW_lag140', 'quantity_MW_lag141', 'quantity_MW_lag142', 'quantity_MW_lag143', 'quantity_MW_lag144', 'quantity_MW_lag145', 'quantity_MW_lag146', 'quantity_MW_lag147', 'quantity_MW_lag148', 'quantity_MW_lag149', 'quantity_MW_lag150', 'quantity_MW_lag151', 'quantity_MW_lag152', 'quantity_MW_lag153', 'quantity_MW_lag154', 'quantity_MW_lag155', 'quantity_MW_lag156', 'quantity_MW_lag157', 'quantity_MW_lag158', 'quantity_MW_lag159', 'quantity_MW_lag160', 'quantity_MW_lag161', 'quantity_MW_lag162', 'quantity_MW_lag163', 'quantity_MW_lag164', 'quantity_MW_lag165', 'quantity_MW_lag166', 'quantity_MW_lag167', 'quantity_MW_lag168', 'quantity_MW_lag169', 'quantity_MW_lag170', 'quantity_MW_lag171', 'quantity_MW_lag172', 'quantity_MW_lag173', 'quantity_MW_lag174', 'quantity_MW_lag175', 'quantity_MW_lag176', 'quantity_MW_lag177', 'quantity_MW_lag178', 'quantity_MW_lag179', 'quantity_MW_lag180', 'quantity_MW_lag181', 'quantity_MW_lag182', 'quantity_MW_lag183', 'quantity_MW_lag184', 'quantity_MW_lag185', 'quantity_MW_lag186', 'quantity_MW_lag187', 'quantity_MW_lag188', 'quantity_MW_lag189', 'quantity_MW_lag190', 'quantity_MW_lag191', 'quantity_MW_lag192', 'quantity_MW_lag193', 'quantity_MW_lag194', 'quantity_MW_lag195', 'quantity_MW_lag196', 'quantity_MW_lag197', 'quantity_MW_lag198', 'quantity_MW_lag199', 'quantity_MW_lag200', 'quantity_MW_lag201', 'quantity_MW_lag202', 'quantity_MW_lag203', 'quantity_MW_lag204', 'quantity_MW_lag205', 'quantity_MW_lag206', 'quantity_MW_lag207', 'quantity_MW_lag208', 'quantity_MW_lag209', 'quantity_MW_lag210', 'quantity_MW_lag211', 'quantity_MW_lag212', 'quantity_MW_lag213', 'quantity_MW_lag214', 'quantity_MW_lag215', 'quantity_MW_lag216', 'quantity_MW_lag217', 'quantity_MW_lag218', 'quantity_MW_lag219', 'quantity_MW_lag220', 'quantity_MW_lag221', 'quantity_MW_lag222', 'quantity_MW_lag223', 'quantity_MW_lag224', 'quantity_MW_lag225', 'quantity_MW_lag226', 'quantity_MW_lag227', 'quantity_MW_lag228', 'quantity_MW_lag229', 'quantity_MW_lag230', 'quantity_MW_lag231', 'quantity_MW_lag232', 'quantity_MW_lag233', 'quantity_MW_lag234', 'quantity_MW_lag235', 'quantity_MW_lag236', 'quantity_MW_lag237', 'quantity_MW_lag238', 'quantity_MW_lag239', 'quantity_MW_lag240']\n",
      "[INFO] Ignorando targets n√£o num√©ricos/bool em build_flat_matrix: ['quantity_MW_lead1', 'quantity_MW_lead2', 'quantity_MW_lead3', 'quantity_MW_lead4', 'quantity_MW_lead5', 'quantity_MW_lead6', 'quantity_MW_lead7', 'quantity_MW_lead8', 'quantity_MW_lead9', 'quantity_MW_lead10', 'quantity_MW_lead11', 'quantity_MW_lead12', 'quantity_MW_lead13', 'quantity_MW_lead14', 'quantity_MW_lead15', 'quantity_MW_lead16', 'quantity_MW_lead17', 'quantity_MW_lead18', 'quantity_MW_lead19', 'quantity_MW_lead20', 'quantity_MW_lead21', 'quantity_MW_lead22', 'quantity_MW_lead23', 'quantity_MW_lead24', 'quantity_MW_lead25', 'quantity_MW_lead26', 'quantity_MW_lead27', 'quantity_MW_lead28', 'quantity_MW_lead29', 'quantity_MW_lead30', 'quantity_MW_lead31', 'quantity_MW_lead32', 'quantity_MW_lead33', 'quantity_MW_lead34', 'quantity_MW_lead35', 'quantity_MW_lead36', 'quantity_MW_lead37', 'quantity_MW_lead38', 'quantity_MW_lead39', 'quantity_MW_lead40', 'quantity_MW_lead41', 'quantity_MW_lead42', 'quantity_MW_lead43', 'quantity_MW_lead44', 'quantity_MW_lead45', 'quantity_MW_lead46', 'quantity_MW_lead47', 'quantity_MW_lead48', 'quantity_MW_lead49', 'quantity_MW_lead50', 'quantity_MW_lead51', 'quantity_MW_lead52', 'quantity_MW_lead53', 'quantity_MW_lead54', 'quantity_MW_lead55', 'quantity_MW_lead56', 'quantity_MW_lead57', 'quantity_MW_lead58', 'quantity_MW_lead59', 'quantity_MW_lead60', 'quantity_MW_lead61', 'quantity_MW_lead62', 'quantity_MW_lead63', 'quantity_MW_lead64', 'quantity_MW_lead65', 'quantity_MW_lead66', 'quantity_MW_lead67', 'quantity_MW_lead68', 'quantity_MW_lead69', 'quantity_MW_lead70', 'quantity_MW_lead71', 'quantity_MW_lead72']\n",
      "[‚ÑπÔ∏è] Reduzindo lags: usando 72 e mascarando 168 restantes com -999.0\n",
      "üìê Normaliza√ß√£o ...\n",
      "üíæ Salvando parquets LINEAR/MLP ...\n",
      "üíæ Salvando parquets LINEAR/MLP ...\n",
      "üìê Normaliza√ß√£o ...\n",
      "[Linear:train] linhas=15,830  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[INFO] Ignorando features n√£o num√©ricas/bool em build_flat_matrix: ['quantity_MW_lag1', 'quantity_MW_lag2', 'quantity_MW_lag3', 'quantity_MW_lag4', 'quantity_MW_lag5', 'quantity_MW_lag6', 'quantity_MW_lag7', 'quantity_MW_lag8', 'quantity_MW_lag9', 'quantity_MW_lag10', 'quantity_MW_lag11', 'quantity_MW_lag12', 'quantity_MW_lag13', 'quantity_MW_lag14', 'quantity_MW_lag15', 'quantity_MW_lag16', 'quantity_MW_lag17', 'quantity_MW_lag18', 'quantity_MW_lag19', 'quantity_MW_lag20', 'quantity_MW_lag21', 'quantity_MW_lag22', 'quantity_MW_lag23', 'quantity_MW_lag24', 'quantity_MW_lag25', 'quantity_MW_lag26', 'quantity_MW_lag27', 'quantity_MW_lag28', 'quantity_MW_lag29', 'quantity_MW_lag30', 'quantity_MW_lag31', 'quantity_MW_lag32', 'quantity_MW_lag33', 'quantity_MW_lag34', 'quantity_MW_lag35', 'quantity_MW_lag36', 'quantity_MW_lag37', 'quantity_MW_lag38', 'quantity_MW_lag39', 'quantity_MW_lag40', 'quantity_MW_lag41', 'quantity_MW_lag42', 'quantity_MW_lag43', 'quantity_MW_lag44', 'quantity_MW_lag45', 'quantity_MW_lag46', 'quantity_MW_lag47', 'quantity_MW_lag48', 'quantity_MW_lag49', 'quantity_MW_lag50', 'quantity_MW_lag51', 'quantity_MW_lag52', 'quantity_MW_lag53', 'quantity_MW_lag54', 'quantity_MW_lag55', 'quantity_MW_lag56', 'quantity_MW_lag57', 'quantity_MW_lag58', 'quantity_MW_lag59', 'quantity_MW_lag60', 'quantity_MW_lag61', 'quantity_MW_lag62', 'quantity_MW_lag63', 'quantity_MW_lag64', 'quantity_MW_lag65', 'quantity_MW_lag66', 'quantity_MW_lag67', 'quantity_MW_lag68', 'quantity_MW_lag69', 'quantity_MW_lag70', 'quantity_MW_lag71', 'quantity_MW_lag72', 'quantity_MW_lag73', 'quantity_MW_lag74', 'quantity_MW_lag75', 'quantity_MW_lag76', 'quantity_MW_lag77', 'quantity_MW_lag78', 'quantity_MW_lag79', 'quantity_MW_lag80', 'quantity_MW_lag81', 'quantity_MW_lag82', 'quantity_MW_lag83', 'quantity_MW_lag84', 'quantity_MW_lag85', 'quantity_MW_lag86', 'quantity_MW_lag87', 'quantity_MW_lag88', 'quantity_MW_lag89', 'quantity_MW_lag90', 'quantity_MW_lag91', 'quantity_MW_lag92', 'quantity_MW_lag93', 'quantity_MW_lag94', 'quantity_MW_lag95', 'quantity_MW_lag96', 'quantity_MW_lag97', 'quantity_MW_lag98', 'quantity_MW_lag99', 'quantity_MW_lag100', 'quantity_MW_lag101', 'quantity_MW_lag102', 'quantity_MW_lag103', 'quantity_MW_lag104', 'quantity_MW_lag105', 'quantity_MW_lag106', 'quantity_MW_lag107', 'quantity_MW_lag108', 'quantity_MW_lag109', 'quantity_MW_lag110', 'quantity_MW_lag111', 'quantity_MW_lag112', 'quantity_MW_lag113', 'quantity_MW_lag114', 'quantity_MW_lag115', 'quantity_MW_lag116', 'quantity_MW_lag117', 'quantity_MW_lag118', 'quantity_MW_lag119', 'quantity_MW_lag120', 'quantity_MW_lag121', 'quantity_MW_lag122', 'quantity_MW_lag123', 'quantity_MW_lag124', 'quantity_MW_lag125', 'quantity_MW_lag126', 'quantity_MW_lag127', 'quantity_MW_lag128', 'quantity_MW_lag129', 'quantity_MW_lag130', 'quantity_MW_lag131', 'quantity_MW_lag132', 'quantity_MW_lag133', 'quantity_MW_lag134', 'quantity_MW_lag135', 'quantity_MW_lag136', 'quantity_MW_lag137', 'quantity_MW_lag138', 'quantity_MW_lag139', 'quantity_MW_lag140', 'quantity_MW_lag141', 'quantity_MW_lag142', 'quantity_MW_lag143', 'quantity_MW_lag144', 'quantity_MW_lag145', 'quantity_MW_lag146', 'quantity_MW_lag147', 'quantity_MW_lag148', 'quantity_MW_lag149', 'quantity_MW_lag150', 'quantity_MW_lag151', 'quantity_MW_lag152', 'quantity_MW_lag153', 'quantity_MW_lag154', 'quantity_MW_lag155', 'quantity_MW_lag156', 'quantity_MW_lag157', 'quantity_MW_lag158', 'quantity_MW_lag159', 'quantity_MW_lag160', 'quantity_MW_lag161', 'quantity_MW_lag162', 'quantity_MW_lag163', 'quantity_MW_lag164', 'quantity_MW_lag165', 'quantity_MW_lag166', 'quantity_MW_lag167', 'quantity_MW_lag168', 'quantity_MW_lag169', 'quantity_MW_lag170', 'quantity_MW_lag171', 'quantity_MW_lag172', 'quantity_MW_lag173', 'quantity_MW_lag174', 'quantity_MW_lag175', 'quantity_MW_lag176', 'quantity_MW_lag177', 'quantity_MW_lag178', 'quantity_MW_lag179', 'quantity_MW_lag180', 'quantity_MW_lag181', 'quantity_MW_lag182', 'quantity_MW_lag183', 'quantity_MW_lag184', 'quantity_MW_lag185', 'quantity_MW_lag186', 'quantity_MW_lag187', 'quantity_MW_lag188', 'quantity_MW_lag189', 'quantity_MW_lag190', 'quantity_MW_lag191', 'quantity_MW_lag192', 'quantity_MW_lag193', 'quantity_MW_lag194', 'quantity_MW_lag195', 'quantity_MW_lag196', 'quantity_MW_lag197', 'quantity_MW_lag198', 'quantity_MW_lag199', 'quantity_MW_lag200', 'quantity_MW_lag201', 'quantity_MW_lag202', 'quantity_MW_lag203', 'quantity_MW_lag204', 'quantity_MW_lag205', 'quantity_MW_lag206', 'quantity_MW_lag207', 'quantity_MW_lag208', 'quantity_MW_lag209', 'quantity_MW_lag210', 'quantity_MW_lag211', 'quantity_MW_lag212', 'quantity_MW_lag213', 'quantity_MW_lag214', 'quantity_MW_lag215', 'quantity_MW_lag216', 'quantity_MW_lag217', 'quantity_MW_lag218', 'quantity_MW_lag219', 'quantity_MW_lag220', 'quantity_MW_lag221', 'quantity_MW_lag222', 'quantity_MW_lag223', 'quantity_MW_lag224', 'quantity_MW_lag225', 'quantity_MW_lag226', 'quantity_MW_lag227', 'quantity_MW_lag228', 'quantity_MW_lag229', 'quantity_MW_lag230', 'quantity_MW_lag231', 'quantity_MW_lag232', 'quantity_MW_lag233', 'quantity_MW_lag234', 'quantity_MW_lag235', 'quantity_MW_lag236', 'quantity_MW_lag237', 'quantity_MW_lag238', 'quantity_MW_lag239', 'quantity_MW_lag240']\n",
      "[INFO] Ignorando targets n√£o num√©ricos/bool em build_flat_matrix: ['quantity_MW_lead1', 'quantity_MW_lead2', 'quantity_MW_lead3', 'quantity_MW_lead4', 'quantity_MW_lead5', 'quantity_MW_lead6', 'quantity_MW_lead7', 'quantity_MW_lead8', 'quantity_MW_lead9', 'quantity_MW_lead10', 'quantity_MW_lead11', 'quantity_MW_lead12', 'quantity_MW_lead13', 'quantity_MW_lead14', 'quantity_MW_lead15', 'quantity_MW_lead16', 'quantity_MW_lead17', 'quantity_MW_lead18', 'quantity_MW_lead19', 'quantity_MW_lead20', 'quantity_MW_lead21', 'quantity_MW_lead22', 'quantity_MW_lead23', 'quantity_MW_lead24', 'quantity_MW_lead25', 'quantity_MW_lead26', 'quantity_MW_lead27', 'quantity_MW_lead28', 'quantity_MW_lead29', 'quantity_MW_lead30', 'quantity_MW_lead31', 'quantity_MW_lead32', 'quantity_MW_lead33', 'quantity_MW_lead34', 'quantity_MW_lead35', 'quantity_MW_lead36', 'quantity_MW_lead37', 'quantity_MW_lead38', 'quantity_MW_lead39', 'quantity_MW_lead40', 'quantity_MW_lead41', 'quantity_MW_lead42', 'quantity_MW_lead43', 'quantity_MW_lead44', 'quantity_MW_lead45', 'quantity_MW_lead46', 'quantity_MW_lead47', 'quantity_MW_lead48', 'quantity_MW_lead49', 'quantity_MW_lead50', 'quantity_MW_lead51', 'quantity_MW_lead52', 'quantity_MW_lead53', 'quantity_MW_lead54', 'quantity_MW_lead55', 'quantity_MW_lead56', 'quantity_MW_lead57', 'quantity_MW_lead58', 'quantity_MW_lead59', 'quantity_MW_lead60', 'quantity_MW_lead61', 'quantity_MW_lead62', 'quantity_MW_lead63', 'quantity_MW_lead64', 'quantity_MW_lead65', 'quantity_MW_lead66', 'quantity_MW_lead67', 'quantity_MW_lead68', 'quantity_MW_lead69', 'quantity_MW_lead70', 'quantity_MW_lead71', 'quantity_MW_lead72']\n",
      "[‚ÑπÔ∏è] Reduzindo lags: usando 168 e mascarando 72 restantes com -999.0\n",
      "[Linear:train] linhas=27,787  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "üíæ Salvando parquets LINEAR/MLP ...\n",
      "üìê Normaliza√ß√£o ...\n",
      "[INFO] Ignorando features n√£o num√©ricas/bool em build_flat_matrix: ['quantity_MW_lag1', 'quantity_MW_lag2', 'quantity_MW_lag3', 'quantity_MW_lag4', 'quantity_MW_lag5', 'quantity_MW_lag6', 'quantity_MW_lag7', 'quantity_MW_lag8', 'quantity_MW_lag9', 'quantity_MW_lag10', 'quantity_MW_lag11', 'quantity_MW_lag12', 'quantity_MW_lag13', 'quantity_MW_lag14', 'quantity_MW_lag15', 'quantity_MW_lag16', 'quantity_MW_lag17', 'quantity_MW_lag18', 'quantity_MW_lag19', 'quantity_MW_lag20', 'quantity_MW_lag21', 'quantity_MW_lag22', 'quantity_MW_lag23', 'quantity_MW_lag24', 'quantity_MW_lag25', 'quantity_MW_lag26', 'quantity_MW_lag27', 'quantity_MW_lag28', 'quantity_MW_lag29', 'quantity_MW_lag30', 'quantity_MW_lag31', 'quantity_MW_lag32', 'quantity_MW_lag33', 'quantity_MW_lag34', 'quantity_MW_lag35', 'quantity_MW_lag36', 'quantity_MW_lag37', 'quantity_MW_lag38', 'quantity_MW_lag39', 'quantity_MW_lag40', 'quantity_MW_lag41', 'quantity_MW_lag42', 'quantity_MW_lag43', 'quantity_MW_lag44', 'quantity_MW_lag45', 'quantity_MW_lag46', 'quantity_MW_lag47', 'quantity_MW_lag48', 'quantity_MW_lag49', 'quantity_MW_lag50', 'quantity_MW_lag51', 'quantity_MW_lag52', 'quantity_MW_lag53', 'quantity_MW_lag54', 'quantity_MW_lag55', 'quantity_MW_lag56', 'quantity_MW_lag57', 'quantity_MW_lag58', 'quantity_MW_lag59', 'quantity_MW_lag60', 'quantity_MW_lag61', 'quantity_MW_lag62', 'quantity_MW_lag63', 'quantity_MW_lag64', 'quantity_MW_lag65', 'quantity_MW_lag66', 'quantity_MW_lag67', 'quantity_MW_lag68', 'quantity_MW_lag69', 'quantity_MW_lag70', 'quantity_MW_lag71', 'quantity_MW_lag72', 'quantity_MW_lag73', 'quantity_MW_lag74', 'quantity_MW_lag75', 'quantity_MW_lag76', 'quantity_MW_lag77', 'quantity_MW_lag78', 'quantity_MW_lag79', 'quantity_MW_lag80', 'quantity_MW_lag81', 'quantity_MW_lag82', 'quantity_MW_lag83', 'quantity_MW_lag84', 'quantity_MW_lag85', 'quantity_MW_lag86', 'quantity_MW_lag87', 'quantity_MW_lag88', 'quantity_MW_lag89', 'quantity_MW_lag90', 'quantity_MW_lag91', 'quantity_MW_lag92', 'quantity_MW_lag93', 'quantity_MW_lag94', 'quantity_MW_lag95', 'quantity_MW_lag96', 'quantity_MW_lag97', 'quantity_MW_lag98', 'quantity_MW_lag99', 'quantity_MW_lag100', 'quantity_MW_lag101', 'quantity_MW_lag102', 'quantity_MW_lag103', 'quantity_MW_lag104', 'quantity_MW_lag105', 'quantity_MW_lag106', 'quantity_MW_lag107', 'quantity_MW_lag108', 'quantity_MW_lag109', 'quantity_MW_lag110', 'quantity_MW_lag111', 'quantity_MW_lag112', 'quantity_MW_lag113', 'quantity_MW_lag114', 'quantity_MW_lag115', 'quantity_MW_lag116', 'quantity_MW_lag117', 'quantity_MW_lag118', 'quantity_MW_lag119', 'quantity_MW_lag120', 'quantity_MW_lag121', 'quantity_MW_lag122', 'quantity_MW_lag123', 'quantity_MW_lag124', 'quantity_MW_lag125', 'quantity_MW_lag126', 'quantity_MW_lag127', 'quantity_MW_lag128', 'quantity_MW_lag129', 'quantity_MW_lag130', 'quantity_MW_lag131', 'quantity_MW_lag132', 'quantity_MW_lag133', 'quantity_MW_lag134', 'quantity_MW_lag135', 'quantity_MW_lag136', 'quantity_MW_lag137', 'quantity_MW_lag138', 'quantity_MW_lag139', 'quantity_MW_lag140', 'quantity_MW_lag141', 'quantity_MW_lag142', 'quantity_MW_lag143', 'quantity_MW_lag144', 'quantity_MW_lag145', 'quantity_MW_lag146', 'quantity_MW_lag147', 'quantity_MW_lag148', 'quantity_MW_lag149', 'quantity_MW_lag150', 'quantity_MW_lag151', 'quantity_MW_lag152', 'quantity_MW_lag153', 'quantity_MW_lag154', 'quantity_MW_lag155', 'quantity_MW_lag156', 'quantity_MW_lag157', 'quantity_MW_lag158', 'quantity_MW_lag159', 'quantity_MW_lag160', 'quantity_MW_lag161', 'quantity_MW_lag162', 'quantity_MW_lag163', 'quantity_MW_lag164', 'quantity_MW_lag165', 'quantity_MW_lag166', 'quantity_MW_lag167', 'quantity_MW_lag168', 'quantity_MW_lag169', 'quantity_MW_lag170', 'quantity_MW_lag171', 'quantity_MW_lag172', 'quantity_MW_lag173', 'quantity_MW_lag174', 'quantity_MW_lag175', 'quantity_MW_lag176', 'quantity_MW_lag177', 'quantity_MW_lag178', 'quantity_MW_lag179', 'quantity_MW_lag180', 'quantity_MW_lag181', 'quantity_MW_lag182', 'quantity_MW_lag183', 'quantity_MW_lag184', 'quantity_MW_lag185', 'quantity_MW_lag186', 'quantity_MW_lag187', 'quantity_MW_lag188', 'quantity_MW_lag189', 'quantity_MW_lag190', 'quantity_MW_lag191', 'quantity_MW_lag192', 'quantity_MW_lag193', 'quantity_MW_lag194', 'quantity_MW_lag195', 'quantity_MW_lag196', 'quantity_MW_lag197', 'quantity_MW_lag198', 'quantity_MW_lag199', 'quantity_MW_lag200', 'quantity_MW_lag201', 'quantity_MW_lag202', 'quantity_MW_lag203', 'quantity_MW_lag204', 'quantity_MW_lag205', 'quantity_MW_lag206', 'quantity_MW_lag207', 'quantity_MW_lag208', 'quantity_MW_lag209', 'quantity_MW_lag210', 'quantity_MW_lag211', 'quantity_MW_lag212', 'quantity_MW_lag213', 'quantity_MW_lag214', 'quantity_MW_lag215', 'quantity_MW_lag216', 'quantity_MW_lag217', 'quantity_MW_lag218', 'quantity_MW_lag219', 'quantity_MW_lag220', 'quantity_MW_lag221', 'quantity_MW_lag222', 'quantity_MW_lag223', 'quantity_MW_lag224', 'quantity_MW_lag225', 'quantity_MW_lag226', 'quantity_MW_lag227', 'quantity_MW_lag228', 'quantity_MW_lag229', 'quantity_MW_lag230', 'quantity_MW_lag231', 'quantity_MW_lag232', 'quantity_MW_lag233', 'quantity_MW_lag234', 'quantity_MW_lag235', 'quantity_MW_lag236', 'quantity_MW_lag237', 'quantity_MW_lag238', 'quantity_MW_lag239', 'quantity_MW_lag240']\n",
      "[INFO] Ignorando targets n√£o num√©ricos/bool em build_flat_matrix: ['quantity_MW_lead1', 'quantity_MW_lead2', 'quantity_MW_lead3', 'quantity_MW_lead4', 'quantity_MW_lead5', 'quantity_MW_lead6', 'quantity_MW_lead7', 'quantity_MW_lead8', 'quantity_MW_lead9', 'quantity_MW_lead10', 'quantity_MW_lead11', 'quantity_MW_lead12', 'quantity_MW_lead13', 'quantity_MW_lead14', 'quantity_MW_lead15', 'quantity_MW_lead16', 'quantity_MW_lead17', 'quantity_MW_lead18', 'quantity_MW_lead19', 'quantity_MW_lead20', 'quantity_MW_lead21', 'quantity_MW_lead22', 'quantity_MW_lead23', 'quantity_MW_lead24', 'quantity_MW_lead25', 'quantity_MW_lead26', 'quantity_MW_lead27', 'quantity_MW_lead28', 'quantity_MW_lead29', 'quantity_MW_lead30', 'quantity_MW_lead31', 'quantity_MW_lead32', 'quantity_MW_lead33', 'quantity_MW_lead34', 'quantity_MW_lead35', 'quantity_MW_lead36', 'quantity_MW_lead37', 'quantity_MW_lead38', 'quantity_MW_lead39', 'quantity_MW_lead40', 'quantity_MW_lead41', 'quantity_MW_lead42', 'quantity_MW_lead43', 'quantity_MW_lead44', 'quantity_MW_lead45', 'quantity_MW_lead46', 'quantity_MW_lead47', 'quantity_MW_lead48', 'quantity_MW_lead49', 'quantity_MW_lead50', 'quantity_MW_lead51', 'quantity_MW_lead52', 'quantity_MW_lead53', 'quantity_MW_lead54', 'quantity_MW_lead55', 'quantity_MW_lead56', 'quantity_MW_lead57', 'quantity_MW_lead58', 'quantity_MW_lead59', 'quantity_MW_lead60', 'quantity_MW_lead61', 'quantity_MW_lead62', 'quantity_MW_lead63', 'quantity_MW_lead64', 'quantity_MW_lead65', 'quantity_MW_lead66', 'quantity_MW_lead67', 'quantity_MW_lead68', 'quantity_MW_lead69', 'quantity_MW_lead70', 'quantity_MW_lead71', 'quantity_MW_lead72']\n",
      "[Linear:train] linhas=27,691  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[üíæ] Parquet salvo: data/N1A/linear_dataset_train.parquet\n",
      "üíæ Salvando parquets LINEAR/MLP ...\n",
      "[Linear:val] linhas=3,347  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[üíæ] Parquet salvo: data/CV/linear_dataset_train.parquet\n",
      "[Linear:val] linhas=1,159  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[Linear:train] linhas=27,619  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[üíæ] Parquet salvo: data/CV/linear_dataset_val.parquet\n",
      "[üíæ] Parquet salvo: data/N1A/linear_dataset_val.parquet\n",
      "[Linear:test] linhas=1,161  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "üìê Normaliza√ß√£o ...\n",
      "[Linear:test] linhas=3,348  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[üíæ] Parquet salvo: data/CV/linear_dataset_test.parquet\n",
      "üì¶ Dataset Parquet carregado para treinamento.\n",
      "==== PIPELINE LSTM ====\n",
      "üîÑ Carregando dados brutos ...\n",
      "[INFO] Carregado 20,958 linhas (20.0%) de data/raw/raw_dataset.parquet [balanceado por 'country']\n",
      "üî§ Encoding ...\n",
      "[üíæ] Parquet salvo: data/N1B/linear_dataset_train.parquet\n",
      "‚úÇÔ∏è Split train/val/test ...\n",
      "[DIVIDIDO] train: 12,574 linhas\n",
      "[DIVIDIDO] val: 4,191 linhas\n",
      "[DIVIDIDO] test: 4,193 linhas\n",
      "üìê Normaliza√ß√£o ...\n",
      "üî≥ Construindo matrizes sequenciais para LSTM ...\n",
      "[INFO] Ignorando features n√£o num√©ricas/bool em build_sequence_matrix: ['datetime']\n",
      "[üíæ] Parquet salvo: data/N1A/linear_dataset_test.parquet\n",
      "üì¶ Dataset Parquet carregado para treinamento.\n",
      "==== PIPELINE LSTM ====\n",
      "üîÑ Carregando dados brutos ...\n",
      "[Linear:val] linhas=3,251  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "üî§ Encoding ...\n",
      "‚úÇÔ∏è Split train/val/test ...\n",
      "[DIVIDIDO] train: 20,948 linhas\n",
      "[DIVIDIDO] val: 6,982 linhas\n",
      "[DIVIDIDO] test: 6,984 linhas\n",
      "üìê Normaliza√ß√£o ...\n",
      "üî≥ Construindo matrizes sequenciais para LSTM ...\n",
      "[INFO] Ignorando features n√£o num√©ricas/bool em build_sequence_matrix: ['datetime']\n",
      "[üíæ] Parquet salvo: data/N1C/linear_dataset_train.parquet\n",
      "[Linear:val] linhas=3,179  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "üíæ Salvando parquets LINEAR/MLP ...\n",
      "[üíæ] Parquet salvo: data/N1B/linear_dataset_val.parquet\n",
      "üìê Normaliza√ß√£o ...\n",
      "[Linear:test] linhas=3,252  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[üíæ] Parquet salvo: data/N1C/linear_dataset_val.parquet\n",
      "[Linear:test] linhas=3,180  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[üíæ] Parquet salvo: data/N1B/linear_dataset_test.parquet\n",
      "üì¶ Dataset Parquet carregado para treinamento.\n",
      "==== PIPELINE LSTM ====\n",
      "üîÑ Carregando dados brutos ...\n",
      "[Linear:train] linhas=83,399  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "üî§ Encoding ...\n",
      "‚úÇÔ∏è Split train/val/test ...\n",
      "[DIVIDIDO] train: 20,948 linhas\n",
      "[DIVIDIDO] val: 6,982 linhas\n",
      "[DIVIDIDO] test: 6,984 linhas\n",
      "üìê Normaliza√ß√£o ...\n",
      "üî≥ Construindo matrizes sequenciais para LSTM ...\n",
      "[INFO] Ignorando features n√£o num√©ricas/bool em build_sequence_matrix: ['datetime']\n",
      "[JANELAS] X=(11641, 240, 12), Y=(11641, 72, 1), seq_len=240, lead=72\n",
      "[JANELAS] X=(20805, 72, 12), Y=(20805, 72, 1), seq_len=72, lead=72\n",
      "[üíæ] Parquet salvo: data/N1C/linear_dataset_test.parquet\n",
      "üì¶ Dataset Parquet carregado para treinamento.\n",
      "==== PIPELINE LSTM ====\n",
      "üîÑ Carregando dados brutos ...\n",
      "üî§ Encoding ...\n",
      "‚úÇÔ∏è Split train/val/test ...\n",
      "[DIVIDIDO] train: 20,948 linhas\n",
      "[DIVIDIDO] val: 6,982 linhas\n",
      "[DIVIDIDO] test: 6,984 linhas\n",
      "üìê Normaliza√ß√£o ...\n",
      "üî≥ Construindo matrizes sequenciais para LSTM ...\n",
      "[INFO] Ignorando features n√£o num√©ricas/bool em build_sequence_matrix: ['datetime']\n",
      "üíæ Salvando parquets LINEAR/MLP ...\n",
      "[üíæ] Parquet salvo: data/N2A/linear_dataset_train.parquet\n",
      "[Linear:val] linhas=10,046  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[Linear:train] linhas=83,111  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "üíæ Salvando parquets LINEAR/MLP ...\n",
      "[üíæ] Parquet salvo: data/N2A/linear_dataset_val.parquet\n",
      "[Linear:train] linhas=82,895  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[Linear:test] linhas=10,048  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[JANELAS] X=(20709, 168, 12), Y=(20709, 72, 1), seq_len=168, lead=72\n",
      "[JANELAS] X=(20637, 240, 12), Y=(20637, 72, 1), seq_len=240, lead=72\n",
      "[üíæ] Parquet salvo: data/N2A/linear_dataset_test.parquet\n",
      "üì¶ Dataset Parquet carregado para treinamento.\n",
      "==== PIPELINE LSTM ====\n",
      "üîÑ Carregando dados brutos ...\n",
      "üî§ Encoding ...\n",
      "‚úÇÔ∏è Split train/val/test ...\n",
      "[DIVIDIDO] train: 62,873 linhas\n",
      "[DIVIDIDO] val: 20,957 linhas\n",
      "[DIVIDIDO] test: 20,959 linhas\n",
      "üìê Normaliza√ß√£o ...\n",
      "üî≥ Construindo matrizes sequenciais para LSTM ...\n",
      "[INFO] Ignorando features n√£o num√©ricas/bool em build_sequence_matrix: ['datetime']\n",
      "[üíæ] Parquet salvo: data/N2B/linear_dataset_train.parquet\n",
      "[JANELAS] X=(3258, 240, 12), Y=(3258, 72, 1), seq_len=240, lead=72\n",
      "[Linear:val] linhas=9,758  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[JANELAS] X=(6839, 72, 12), Y=(6839, 72, 1), seq_len=72, lead=72\n",
      "[üíæ] Parquet salvo: data/N2B/linear_dataset_val.parquet\n",
      "[Linear:test] linhas=9,760  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[JANELAS] X=(3260, 240, 12), Y=(3260, 72, 1), seq_len=240, lead=72\n",
      "[üíæ] Parquet salvo: data/N2C/linear_dataset_train.parquet\n",
      "[üíæ] Parquet salvo: data/N2B/linear_dataset_test.parquet\n",
      "üì¶ Dataset Parquet carregado para treinamento.\n",
      "==== PIPELINE LSTM ====\n",
      "üîÑ Carregando dados brutos ...\n",
      "üî§ Encoding ...\n",
      "[Linear:val] linhas=9,542  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[JANELAS] X=(62444, 72, 12), Y=(62444, 72, 1), seq_len=72, lead=72\n",
      "‚úÇÔ∏è Split train/val/test ...\n",
      "[DIVIDIDO] train: 62,873 linhas\n",
      "[DIVIDIDO] val: 20,957 linhas\n",
      "[DIVIDIDO] test: 20,959 linhas\n",
      "üìê Normaliza√ß√£o ...\n",
      "[JANELAS] X=(6841, 72, 12), Y=(6841, 72, 1), seq_len=72, lead=72\n",
      "üî≥ Construindo matrizes sequenciais para LSTM ...\n",
      "[INFO] Ignorando features n√£o num√©ricas/bool em build_sequence_matrix: ['datetime']\n",
      "üíæ Salvando parquets LSTM ...\n",
      "[üíæ] Parquet salvo: data/N2C/linear_dataset_val.parquet\n",
      "[Linear:test] linhas=9,544  X=251  Y=73  escrevendo em chunks de 200,000‚Ä¶\n",
      "[JANELAS] X=(6743, 168, 12), Y=(6743, 72, 1), seq_len=168, lead=72\n",
      "üíæ Salvando parquets LSTM ...\n",
      "[üíæ] Parquet salvo: data/N2C/linear_dataset_test.parquet\n",
      "üì¶ Dataset Parquet carregado para treinamento.\n",
      "==== PIPELINE LSTM ====\n",
      "üîÑ Carregando dados brutos ...\n",
      "üî§ Encoding ...\n",
      "‚úÇÔ∏è Split train/val/test ...\n",
      "[üíæ] Parquet salvo: data/CV/lstm_dataset_train.parquet (11,641 linhas)\n",
      "[DIVIDIDO] train: 62,873 linhas\n",
      "[DIVIDIDO] val: 20,957 linhas\n",
      "[DIVIDIDO] test: 20,959 linhas\n",
      "üìê Normaliza√ß√£o ...\n",
      "üî≥ Construindo matrizes sequenciais para LSTM ...\n",
      "[INFO] Ignorando features n√£o num√©ricas/bool em build_sequence_matrix: ['datetime']\n",
      "[üíæ] Parquet salvo: data/N1A/lstm_dataset_train.parquet (20,805 linhas)\n",
      "[üíæ] Parquet salvo: data/CV/lstm_dataset_val.parquet (3,258 linhas)\n",
      "[üíæ] Parquet salvo: data/N1A/lstm_dataset_val.parquet (6,839 linhas)\n",
      "[üíæ] Parquet salvo: data/CV/lstm_dataset_test.parquet (3,260 linhas)\n",
      "üì¶ Dataset LSTM Parquet carregado para treinamento.\n",
      "==== PIPELINE TFT ====\n",
      "üîÑ Carregando dados brutos ...\n",
      "[INFO] Carregado 20,958 linhas (20.0%) de data/raw/raw_dataset.parquet [balanceado por 'country']\n",
      "üî§ Encoding ...\n",
      "‚úÇÔ∏è Split train/val/test ...\n",
      "[DIVIDIDO] train: 16,766 linhas\n",
      "[DIVIDIDO] val: 2,095 linhas\n",
      "[DIVIDIDO] test: 2,097 linhas\n",
      "üìê Normaliza√ß√£o ...\n",
      "üß± Construindo parquets para TFT ...\n",
      "[üíæ] Parquet salvo: data/N1A/lstm_dataset_test.parquet (6,841 linhas)\n",
      "üì¶ Dataset LSTM Parquet carregado para treinamento.\n",
      "==== PIPELINE TFT ====\n",
      "üîÑ Carregando dados brutos ...\n",
      "üî§ Encoding ...\n",
      "üíæ Split 'train' salvo em data/CV/tft_dataset_train.parquet (16766 linhas, grupos=3, max local time_idx=5592).\n",
      "üíæ Split 'val' salvo em data/CV/tft_dataset_val.parquet (2095 linhas, grupos=3, max local time_idx=705).\n",
      "‚úÇÔ∏è Split train/val/test ...\n",
      "[DIVIDIDO] train: 27,931 linhas\n",
      "[DIVIDIDO] val: 3,491 linhas\n",
      "[DIVIDIDO] test: 3,492 linhas\n",
      "üìê Normaliza√ß√£o ...\n",
      "üíæ Split 'test' salvo em data/CV/tft_dataset_test.parquet (2097 linhas, grupos=3, max local time_idx=708).\n",
      "üß± Construindo parquets para TFT ...\n",
      "üì¶ Parquets TFT salvos. Carregando dataset ...\n",
      "  Progresso: 1/7\n",
      "üíæ Split 'train' salvo em data/N1A/tft_dataset_train.parquet (27931 linhas, grupos=1, max local time_idx=27930).\n",
      "üíæ Split 'val' salvo em data/N1A/tft_dataset_val.parquet (3491 linhas, grupos=1, max local time_idx=3490).\n",
      "üíæ Split 'test' salvo em data/N1A/tft_dataset_test.parquet (3492 linhas, grupos=1, max local time_idx=3491).\n",
      "üì¶ Parquets TFT salvos. Carregando dataset ...\n",
      "  Progresso: 2/7\n",
      "[JANELAS] X=(6745, 168, 12), Y=(6745, 72, 1), seq_len=168, lead=72\n",
      "üíæ Salvando parquets LSTM ...\n",
      "[JANELAS] X=(6671, 240, 12), Y=(6671, 72, 1), seq_len=240, lead=72\n",
      "[JANELAS] X=(62156, 168, 12), Y=(62156, 72, 1), seq_len=168, lead=72\n",
      "[JANELAS] X=(61940, 240, 12), Y=(61940, 72, 1), seq_len=240, lead=72\n",
      "[JANELAS] X=(6673, 240, 12), Y=(6673, 72, 1), seq_len=240, lead=72\n",
      "üíæ Salvando parquets LSTM ...\n",
      "[üíæ] Parquet salvo: data/N1B/lstm_dataset_train.parquet (20,709 linhas)\n",
      "[üíæ] Parquet salvo: data/N1B/lstm_dataset_val.parquet (6,743 linhas)\n",
      "[üíæ] Parquet salvo: data/N1B/lstm_dataset_test.parquet (6,745 linhas)\n",
      "üì¶ Dataset LSTM Parquet carregado para treinamento.\n",
      "==== PIPELINE TFT ====\n",
      "üîÑ Carregando dados brutos ...\n",
      "üî§ Encoding ...\n",
      "‚úÇÔ∏è Split train/val/test ...\n",
      "[DIVIDIDO] train: 27,931 linhas\n",
      "[DIVIDIDO] val: 3,491 linhas\n",
      "[DIVIDIDO] test: 3,492 linhas\n",
      "üìê Normaliza√ß√£o ...\n",
      "üß± Construindo parquets para TFT ...\n",
      "[üíæ] Parquet salvo: data/N1C/lstm_dataset_train.parquet (20,637 linhas)\n",
      "üíæ Split 'train' salvo em data/N1B/tft_dataset_train.parquet (27931 linhas, grupos=1, max local time_idx=27930).\n",
      "üíæ Split 'val' salvo em data/N1B/tft_dataset_val.parquet (3491 linhas, grupos=1, max local time_idx=3490).\n",
      "üíæ Split 'test' salvo em data/N1B/tft_dataset_test.parquet (3492 linhas, grupos=1, max local time_idx=3491).\n",
      "üì¶ Parquets TFT salvos. Carregando dataset ...\n",
      "  Progresso: 3/7\n",
      "[JANELAS] X=(20528, 72, 12), Y=(20528, 72, 1), seq_len=72, lead=72\n",
      "[üíæ] Parquet salvo: data/N1C/lstm_dataset_val.parquet (6,671 linhas)\n",
      "[üíæ] Parquet salvo: data/N1C/lstm_dataset_test.parquet (6,673 linhas)\n",
      "üì¶ Dataset LSTM Parquet carregado para treinamento.\n",
      "==== PIPELINE TFT ====\n",
      "üîÑ Carregando dados brutos ...\n",
      "üî§ Encoding ...\n",
      "‚úÇÔ∏è Split train/val/test ...\n",
      "[DIVIDIDO] train: 27,931 linhas\n",
      "[DIVIDIDO] val: 3,491 linhas\n",
      "[DIVIDIDO] test: 3,492 linhas\n",
      "üìê Normaliza√ß√£o ...\n",
      "üß± Construindo parquets para TFT ...\n",
      "üíæ Split 'train' salvo em data/N1C/tft_dataset_train.parquet (27931 linhas, grupos=1, max local time_idx=27930).\n",
      "üíæ Split 'val' salvo em data/N1C/tft_dataset_val.parquet (3491 linhas, grupos=1, max local time_idx=3490).\n",
      "üíæ Split 'test' salvo em data/N1C/tft_dataset_test.parquet (3492 linhas, grupos=1, max local time_idx=3491).\n",
      "üì¶ Parquets TFT salvos. Carregando dataset ...\n",
      "  Progresso: 4/7\n",
      "[JANELAS] X=(20530, 72, 12), Y=(20530, 72, 1), seq_len=72, lead=72\n",
      "[JANELAS] X=(20240, 168, 12), Y=(20240, 72, 1), seq_len=168, lead=72\n",
      "üíæ Salvando parquets LSTM ...\n",
      "[JANELAS] X=(20242, 168, 12), Y=(20242, 72, 1), seq_len=168, lead=72\n",
      "üíæ Salvando parquets LSTM ...\n",
      "[üíæ] Parquet salvo: data/N2A/lstm_dataset_train.parquet (62,444 linhas)\n",
      "[JANELAS] X=(20024, 240, 12), Y=(20024, 72, 1), seq_len=240, lead=72\n",
      "[üíæ] Parquet salvo: data/N2A/lstm_dataset_val.parquet (20,528 linhas)\n",
      "[JANELAS] X=(20026, 240, 12), Y=(20026, 72, 1), seq_len=240, lead=72\n",
      "[üíæ] Parquet salvo: data/N2A/lstm_dataset_test.parquet (20,530 linhas)\n",
      "üì¶ Dataset LSTM Parquet carregado para treinamento.\n",
      "==== PIPELINE TFT ====\n",
      "üîÑ Carregando dados brutos ...\n",
      "üî§ Encoding ...\n",
      "‚úÇÔ∏è Split train/val/test ...\n",
      "[DIVIDIDO] train: 83,831 linhas\n",
      "[DIVIDIDO] val: 10,478 linhas\n",
      "[DIVIDIDO] test: 10,480 linhas\n",
      "üìê Normaliza√ß√£o ...\n",
      "üß± Construindo parquets para TFT ...\n",
      "üíæ Split 'train' salvo em data/N2A/tft_dataset_train.parquet (83831 linhas, grupos=3, max local time_idx=28018).\n",
      "üíæ Salvando parquets LSTM ...\n",
      "üíæ Split 'val' salvo em data/N2A/tft_dataset_val.parquet (10478 linhas, grupos=3, max local time_idx=3506).\n",
      "[üíæ] Parquet salvo: data/N2B/lstm_dataset_train.parquet (62,156 linhas)\n",
      "üíæ Split 'test' salvo em data/N2A/tft_dataset_test.parquet (10480 linhas, grupos=3, max local time_idx=3513).\n",
      "üì¶ Parquets TFT salvos. Carregando dataset ...\n",
      "  Progresso: 5/7\n",
      "[üíæ] Parquet salvo: data/N2B/lstm_dataset_val.parquet (20,240 linhas)\n",
      "[üíæ] Parquet salvo: data/N2B/lstm_dataset_test.parquet (20,242 linhas)\n",
      "üì¶ Dataset LSTM Parquet carregado para treinamento.\n",
      "==== PIPELINE TFT ====\n",
      "üîÑ Carregando dados brutos ...\n",
      "üî§ Encoding ...\n",
      "‚úÇÔ∏è Split train/val/test ...\n",
      "[DIVIDIDO] train: 83,831 linhas\n",
      "[DIVIDIDO] val: 10,478 linhas\n",
      "[DIVIDIDO] test: 10,480 linhas\n",
      "üìê Normaliza√ß√£o ...\n",
      "üß± Construindo parquets para TFT ...\n",
      "üíæ Split 'train' salvo em data/N2B/tft_dataset_train.parquet (83831 linhas, grupos=3, max local time_idx=28018).\n",
      "üíæ Split 'val' salvo em data/N2B/tft_dataset_val.parquet (10478 linhas, grupos=3, max local time_idx=3506).\n",
      "[üíæ] Parquet salvo: data/N2C/lstm_dataset_train.parquet (61,940 linhas)\n",
      "üíæ Split 'test' salvo em data/N2B/tft_dataset_test.parquet (10480 linhas, grupos=3, max local time_idx=3513).\n",
      "üì¶ Parquets TFT salvos. Carregando dataset ...\n",
      "  Progresso: 6/7\n",
      "[üíæ] Parquet salvo: data/N2C/lstm_dataset_val.parquet (20,024 linhas)\n",
      "[üíæ] Parquet salvo: data/N2C/lstm_dataset_test.parquet (20,026 linhas)\n",
      "üì¶ Dataset LSTM Parquet carregado para treinamento.\n",
      "==== PIPELINE TFT ====\n",
      "üîÑ Carregando dados brutos ...\n",
      "üî§ Encoding ...\n",
      "‚úÇÔ∏è Split train/val/test ...\n",
      "[DIVIDIDO] train: 83,831 linhas\n",
      "[DIVIDIDO] val: 10,478 linhas\n",
      "[DIVIDIDO] test: 10,480 linhas\n",
      "üìê Normaliza√ß√£o ...\n",
      "üß± Construindo parquets para TFT ...\n",
      "üíæ Split 'train' salvo em data/N2C/tft_dataset_train.parquet (83831 linhas, grupos=3, max local time_idx=28018).\n",
      "üíæ Split 'val' salvo em data/N2C/tft_dataset_val.parquet (10478 linhas, grupos=3, max local time_idx=3506).\n",
      "üíæ Split 'test' salvo em data/N2C/tft_dataset_test.parquet (10480 linhas, grupos=3, max local time_idx=3513).\n",
      "üì¶ Parquets TFT salvos. Carregando dataset ...\n",
      "  Progresso: 7/7\n",
      "‚úÖ Tarefa conclu√≠da.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import optuna\n",
    "except ImportError:\n",
    "    optuna = None\n",
    "    print(\"[WARN] Optuna n√£o instalado. Instale com pip install optuna para habilitar HPO.\")\n",
    "\n",
    "full_configs = perguntas\n",
    "full_configs.append(cv)  # lista de problemas adicionais  e dataset reduzido para CV (N1A.. etc.)\n",
    "\n",
    "# =============================\n",
    "# Preprocessamento base para treinamento (cfg 'treinamento')\n",
    "# =============================\n",
    "preprocess_collector = {}\n",
    "\n",
    "def run_preprocessing(cfg: Dict[str, Any],\n",
    "                      preproc_train_linear: Optional[LinearPreprocessor] = None,\n",
    "                      preproc_train_lstm: Optional[LSTMPreprocessor] = None,\n",
    "                      preproc_train_tft: Optional[TFTPreprocessor] = None):\n",
    "    \"\"\"Executa sequencialmente as tr√™s pipelines gerando datasets parquet.\n",
    "    Retorna inst√¢ncias dos preprocessadores com splits salvos.\n",
    "    \"\"\"\n",
    "    destino_dir = cfg[\"data_dir\"]\n",
    "    lag = cfg.get(\"lag\", 24)\n",
    "    lead = cfg.get(\"lead\", 24)\n",
    "    seq_len = cfg.get(\"seq_len\", lag)\n",
    "\n",
    "    preproc_lin = LinearPreprocessor(\n",
    "        model_name=\"Linear\",\n",
    "        lag=lag,\n",
    "        lead=lead,\n",
    "        country_list=cfg[\"countries\"],\n",
    "        feature_cols=cfg[\"feats\"],\n",
    "        target_cols=cfg[\"tgts\"],\n",
    "        data_dir=destino_dir,\n",
    "        num_cols=cfg.get(\"vals\")\n",
    "    )\n",
    "\n",
    "    preproc_lstm = LSTMPreprocessor(\n",
    "        model_name=\"LSTM\",\n",
    "        lag=seq_len,\n",
    "        lead=lead,\n",
    "        country_list=cfg[\"countries\"],\n",
    "        feature_cols=cfg[\"feats\"],\n",
    "        target_cols=cfg[\"tgts\"],\n",
    "        data_dir=destino_dir,\n",
    "        num_cols=cfg.get(\"vals\")\n",
    "    )\n",
    "\n",
    "    preproc_tft = TFTPreprocessor(\n",
    "        model_name=\"TFT\",\n",
    "        seq_len=seq_len,\n",
    "        lead=lead,\n",
    "        country_list=cfg[\"countries\"],\n",
    "        feature_cols=cfg[\"feats\"],\n",
    "        target_cols=cfg[\"tgts\"],\n",
    "        data_dir=destino_dir,\n",
    "    )\n",
    "    # Ajusta num_cols se n√£o fornecido explicitamente\n",
    "    if not preproc_tft.num_cols and cfg.get(\"vals\"):\n",
    "        preproc_tft.num_cols = list(cfg[\"vals\"])  # compatibilidade\n",
    "\n",
    "    # Herdando encoders / normalizadores se fornecidos\n",
    "    for src, dst in [\n",
    "        (preproc_train_linear, preproc_lin),\n",
    "        (preproc_train_lstm, preproc_lstm),\n",
    "        (preproc_train_tft, preproc_tft),\n",
    "    ]:\n",
    "        if src:\n",
    "            try:\n",
    "                dst.encod_objects = src.encod_objects\n",
    "                dst.norm_objects = src.norm_objects\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    print(\"==== PIPELINE LINEAR/MLP ====\")\n",
    "    preproc_lin = linear_preproccess_pipeline(\n",
    "        preproc=preproc_lin,\n",
    "        destino_dir=destino_dir,\n",
    "        seq_len=seq_len,\n",
    "        masked_value=-999.0,\n",
    "        save_instance=True,\n",
    "        train=cfg.get(\"train\", False),\n",
    "        size=cfg.get(\"size\", 1.0)\n",
    "    )\n",
    "\n",
    "    print(\"==== PIPELINE LSTM ====\")\n",
    "    preproc_lstm = lstm_preproccess_pipeline(\n",
    "        preproc=preproc_lstm,\n",
    "        destino_dir=destino_dir,\n",
    "        save_instance=True,\n",
    "        train=cfg.get(\"train\", False),\n",
    "        size=cfg.get(\"size\", 1.0)\n",
    "    )\n",
    "\n",
    "    print(\"==== PIPELINE TFT ====\")\n",
    "    preproc_tft = tft_preproccess_pipeline(\n",
    "        preproc=preproc_tft,\n",
    "        destino_dir=destino_dir,\n",
    "        save_instance=True,\n",
    "        train=cfg.get(\"train\", False),\n",
    "        size=cfg.get(\"size\", 1.0)\n",
    "    )\n",
    "\n",
    "    preprocess_collector[cfg[\"name\"]] = {\n",
    "        \"linear\": preproc_lin,\n",
    "        \"lstm\": preproc_lstm,\n",
    "        \"tft\": preproc_tft,\n",
    "    }\n",
    "\n",
    "    return preproc_lin, preproc_lstm, preproc_tft\n",
    "\n",
    "\n",
    "## Processamento individual de dataset completo, para setting de par√¢metros de encodding e decoding\n",
    "train_preproc_linear, train_preproc_lstm, train_preproc_tft = run_preprocessing(treinamento)\n",
    "\n",
    "## Paraleliza√ß√£o da gera√ß√£o de dataset de Perguntas N1A -> N3C\n",
    "max_workers = min(len(full_configs), multiprocessing.cpu_count())\n",
    "\n",
    "print(f\"üöÄ Executando {len(full_configs)} tarefas em {max_workers} workers...\")\n",
    "\n",
    "results = [None] * len(full_configs)\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = {executor.submit(run_preprocessing, \n",
    "        item,\n",
    "        preproc_train_linear=train_preproc_linear,\n",
    "        preproc_train_lstm=train_preproc_lstm,\n",
    "        preproc_train_tft=train_preproc_tft): \n",
    "        idx for idx, item in enumerate(full_configs)}\n",
    "    \n",
    "    for i, future in enumerate(as_completed(futures)):\n",
    "        idx = futures[future]\n",
    "        try:\n",
    "            results[idx] = future.result()\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] item {idx}: {e}\")\n",
    "            results[idx] = None\n",
    "        if (i + 1) % max(1, len(full_configs)//10) == 0:\n",
    "            print(f\"  Progresso: {i+1}/{len(full_configs)}\")\n",
    "\n",
    "print(\"‚úÖ Tarefa conclu√≠da.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b138dc",
   "metadata": {},
   "source": [
    "# Cap√≠tulo 5: Defini√ß√£o da estrutura dos modelos - Com otimiza√ß√£o de hiperparm para o MLP, Linear, LSTM\n",
    "Configura√ß√£o dos par√¢metros dos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d8e826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 12:35:54,190] A new study created in memory with name: CV_linear_opt\n",
      "I0000 00:00:1762875355.917982    4290 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2025-11-11 12:35:59,186] Trial 0 finished with value: 0.04793832451105118 and parameters: {'units1': 64, 'units2': 16, 'n_layers': 2, 'lr': 0.0016359685594031128, 'l2': 0.00045661066762539337}. Best is trial 0 with value: 0.04793832451105118.\n",
      "[I 2025-11-11 12:36:02,777] Trial 1 finished with value: 0.04887697100639343 and parameters: {'units1': 256, 'units2': 176, 'n_layers': 1, 'lr': 0.0016396975471659634, 'l2': 4.1787733238967005e-05}. Best is trial 0 with value: 0.04793832451105118.\n",
      "[I 2025-11-11 12:36:06,289] Trial 2 finished with value: 0.06466010957956314 and parameters: {'units1': 128, 'units2': 144, 'n_layers': 3, 'lr': 0.00038712476442352955, 'l2': 1.1253437647719851e-05}. Best is trial 0 with value: 0.04793832451105118.\n",
      "[I 2025-11-11 12:36:09,790] Trial 3 finished with value: 0.06168622151017189 and parameters: {'units1': 64, 'units2': 16, 'n_layers': 1, 'lr': 0.00040051786705381696, 'l2': 3.347345932798056e-06}. Best is trial 0 with value: 0.04793832451105118.\n",
      "[I 2025-11-11 12:36:13,311] Trial 4 finished with value: 0.1656329184770584 and parameters: {'units1': 224, 'units2': 224, 'n_layers': 2, 'lr': 1.24276061575707e-05, 'l2': 2.0978024063934508e-05}. Best is trial 0 with value: 0.04793832451105118.\n",
      "[I 2025-11-11 12:36:16,752] Trial 5 finished with value: 0.05227933079004288 and parameters: {'units1': 160, 'units2': 192, 'n_layers': 1, 'lr': 0.002743532965350746, 'l2': 4.6265557816381796e-07}. Best is trial 0 with value: 0.04793832451105118.\n",
      "[I 2025-11-11 12:36:20,255] Trial 6 pruned. \n",
      "[I 2025-11-11 12:36:23,792] Trial 7 finished with value: 0.056970372796058655 and parameters: {'units1': 96, 'units2': 112, 'n_layers': 1, 'lr': 0.0006168335784942416, 'l2': 1.043017710066319e-06}. Best is trial 0 with value: 0.04793832451105118.\n",
      "[I 2025-11-11 12:36:27,251] Trial 8 finished with value: 0.052507270127534866 and parameters: {'units1': 256, 'units2': 16, 'n_layers': 1, 'lr': 0.0011420996693477594, 'l2': 4.352427782170428e-05}. Best is trial 0 with value: 0.04793832451105118.\n",
      "[I 2025-11-11 12:36:30,740] Trial 9 finished with value: 0.049564067274332047 and parameters: {'units1': 128, 'units2': 96, 'n_layers': 3, 'lr': 0.002207857670221281, 'l2': 1.5429389212700796e-07}. Best is trial 0 with value: 0.04793832451105118.\n",
      "[I 2025-11-11 12:36:34,214] Trial 10 pruned. \n",
      "[I 2025-11-11 12:36:38,043] Trial 11 pruned. \n",
      "[I 2025-11-11 12:36:41,559] Trial 12 pruned. \n",
      "[I 2025-11-11 12:36:45,024] Trial 13 finished with value: 0.05199570581316948 and parameters: {'units1': 192, 'units2': 176, 'n_layers': 1, 'lr': 0.0010934631124009564, 'l2': 0.000765889493849135}. Best is trial 0 with value: 0.04793832451105118.\n",
      "[I 2025-11-11 12:36:48,579] Trial 14 finished with value: 0.04963543266057968 and parameters: {'units1': 32, 'units2': 96, 'n_layers': 2, 'lr': 0.0046499978941896445, 'l2': 0.00012185164852577029}. Best is trial 0 with value: 0.04793832451105118.\n",
      "[I 2025-11-11 12:36:52,113] Trial 15 pruned. \n",
      "[I 2025-11-11 12:36:55,597] Trial 16 finished with value: 0.05202909931540489 and parameters: {'units1': 160, 'units2': 128, 'n_layers': 1, 'lr': 0.0011360713518233205, 'l2': 0.0003545495145795904}. Best is trial 0 with value: 0.04793832451105118.\n",
      "[I 2025-11-11 12:36:59,066] Trial 17 finished with value: 0.04842850938439369 and parameters: {'units1': 224, 'units2': 64, 'n_layers': 3, 'lr': 0.001620281608195704, 'l2': 5.962454443365503e-06}. Best is trial 0 with value: 0.04793832451105118.\n",
      "[I 2025-11-11 12:37:02,517] Trial 18 pruned. \n",
      "[I 2025-11-11 12:37:06,003] Trial 19 pruned. \n",
      "[I 2025-11-11 12:37:09,537] Trial 20 finished with value: 0.04940685257315636 and parameters: {'units1': 96, 'units2': 80, 'n_layers': 3, 'lr': 0.004832100247833646, 'l2': 1.0386588475714023e-06}. Best is trial 0 with value: 0.04793832451105118.\n",
      "[I 2025-11-11 12:37:12,970] Trial 21 finished with value: 0.048185210675001144 and parameters: {'units1': 224, 'units2': 48, 'n_layers': 2, 'lr': 0.001743182061869007, 'l2': 3.892678144941973e-05}. Best is trial 0 with value: 0.04793832451105118.\n",
      "[I 2025-11-11 12:37:16,558] Trial 22 finished with value: 0.04838119074702263 and parameters: {'units1': 224, 'units2': 48, 'n_layers': 2, 'lr': 0.001727519832062896, 'l2': 0.00015278409196445495}. Best is trial 0 with value: 0.04793832451105118.\n",
      "[I 2025-11-11 12:37:19,990] Trial 23 pruned. \n",
      "[I 2025-11-11 12:37:23,515] Trial 24 finished with value: 0.049819253385066986 and parameters: {'units1': 160, 'units2': 32, 'n_layers': 2, 'lr': 0.0023188664411678685, 'l2': 0.00010795239919853638}. Best is trial 0 with value: 0.04793832451105118.\n",
      "[I 2025-11-11 12:37:27,027] Trial 25 pruned. \n",
      "[I 2025-11-11 12:37:30,472] Trial 26 pruned. \n",
      "[I 2025-11-11 12:37:33,976] Trial 27 pruned. \n",
      "[I 2025-11-11 12:37:37,434] Trial 28 finished with value: 0.0478443019092083 and parameters: {'units1': 256, 'units2': 48, 'n_layers': 2, 'lr': 0.001640955471403829, 'l2': 0.0001810217260381896}. Best is trial 28 with value: 0.0478443019092083.\n",
      "[I 2025-11-11 12:37:40,933] Trial 29 finished with value: 0.048613253980875015 and parameters: {'units1': 256, 'units2': 112, 'n_layers': 2, 'lr': 0.0016080277090272693, 'l2': 3.484134174139624e-05}. Best is trial 28 with value: 0.0478443019092083.\n",
      "[I 2025-11-11 12:37:40,934] A new study created in memory with name: CV_mlp_opt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Best linear params saved to resultados/hparams/CV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 12:37:55,077] Trial 0 finished with value: 0.0765860453248024 and parameters: {'units1': 256, 'units2': 96, 'n_layers': 2, 'lr': 0.0003525175888811857, 'l2': 0.0003068418106735204, 'act': 'tanh', 'layer_norm': True}. Best is trial 0 with value: 0.0765860453248024.\n",
      "[I 2025-11-11 12:38:05,016] Trial 1 finished with value: 0.06224062666296959 and parameters: {'units1': 160, 'units2': 32, 'n_layers': 3, 'lr': 4.704418042331307e-05, 'l2': 1.9487959942126205e-06, 'act': 'leaky_relu', 'layer_norm': True}. Best is trial 1 with value: 0.06224062666296959.\n",
      "[I 2025-11-11 12:38:11,504] Trial 2 finished with value: 0.12879562377929688 and parameters: {'units1': 256, 'units2': 128, 'n_layers': 2, 'lr': 1.2175653878335605e-05, 'l2': 4.3147749099734166e-05, 'act': 'tanh', 'layer_norm': False}. Best is trial 1 with value: 0.06224062666296959.\n",
      "[I 2025-11-11 12:38:17,614] Trial 3 finished with value: 0.1578703373670578 and parameters: {'units1': 224, 'units2': 144, 'n_layers': 2, 'lr': 0.00011040225194968143, 'l2': 0.0006713359545147231, 'act': 'tanh', 'layer_norm': False}. Best is trial 1 with value: 0.06224062666296959.\n",
      "[I 2025-11-11 12:38:23,246] Trial 4 finished with value: 0.044614143669605255 and parameters: {'units1': 224, 'units2': 144, 'n_layers': 3, 'lr': 0.0005286283360741857, 'l2': 7.4714257375772706e-06, 'act': 'leaky_relu', 'layer_norm': True}. Best is trial 4 with value: 0.044614143669605255.\n",
      "[I 2025-11-11 12:38:29,291] Trial 5 finished with value: 0.042557086795568466 and parameters: {'units1': 96, 'units2': 144, 'n_layers': 1, 'lr': 0.00027584914755005154, 'l2': 1.5668654839989308e-07, 'act': 'tanh', 'layer_norm': True}. Best is trial 5 with value: 0.042557086795568466.\n",
      "[I 2025-11-11 12:38:33,962] Trial 6 finished with value: 0.041423164308071136 and parameters: {'units1': 160, 'units2': 48, 'n_layers': 1, 'lr': 0.004786610961265601, 'l2': 7.109209983717106e-06, 'act': 'leaky_relu', 'layer_norm': False}. Best is trial 6 with value: 0.041423164308071136.\n",
      "[I 2025-11-11 12:38:39,559] Trial 7 pruned. \n",
      "[I 2025-11-11 12:38:45,602] Trial 8 pruned. \n",
      "[I 2025-11-11 12:38:51,201] Trial 9 finished with value: 0.040121838450431824 and parameters: {'units1': 128, 'units2': 64, 'n_layers': 3, 'lr': 0.002426615345277053, 'l2': 4.371850237604089e-06, 'act': 'leaky_relu', 'layer_norm': False}. Best is trial 9 with value: 0.040121838450431824.\n",
      "[I 2025-11-11 12:38:56,791] Trial 10 finished with value: 0.03815411031246185 and parameters: {'units1': 32, 'units2': 224, 'n_layers': 3, 'lr': 0.004754296233121152, 'l2': 4.066526165887229e-07, 'act': 'leaky_relu', 'layer_norm': False}. Best is trial 10 with value: 0.03815411031246185.\n",
      "[I 2025-11-11 12:39:02,368] Trial 11 finished with value: 0.039149537682533264 and parameters: {'units1': 32, 'units2': 240, 'n_layers': 3, 'lr': 0.004199286127666843, 'l2': 4.094111152874569e-07, 'act': 'leaky_relu', 'layer_norm': False}. Best is trial 10 with value: 0.03815411031246185.\n",
      "[I 2025-11-11 12:39:07,565] Trial 12 finished with value: 0.0409281887114048 and parameters: {'units1': 32, 'units2': 240, 'n_layers': 3, 'lr': 0.001387041861333763, 'l2': 1.9613205366043588e-07, 'act': 'leaky_relu', 'layer_norm': False}. Best is trial 10 with value: 0.03815411031246185.\n",
      "[I 2025-11-11 12:39:12,302] Trial 13 pruned. \n",
      "[I 2025-11-11 12:39:17,720] Trial 14 finished with value: 0.03877759352326393 and parameters: {'units1': 64, 'units2': 208, 'n_layers': 3, 'lr': 0.004538472017036773, 'l2': 6.227959654698647e-07, 'act': 'leaky_relu', 'layer_norm': False}. Best is trial 10 with value: 0.03815411031246185.\n",
      "[I 2025-11-11 12:39:23,286] Trial 15 finished with value: 0.041867054998874664 and parameters: {'units1': 96, 'units2': 192, 'n_layers': 2, 'lr': 0.0009267306346001123, 'l2': 1.4289870851486215e-06, 'act': 'leaky_relu', 'layer_norm': False}. Best is trial 10 with value: 0.03815411031246185.\n",
      "[I 2025-11-11 12:39:28,863] Trial 16 finished with value: 0.03980021923780441 and parameters: {'units1': 64, 'units2': 192, 'n_layers': 3, 'lr': 0.00259456779909141, 'l2': 1.1265108687053261e-07, 'act': 'leaky_relu', 'layer_norm': False}. Best is trial 10 with value: 0.03815411031246185.\n",
      "[I 2025-11-11 12:39:34,427] Trial 17 finished with value: 0.040808144956827164 and parameters: {'units1': 64, 'units2': 208, 'n_layers': 2, 'lr': 0.0006879141997740726, 'l2': 4.821281815428529e-07, 'act': 'leaky_relu', 'layer_norm': False}. Best is trial 10 with value: 0.03815411031246185.\n",
      "[I 2025-11-11 12:39:40,539] Trial 18 pruned. \n",
      "[I 2025-11-11 12:39:46,160] Trial 19 pruned. \n",
      "[I 2025-11-11 12:39:51,814] Trial 20 finished with value: 0.03927905112504959 and parameters: {'units1': 64, 'units2': 176, 'n_layers': 2, 'lr': 0.001782257826720322, 'l2': 8.135907372693075e-07, 'act': 'leaky_relu', 'layer_norm': False}. Best is trial 10 with value: 0.03815411031246185.\n",
      "[I 2025-11-11 12:39:56,668] Trial 21 finished with value: 0.03986816108226776 and parameters: {'units1': 32, 'units2': 256, 'n_layers': 3, 'lr': 0.004504554836757571, 'l2': 3.4154051133322025e-07, 'act': 'leaky_relu', 'layer_norm': False}. Best is trial 10 with value: 0.03815411031246185.\n",
      "[I 2025-11-11 12:40:02,259] Trial 22 finished with value: 0.03959491103887558 and parameters: {'units1': 32, 'units2': 224, 'n_layers': 3, 'lr': 0.0048684344389921625, 'l2': 2.7488679189260613e-07, 'act': 'leaky_relu', 'layer_norm': False}. Best is trial 10 with value: 0.03815411031246185.\n",
      "[I 2025-11-11 12:40:07,768] Trial 23 finished with value: 0.03881613165140152 and parameters: {'units1': 32, 'units2': 240, 'n_layers': 3, 'lr': 0.003443007865191695, 'l2': 9.26400919063009e-07, 'act': 'leaky_relu', 'layer_norm': False}. Best is trial 10 with value: 0.03815411031246185.\n",
      "[I 2025-11-11 12:40:13,377] Trial 24 finished with value: 0.04068036004900932 and parameters: {'units1': 64, 'units2': 224, 'n_layers': 3, 'lr': 0.0024540292901249878, 'l2': 2.888008546477008e-06, 'act': 'leaky_relu', 'layer_norm': False}. Best is trial 10 with value: 0.03815411031246185.\n",
      "[I 2025-11-11 12:40:18,980] Trial 25 finished with value: 0.03977495804429054 and parameters: {'units1': 96, 'units2': 176, 'n_layers': 3, 'lr': 0.0009376751853561477, 'l2': 1.0551333104856207e-06, 'act': 'leaky_relu', 'layer_norm': False}. Best is trial 10 with value: 0.03815411031246185.\n",
      "[I 2025-11-11 12:40:23,990] Trial 26 pruned. \n",
      "[I 2025-11-11 12:40:29,195] Trial 27 pruned. \n",
      "[I 2025-11-11 12:40:34,756] Trial 28 pruned. \n",
      "[I 2025-11-11 12:40:40,767] Trial 29 pruned. \n",
      "[I 2025-11-11 12:40:40,768] A new study created in memory with name: CV_lstm_opt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Best mlp params saved to resultados/hparams/CV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 12:42:40,256] Trial 0 finished with value: 0.39157769083976746 and parameters: {'emb_dim': 28, 'lstm_units': 192, 'dense_units': 16, 'lr': 0.00010684756824871038}. Best is trial 0 with value: 0.39157769083976746.\n",
      "[I 2025-11-11 12:44:39,480] Trial 1 finished with value: 0.2484598606824875 and parameters: {'emb_dim': 20, 'lstm_units': 192, 'dense_units': 16, 'lr': 0.00041388199217116695}. Best is trial 1 with value: 0.2484598606824875.\n",
      "[I 2025-11-11 12:46:00,338] Trial 2 finished with value: 0.10970410704612732 and parameters: {'emb_dim': 16, 'lstm_units': 96, 'dense_units': 96, 'lr': 1.0525015921811265e-05}. Best is trial 2 with value: 0.10970410704612732.\n"
     ]
    }
   ],
   "source": [
    "# Configura√ß√µes padr√µes dos modelos\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "# Configura√ß√µes dos modelos\n",
    "configs_linear = {\n",
    "    \"linear\": {\n",
    "        \"linear\": True,\n",
    "        \"units\": [128, 64],\n",
    "        \"dropout\": 0.1,\n",
    "        \"lr\": 5e-4,\n",
    "        \"l2\": 1e-6,\n",
    "        \"act\": \"leaky_relu\",\n",
    "        \"layer_norm\": False,\n",
    "        \"mask_value\": -999.0,\n",
    "    }\n",
    "}\n",
    "\n",
    "configs_mlp = {\n",
    "    \"mlp\": {\n",
    "        \"linear\": False,\n",
    "        \"units\": [128, 64],\n",
    "        \"dropout\": 0.1,\n",
    "        \"lr\": 5e-4,\n",
    "        \"l2\": 1e-6,\n",
    "        \"act\": \"leaky_relu\",\n",
    "        \"layer_norm\": False,\n",
    "        \"mask_value\": -999.0,\n",
    "    }\n",
    "}\n",
    "\n",
    "configs_lstm = {\n",
    "    \"lstm\": {\n",
    "        \"emb_dim\": 8,\n",
    "        \"lstm_units\": 128,\n",
    "        \"dense_units\": 64,\n",
    "        \"dropout\": 0 if gpu_devices else 0.15,\n",
    "        \"lr\": 5e-4\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Presets de TFT compat√≠veis com build_tft_model (PyTorch Forecasting)\n",
    "# Campos utilizados: hidden_size, dropout, lstm_layers, num_heads, lr, epochs, patience\n",
    "config_tft = {\n",
    "    \"tft\": {\n",
    "        \"hidden_size\": 64,\n",
    "        \"lstm_layers\": 1,\n",
    "        \"num_heads\": 2,\n",
    "        \"dropout\": 0.2,\n",
    "        \"hidden_continuous_size\": 32,\n",
    "        \"attention_head_size\": 2,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"patience\": 20,\n",
    "        \"epochs\": 200,\n",
    "    }\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Optuna: otimiza√ß√£o (Linear/MLP/LSTM)\n",
    "# ============================\n",
    "import os, json\n",
    "from typing import Dict, Any\n",
    "import optuna\n",
    "\n",
    "def _ensure_dirs(*paths):\n",
    "    for p in paths:\n",
    "        os.makedirs(p, exist_ok=True)\n",
    "\n",
    "\n",
    "# ============================\n",
    "# LINEAR / MLP\n",
    "# ============================\n",
    "def _linear_objective(trial: \"optuna.trial.Trial\", data_dir: str, batch_size: int, model_kind: str = \"linear\") -> float:\n",
    "    ds_train, meta_tr = LinearPreprocessor.load_linear_parquet_dataset(\n",
    "        data_dir=data_dir, split='train', batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    ds_val, meta_va = LinearPreprocessor.load_linear_parquet_dataset(\n",
    "        data_dir=data_dir, split='val', batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    x_dim, y_dim = int(meta_tr['x_dim']), int(meta_tr['y_dim'])\n",
    "\n",
    "    # Search space\n",
    "    units1 = trial.suggest_int(\"units1\", 32, 256, step=32)\n",
    "    units2 = trial.suggest_int(\"units2\", 16, 256, step=16)\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.5) if not gpu_devices else 0.0\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 5e-3, log=True)\n",
    "    l2 = trial.suggest_float(\"l2\", 1e-7, 1e-3, log=True)\n",
    "    act = trial.suggest_categorical(\"act\", [\"tanh\", \"leaky_relu\"]) if model_kind != \"linear\" else \"leaky_relu\"\n",
    "    layer_norm = trial.suggest_categorical(\"layer_norm\", [False, True]) if model_kind != \"linear\" else False\n",
    "\n",
    "    params = {\n",
    "        \"linear\": (model_kind == \"linear\"),\n",
    "        \"units\": [units1] + ([units2] if n_layers >= 2 else []),\n",
    "        \"dropout\": dropout,\n",
    "        \"lr\": lr,\n",
    "        \"l2\": l2,\n",
    "        \"act\": act,\n",
    "        \"layer_norm\": layer_norm,\n",
    "        \"mask_value\": -999.0,\n",
    "    }\n",
    "\n",
    "    model = (\n",
    "        build_linear_model(x_dim=x_dim, y_dim=y_dim, params=params, linear=True)\n",
    "        if model_kind == \"linear\"\n",
    "        else build_mlp_model(x_dim=x_dim, y_dim=y_dim, params=params)\n",
    "    )\n",
    "\n",
    "    callbacks = [\n",
    "        TFEarlyStopping(monitor=\"val_loss\", patience=8, restore_best_weights=True, verbose=0),\n",
    "        TFReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=4, min_lr=1e-6, verbose=0),\n",
    "    ]\n",
    "\n",
    "    history = model.fit(ds_train, validation_data=ds_val, epochs=30, verbose=0, callbacks=callbacks)\n",
    "    val_loss = min(history.history.get(\"val_loss\", [float(\"inf\")]))\n",
    "    trial.report(val_loss, step=1)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()\n",
    "    return float(val_loss)\n",
    "\n",
    "\n",
    "def optimize_linear_or_mlp(problem_name: str, data_dir: str, model_kind: str = \"linear\",\n",
    "                           n_trials: int = 20, batch_size: int = 256) -> Dict[str, Any]:\n",
    "    assert model_kind in (\"linear\", \"mlp\")\n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        study_name=f\"{problem_name}_{model_kind}_opt\",\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=1)\n",
    "    )\n",
    "    study.optimize(lambda t: _linear_objective(t, data_dir, batch_size, model_kind), n_trials=n_trials)\n",
    "\n",
    "    best = study.best_trial\n",
    "    best_params = dict(best.params)\n",
    "    units = [best_params.pop(\"units1\"), best_params.pop(\"units2\", None)]\n",
    "    units = [u for u in units if u]\n",
    "    best_params_recon = {\n",
    "        \"linear\": (model_kind == \"linear\"),\n",
    "        \"units\": units if units else [64],\n",
    "        \"dropout\": best_params.get(\"dropout\", 0.1),\n",
    "        \"lr\": best_params.get(\"lr\", 1e-3),\n",
    "        \"l2\": best_params.get(\"l2\", 1e-6),\n",
    "        \"act\": best_params.get(\"act\", \"leaky_relu\"),\n",
    "        \"layer_norm\": best_params.get(\"layer_norm\", False),\n",
    "        \"mask_value\": -999.0,\n",
    "    }\n",
    "    out_dir = os.path.join(\"resultados\", \"hparams\", problem_name)\n",
    "    _ensure_dirs(out_dir)\n",
    "    with open(os.path.join(out_dir, f\"best_{model_kind}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(best_params_recon, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"üíæ Best {model_kind} params saved to {out_dir}\")\n",
    "    return best_params_recon\n",
    "\n",
    "\n",
    "# ============================\n",
    "# LSTM\n",
    "# ============================\n",
    "def _lstm_objective(trial: \"optuna.trial.Trial\", data_dir: str, batch_size: int) -> float:\n",
    "    ds_train, meta_tr = LSTMPreprocessor.load_lstm_parquet_dataset(\n",
    "        data_dir=data_dir, split='train', batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    ds_val, meta_va = LSTMPreprocessor.load_lstm_parquet_dataset(\n",
    "        data_dir=data_dir, split='val', batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    x_dim, y_dim, seq_len, lead = int(meta_tr['x_dim']), int(meta_tr['y_dim']), int(meta_tr['seq_len']), int(meta_tr['lead'])\n",
    "\n",
    "    emb_dim = trial.suggest_int(\"emb_dim\", 4, 32, step=4)\n",
    "    lstm_units = trial.suggest_int(\"lstm_units\", 32, 256, step=32)\n",
    "    dense_units = trial.suggest_int(\"dense_units\", 16, 256, step=16)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.5) if not gpu_devices else 0.0\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 5e-3, log=True)\n",
    "\n",
    "    params = {\"emb_dim\": emb_dim, \"lstm_units\": lstm_units, \"dense_units\": dense_units,\n",
    "              \"dropout\": dropout, \"lr\": lr}\n",
    "\n",
    "    model = build_lstm_model(seq_len=seq_len, x_dim_num=x_dim, y_dim=y_dim, lead=lead, num_countries=4, **params)\n",
    "\n",
    "    callbacks = [\n",
    "        TFEarlyStopping(monitor=\"val_loss\", patience=8, restore_best_weights=True, verbose=0),\n",
    "        TFReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=4, min_lr=1e-6, verbose=0),\n",
    "    ]\n",
    "\n",
    "    history = model.fit(ds_train, validation_data=ds_val, epochs=30, verbose=0, callbacks=callbacks)\n",
    "    val_loss = min(history.history.get(\"val_loss\", [float(\"inf\")]))\n",
    "    trial.report(val_loss, step=1)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()\n",
    "    return float(val_loss)\n",
    "\n",
    "\n",
    "def optimize_lstm(problem_name: str, data_dir: str, n_trials: int = 20, batch_size: int = 128) -> Dict[str, Any]:\n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        study_name=f\"{problem_name}_lstm_opt\",\n",
    "        pruner=optuna.pruners.HyperbandPruner(min_resource=1, max_resource=40, reduction_factor=3)\n",
    "    )\n",
    "    study.optimize(lambda t: _lstm_objective(t, data_dir, batch_size), n_trials=n_trials)\n",
    "\n",
    "    best = study.best_trial\n",
    "    best_params = dict(best.params)\n",
    "\n",
    "    out_dir = os.path.join(\"resultados\", \"hparams\", problem_name)\n",
    "    _ensure_dirs(out_dir)\n",
    "    with open(os.path.join(out_dir, \"best_lstm.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(best_params, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"üíæ Best LSTM params saved to {out_dir}\")\n",
    "    return best_params\n",
    "\n",
    "# Exemplo de uso (descomente para rodar):\n",
    "problem = (cv or {}).get('name')\n",
    "data_dir = (cv or {}).get('data_dir')\n",
    "if problem and data_dir:\n",
    "    configs_linear[\"linear\"] = optimize_linear_or_mlp(problem, data_dir, model_kind='linear', n_trials=30)\n",
    "    configs_linear[\"mlp\"] = optimize_linear_or_mlp(problem, data_dir, model_kind='mlp', n_trials=30)\n",
    "    configs_linear[\"lstm\"] = optimize_lstm(problem, data_dir, n_trials=15)\n",
    "else:\n",
    "    print(\"[INFO] Defina o dicion√°rio 'cv' com 'name' e 'data_dir' para rodar a otimiza√ß√£o.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d441da94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loader de melhores hiperpar√¢metros salvos por Optuna\n",
    "import json, os\n",
    "\n",
    "def load_best_hparams(problem_name: str):\n",
    "    base = os.path.join(\"resultados\", \"hparams\", problem_name)\n",
    "    loaded = {}\n",
    "    for model_kind in [\"linear\", \"mlp\", \"lstm\"]:\n",
    "        path = os.path.join(base, f\"best_{model_kind}.json\")\n",
    "        if os.path.isfile(path):\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                loaded[model_kind] = json.load(f)\n",
    "    if not loaded:\n",
    "        print(f\"[INFO] Nenhum hyperpar√¢metro encontrado em {base}\")\n",
    "    else:\n",
    "        print(f\"[OK] Hiperpar√¢metros carregados: {list(loaded.keys())}\")\n",
    "    return loaded\n",
    "\n",
    "best = load_best_hparams((cv or {}).get('name', ''))\n",
    "if 'linear' in best:\n",
    "    configs_linear = {'linear': {}}\n",
    "    configs_linear['linear'] = best['linear']\n",
    "if 'mlp' in best:\n",
    "    configs_mlp = {'mlp': {}}\n",
    "    configs_mlp['mlp'] = best['mlp']\n",
    "if 'lstm' in best:\n",
    "    configs_lstm = {'lstm': {}}\n",
    "    configs_lstm['lstm'] = best['lstm']\n",
    "print(\"Configs atualizados com melhores hiperpar√¢metros (se encontrados).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e510a9f",
   "metadata": {},
   "source": [
    "# Cap√≠tulo 6: Treinamento dos modelos\n",
    "Este cap√≠tulo executa, por problema: Linear/MLP (configs_linear), MLP (configs_mlp), LSTM (configs_lstm) e TFT (config_tft), liberando mem√≥ria entre execu√ß√µes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2222b57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento sequencial dos modelos (imports centralizados na c√©lula 5)\n",
    "## Gr√°fico de hist√≥rico\n",
    "def plot_training_history(history, title):\n",
    "    \"\"\"Plot training vs validation loss.\"\"\"\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2, linestyle='--')\n",
    "    plt.title('Training vs Validation Loss do modelo ' + title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=':', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "tempo_treino = {}\n",
    "\n",
    "# Carrega configura√ß√£o de treinamento\n",
    "cfg = treinamento\n",
    "\n",
    "if not cfg:\n",
    "    print(\"sem configura√ß√£o de 'treinamento' configurada na lista de treinamento\")\n",
    "else:\n",
    "    histories = {}\n",
    "    name = cfg[\"name\"]\n",
    "    print(f\"\\nüöÄ Iniciando treinamento dos modelos ...\")\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"Modelo Linear...\")\n",
    "    # Treinamento Linear\n",
    "    try:\n",
    "        tempo_treino[\"linear\"] = {}\n",
    "        tempo_treino[\"linear\"][\"inicio\"] = time.time()\n",
    "        models_linear,_= linear_train_pipeline(\n",
    "            problem_name=name,\n",
    "            data_dir=cfg[\"data_dir\"],\n",
    "            batch_size=256,\n",
    "            configs=configs_linear,\n",
    "        )\n",
    "        histories = histories | _\n",
    "        del models_linear\n",
    "        tempo_treino[\"linear\"][\"fim\"] = time.time()\n",
    "        tempo_treino[\"linear\"][\"duracao\"] = tempo_treino[\"linear\"][\"fim\"] - tempo_treino[\"linear\"][\"inicio\"]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao treinar Linear para {name}: {e}\")\n",
    "    finally:\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"Modelo MLP...\")\n",
    "    # Treinamento MLP (configs_mlp)\n",
    "    try:\n",
    "        tempo_treino[\"mlp\"] = {}\n",
    "        tempo_treino[\"mlp\"][\"inicio\"] = time.time()\n",
    "        models_mlp, _ = linear_train_pipeline(\n",
    "            problem_name=name,\n",
    "            data_dir=cfg[\"data_dir\"],\n",
    "            batch_size=256,\n",
    "            configs=configs_mlp,\n",
    "        )\n",
    "        histories = histories | _\n",
    "        del models_mlp\n",
    "        tempo_treino[\"mlp\"][\"fim\"] = time.time()\n",
    "        tempo_treino[\"mlp\"][\"duracao\"] = tempo_treino[\"mlp\"][\"fim\"] - tempo_treino[\"mlp\"][\"inicio\"]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao treinar MLP (configs_mlp) para {name}: {e}\")\n",
    "    finally:\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"Modelo LSTM...\")\n",
    "    # # Treinamento LSTM\n",
    "    try:\n",
    "        tempo_treino[\"lstm\"] = {}\n",
    "        tempo_treino[\"lstm\"][\"inicio\"] = time.time()\n",
    "        models_lstm,_ = lstm_train_pipeline(\n",
    "            problem_name=name,\n",
    "            data_dir=cfg[\"data_dir\"],\n",
    "            seq_len=cfg.get(\"lag\") or cfg.get(\"seq_len\"),\n",
    "            batch_size=256,\n",
    "            configs=configs_lstm,\n",
    "        )\n",
    "        histories = histories | _\n",
    "        del models_lstm\n",
    "        tempo_treino[\"lstm\"][\"fim\"] = time.time()\n",
    "        tempo_treino[\"lstm\"][\"duracao\"] = tempo_treino[\"lstm\"][\"fim\"] - tempo_treino[\"lstm\"][\"inicio\"]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao treinar LSTM para {name}: {e}\")\n",
    "    finally:\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"Modelo TFT...\")\n",
    "    # # Treinamento TFT (Temporal Fusion Transformer)\n",
    "    try:\n",
    "        tempo_treino[\"tft\"] = {}\n",
    "        tempo_treino[\"tft\"][\"inicio\"] = time.time()\n",
    "        models_tft, _ = tft_train_pipeline(\n",
    "            problem_name=name,\n",
    "            data_dir=cfg[\"data_dir\"],\n",
    "            feature_cols=cfg.get(\"feats\") or cfg.get(\"feature_cols\"),\n",
    "            target_cols=cfg.get(\"tgts\") or cfg.get(\"target_cols\"),\n",
    "            seq_len=cfg.get(\"lag\"),\n",
    "            lead=cfg.get(\"lead\"),\n",
    "            batch_size=256,\n",
    "            configs=config_tft,\n",
    "        )\n",
    "        histories = histories | _\n",
    "        del models_tft\n",
    "        tempo_treino[\"tft\"][\"fim\"] = time.time()\n",
    "        tempo_treino[\"tft\"][\"duracao\"] = tempo_treino[\"tft\"][\"fim\"] - tempo_treino[\"tft\"][\"inicio\"]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao treinar TFT para {name}: {e}\")\n",
    "\n",
    "    finally:\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "    print(f\"‚úÖ Problema {name} conclu√≠do ‚Äî mem√≥ria limpa\\n{'-'*60}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c867bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Desempenho de treinamento dos modelos...\")     \n",
    "for modelo, tempo in tempo_treino.items():   \n",
    "    duracao = tempo.get(\"duracao\", 0)\n",
    "    print(f\"‚è±Ô∏è  Tempo de treino {modelo}: {duracao:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076d5715",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in histories.keys():\n",
    "    print(f\"Hist√≥rico de treinamento do modelo: {model_name}...\")\n",
    "    plot_training_history(histories[model_name], model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfc_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

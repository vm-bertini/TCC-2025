{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c49e9f73",
   "metadata": {},
   "source": [
    "# Introdu√ß√£o\n",
    "\n",
    "Esse Notebook ser√° respons√°vel pelo preprocessamento dos dados contidos em ./data/raw para formatos compat√≠veis e otimizados para o treinamento de cada modelo\n",
    "As defini√ß√µes dos problemas aos quais os modelos dever√£o solucionar j√° foram definidas no notebook \"Coleta de Dados\"\n",
    "\n",
    "Modelos a serem Criados:\n",
    "\n",
    "1. Modelo Linear: MLP sem fun√ß√µes de ativa√ß√£o, composta apenas de somas lineares\n",
    "2. MLP: rede neural - efetivamente identica ao modelo linear, no entanto, apresenta fun√ß√£o de ativa√ß√£o ao final do somat√≥rio de fun√ß√µes lineares\n",
    "3. LSTM: Um modelo de rede neural recorrente, com capacidade de diferencia√ß√£o de informa√ß√£o de curto e longo prazo\n",
    "4. TFT: Modelo baseado em LLMs desenvolvido pela microsoft - servir√° como um comparativo mais moderno\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1149ce94",
   "metadata": {},
   "source": [
    "## Depend√™ncias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce23fd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "# Depend√™ncias m√≠nimas para TFT ‚Äî simples e com foco em GPU\n",
    "try:\n",
    "    import torch\n",
    "except Exception:\n",
    "    import shutil\n",
    "    has_gpu = shutil.which(\"nvidia-smi\") is not None\n",
    "    if has_gpu:\n",
    "        # tenta instalar com suporte CUDA (ajuste a vers√£o cu de acordo com sua stack, p.ex. cu121)\n",
    "        %pip install -q torch --index-url https://download.pytorch.org/whl/cu121\n",
    "    else:\n",
    "        %pip install -q torch --index-url https://download.pytorch.org/whl/cpu\n",
    "    # libs do pipeline TFT\n",
    "    %pip install -q pytorch-lightning pytorch-forecasting\n",
    "\n",
    "    import torch\n",
    "\n",
    "\n",
    "\n",
    "print(f\"torch={torch.__version__} | cuda={torch.cuda.is_available()}\")\n",
    "\n",
    "print(\"Verificando depend√™ncias (pyarrow para Parquet)...\")\n",
    "\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    print(f\"PyArrow dispon√≠vel: {pa.__version__}\")\n",
    "except Exception:\n",
    "    print(\"Instalando pyarrow...\")\n",
    "    !pip install --upgrade \"pyarrow>=18\" --quiet\n",
    "    import importlib\n",
    "    importlib.invalidate_caches()\n",
    "    import pyarrow as pa\n",
    "    print(f\"PyArrow instalado: {pa.__version__}\")\n",
    "\n",
    "# fastparquet √© opcional\n",
    "try:\n",
    "    import fastparquet  # noqa: F401\n",
    "    print(\"fastparquet dispon√≠vel (opcional)\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Outras bibliotecas sob demanda\n",
    "for lib in [\n",
    "    \"numpy\", \"python-dotenv\", \"pandas\", \"matplotlib\", \"seaborn\",\n",
    "    \"scikit-learn\", \"tensorflow[and-cuda]\", \"keras\", \"lxml\", \"pytz\"\n",
    "]:\n",
    "    try:\n",
    "        __import__(lib)\n",
    "    except ImportError:\n",
    "        print(f\"Instalando {lib}...\")\n",
    "        !pip install {lib} --quiet\n",
    "\n",
    "print(\"Depend√™ncias prontas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cc7e78",
   "metadata": {},
   "source": [
    "## VARI√ÅVEIS NECESS√ÅRIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89d36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports para a API e utilidades\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import mixed_precision\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "# Silenciando Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)  # last resort\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# GPU CONFIGURATION\n",
    "# ==============================================\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    mixed_precision.set_global_policy('mixed_float16')\n",
    "    print(f\"‚úÖ GPU detected ({gpus[0].name}) - using mixed precision.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected, running on CPU.\")\n",
    "\n",
    "# Carregar vari√°veis de ambiente do .env\n",
    "load_dotenv()\n",
    "# ---------------- CONFIG ---------------- #\n",
    "COUNTRY_DOMAINS = {\n",
    "    \"FR\": {\"domain\": \"10YFR-RTE------C\"},\n",
    "    \"ES\": {\"domain\": \"10YES-REE------0\"},\n",
    "    \"PT\": {\"domain\": \"10YPT-REN------W\"}\n",
    "}\n",
    "\n",
    "DATA_ITEMS = [\n",
    "    {'key': 'load_total', 'documentType': 'A65', 'processType': 'A16', 'domainParam': 'outBiddingZone_Domain', 'parser': 'load'},\n",
    "    {'key': 'market_prices', 'documentType': 'A44', 'processType': 'A07', 'domainParamIn': 'in_Domain', 'domainParamOut': 'out_Domain', 'parser': 'price'}\n",
    "]\n",
    "\n",
    "ENTSOE_TOKEN = os.environ.get(\"ENTSOE_SECURITY_TOKEN\")\n",
    "BASE_URL = \"https://web-api.tp.entsoe.eu/api\"\n",
    "MAX_WORKERS = 100\n",
    "RAW_DIR = os.path.join(\"data\", \"raw\")\n",
    "PARQUET_COMPRESSION = \"zstd\"\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "\n",
    "# ==============================================\n",
    "# DICION√ÅRIO DE PROBLEMAS\n",
    "# ==============================================\n",
    "problemas = [\n",
    "    dict(name=\"treinamento\", data_dir=\"data/treinamento\", feats=[\"country\",\"datetime\",\"quantity_MW\"], tgts=[\"quantity_MW\"], lag=10*24, lead=3*24, vals=[\"quantity_MW\"], countries=COUNTRY_DOMAINS.keys()),\n",
    "    dict(name=\"N1A\", data_dir=\"data/N1A\", feats=[\"country\",\"datetime\",\"quantity_MW\"], tgts=[\"quantity_MW\"], lag=10*24, lead=3*24, vals=[\"quantity_MW\"], countries=[\"ES\"], reduced_dim = 3*24, mask_value=-999.0),\n",
    "    dict(name=\"N1B\", data_dir=\"data/N1B\", feats=[\"country\",\"datetime\",\"quantity_MW\"], tgts=[\"quantity_MW\"], lag=10*24, lead=3*24, vals=[\"quantity_MW\"], countries=[\"ES\"], reduced_dim = 7*24, mask_value=-999.0),\n",
    "    dict(name=\"N1C\", data_dir=\"data/N1C\", feats=[\"country\",\"datetime\",\"quantity_MW\"], tgts=[\"quantity_MW\"], lag=10*24, lead=3*24, vals=[\"quantity_MW\"], countries=[\"ES\"]),\n",
    "    dict(name=\"N2A\", data_dir=\"data/N2A\", feats=[\"country\",\"datetime\",\"quantity_MW\"], tgts=[\"quantity_MW\"], lag=10*24, lead=3*24, vals=[\"quantity_MW\"], countries=COUNTRY_DOMAINS.keys(), reduced_dim = 3*24, mask_value=-999.0),\n",
    "    dict(name=\"N2B\", data_dir=\"data/N2B\", feats=[\"country\",\"datetime\",\"quantity_MW\"], tgts=[\"quantity_MW\"], lag=10*24, lead=3*24, vals=[\"quantity_MW\"], countries=COUNTRY_DOMAINS.keys(), reduced_dim = 7*24, mask_value=-999.0),\n",
    "    dict(name=\"N2C\", data_dir=\"data/N2C\", feats=[\"country\",\"datetime\",\"quantity_MW\"], tgts=[\"quantity_MW\"], lag=10*24, lead=3*24, vals=[\"quantity_MW\"], countries=COUNTRY_DOMAINS.keys()),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5b47ef",
   "metadata": {},
   "source": [
    "# Cap√≠tulo 1: Pr√©processamento de dados\n",
    "\n",
    "Etapa de contru√ß√£o da pipelines de pre-processamento de dados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f852c4",
   "metadata": {},
   "source": [
    "## Classe geral de preprocessamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302b0d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional, List, Tuple\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class Preprocessor:\n",
    "    \"\"\"Pr√©-processador base.\n",
    "\n",
    "    - lag/lead como inteiros s√£o expandidos para ranges [1..N] quando apropriado.\n",
    "    - feature_cols/target_cols definem bases permitidas e servem como sele√ß√£o no export.\n",
    "    - Nenhuma coluna √© removida dos dados; sele√ß√£o ocorre apenas na exporta√ß√£o.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        lag: int,\n",
    "        lead: int,\n",
    "        country_list: Optional[List[str]] = None,\n",
    "        *,\n",
    "        model_name: str = \"linear\",\n",
    "        data_dir: str = \"data/processed\",\n",
    "        feature_cols: Optional[List[str]] = None,\n",
    "        target_cols: Optional[List[str]] = None,\n",
    "    ):\n",
    "        self.lag = lag\n",
    "        self.lead = lead\n",
    "        self.country_list = country_list\n",
    "        self.model_name = model_name\n",
    "        self.data_dir = data_dir\n",
    "        self.save_dir = self.data_dir\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "        self.feature_cols: List[str] = list(feature_cols) if feature_cols else []\n",
    "        self.target_cols: List[str] = list(target_cols) if target_cols else []\n",
    "\n",
    "        self.norm_objects = {}\n",
    "        self.encod_objects = {}\n",
    "        self.df_base = pd.DataFrame()\n",
    "\n",
    "    def _expand_steps(self, steps, default_max: Optional[int]) -> List[int]:\n",
    "        \"\"\"Normaliza passos: int‚Üí[1..N], None‚Üí[1..default_max], lista‚Üícomo est√°.\"\"\"\n",
    "        if isinstance(steps, int):\n",
    "            return list(range(1, steps + 1)) if steps > 0 else [1]\n",
    "        if steps is None and isinstance(default_max, int) and default_max > 0:\n",
    "            return list(range(1, default_max + 1))\n",
    "        if isinstance(steps, (list, tuple)):\n",
    "            return list(steps)\n",
    "        return [1]\n",
    "\n",
    "    def load_data(self, raw_dir: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"Carrega Parquet unificado em data/raw (ou raw_dir) e atualiza self.df_base.\"\"\"\n",
    "        base_raw = raw_dir or os.path.join('data', 'raw')\n",
    "        unified_path = os.path.join(base_raw, f'raw_dataset.parquet')\n",
    "        if not os.path.exists(unified_path):\n",
    "            raise FileNotFoundError(f\"Arquivo unificado n√£o encontrado: {unified_path}. Execute a coleta primeiro.\")\n",
    "        df = pd.read_parquet(unified_path, engine='pyarrow')\n",
    "        if 'datetime' in df.columns:\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "        if self.country_list and 'country' in df.columns:\n",
    "            df = df[df['country'].isin(self.country_list)].copy()\n",
    "        sort_cols = [c for c in ['country', 'datetime'] if c in df.columns]\n",
    "        if sort_cols:\n",
    "            df = df.sort_values(sort_cols).reset_index(drop=True)\n",
    "            \n",
    "        # Filtrando Colunas apenas para as necess√°rias\n",
    "        cols = list(set([c for c in self.feature_cols + self.target_cols if c in df.columns]))\n",
    "        df = df.loc[:, ~df.columns.duplicated()]  # optional: remove duplicates\n",
    "        df = df[cols]\n",
    "\n",
    "        self.df_base = df\n",
    "        return self.df_base\n",
    "\n",
    "    def encode(self, encode_cols: str = 'datetime', encode_method: str = 'label') -> pd.DataFrame:\n",
    "        \"\"\"Codifica de forma n√£o destrutiva e atualiza self.df_base.\n",
    "\n",
    "        - label: usa LabelEncoder com suporte a NaN via placeholder interno que √© revertido no decode.\n",
    "        - time_cycle: adiciona features de calend√°rio e c√≠clicas sem remover datetime.\n",
    "        \"\"\"\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            print(\"df_base vazio. Chame load_data() primeiro.\")\n",
    "            return self.df_base\n",
    "        df = self.df_base.copy()\n",
    "        if encode_method == 'label':\n",
    "            le = LabelEncoder()\n",
    "            s = df[encode_cols].astype(object)\n",
    "            le.fit(s)\n",
    "            df[encode_cols] = le.transform(s)\n",
    "            # salva metadados incluindo o code do NaN\n",
    "            self.encod_objects['label'] = {\n",
    "                'encode_cols': encode_cols,\n",
    "                'label_encoder': le,\n",
    "            }\n",
    "        elif encode_method == 'time_cycle':\n",
    "            if encode_cols not in df.columns:\n",
    "                print(f\"Coluna {encode_cols} n√£o encontrada para time_cycle.\")\n",
    "                self.df_base = df\n",
    "                return df\n",
    "            dt = pd.to_datetime(df[encode_cols], utc=True)\n",
    "            # Mant√©m a coluna original e adiciona componentes discretos e c√≠clicos\n",
    "            df['year'] = dt.dt.year\n",
    "            df['month'] = dt.dt.month\n",
    "            df['day'] = dt.dt.day\n",
    "            df['hour'] = dt.dt.hour\n",
    "            df['minute'] = dt.dt.minute\n",
    "            current_year = time.localtime().tm_year\n",
    "            df['year_sin'] = np.sin(2 * np.pi * df['year'] / max(current_year, 1))\n",
    "            df['year_cos'] = np.cos(2 * np.pi * df['year'] / max(current_year, 1))\n",
    "            df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "            df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "            df['day_sin'] = np.sin(2 * np.pi * df['day'] / 31)\n",
    "            df['day_cos'] = np.cos(2 * np.pi * df['day'] / 31)\n",
    "            df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "            df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "            df['minute_sin'] = np.sin(2 * np.pi * df['minute'] / 60)\n",
    "            df['minute_cos'] = np.cos(2 * np.pi * df['minute'] / 60)\n",
    "            self.encod_objects['time_cycle'] = {'encode_cols': encode_cols}\n",
    "            self.feature_cols.extend([\"year_sin\", \"year_cos\",\n",
    "                                                     \"month_sin\", \"month_cos\",\n",
    "                                                     \"day_sin\", \"day_cos\",\n",
    "                                                     \"hour_sin\", \"hour_cos\",\n",
    "                                                     \"minute_sin\", \"minute_cos\"])\n",
    "        else:\n",
    "            print(f\"encode_method '{encode_method}' n√£o suportado.\")\n",
    "        self.df_base = df\n",
    "        return self.df_base\n",
    "\n",
    "    def decode(self, encode_method: str = 'label', target_col: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"Reverte codifica√ß√µes suportadas (label, time_cycle).\"\"\"\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            print(\"df_base vazio. Nada para decodificar.\")\n",
    "            return self.df_base\n",
    "        df = self.df_base.copy()\n",
    "        if encode_method == 'label':\n",
    "            info = self.encod_objects.get('label')\n",
    "            if not info:\n",
    "                print(\"Nenhuma informa√ß√£o de label encoding salva.\")\n",
    "                return self.df_base\n",
    "            col = info['encode_cols']\n",
    "            le: LabelEncoder = info['label_encoder']\n",
    "            placeholder = info.get('na_placeholder', '__NA__')\n",
    "            try:\n",
    "                inv = le.inverse_transform(df[col].astype(int))\n",
    "                # mapeia placeholder de volta para NaN\n",
    "                inv = pd.Series(inv).replace(placeholder, np.nan).values\n",
    "                df[col] = inv\n",
    "            except Exception as e:\n",
    "                print(f\"Falha ao decodificar label para coluna {col}: {e}\")\n",
    "        elif encode_method == 'time_cycle':\n",
    "            if 'year' not in df.columns:\n",
    "                print(\"Componentes de tempo ausentes para reconstru√ß√£o.\")\n",
    "                return self.df_base\n",
    "            tgt = target_col or 'decoded_datetime'\n",
    "            def _recover_component(sin_col, cos_col, period, offset):\n",
    "                if sin_col not in df.columns or cos_col not in df.columns:\n",
    "                    return pd.Series([np.nan] * len(df))\n",
    "                ang = np.arctan2(df[sin_col], df[cos_col])\n",
    "                ang = (ang + 2 * np.pi) % (2 * np.pi)\n",
    "                idx = np.round((ang / (2 * np.pi)) * period).astype('Int64') % period\n",
    "                return idx + offset\n",
    "            month = _recover_component('month_sin', 'month_cos', 12, 1)\n",
    "            day = _recover_component('day_sin', 'day_cos', 31, 1)\n",
    "            hour = _recover_component('hour_sin', 'hour_cos', 24, 0)\n",
    "            minute = _recover_component('minute_sin', 'minute_cos', 60, 0)\n",
    "            year = df['year'] if 'year' in df.columns else pd.Series([np.nan] * len(df))\n",
    "            dt = pd.to_datetime({\n",
    "                'year': year.astype('Int64'),\n",
    "                'month': month.astype('Int64'),\n",
    "                'day': day.astype('Int64'),\n",
    "                'hour': hour.astype('Int64'),\n",
    "                'minute': minute.astype('Int64'),\n",
    "            }, errors='coerce', utc=True)\n",
    "            df[tgt] = dt\n",
    "        else:\n",
    "            print(f\"encode_method '{encode_method}' n√£o suportado para decode.\")\n",
    "        self.df_base = df\n",
    "        return self.df_base\n",
    "\n",
    "    def normalize(self, value_cols: List[str], normalization_method: str = 'minmax') -> pd.DataFrame:\n",
    "        \"\"\"Normaliza colunas e atualiza self.df_base.\"\"\"\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            print(\"df_base vazio. Chame load_data() primeiro.\")\n",
    "            return self.df_base\n",
    "        df = self.df_base.copy()\n",
    "        scaler = MinMaxScaler() if normalization_method == 'minmax' else (\n",
    "            StandardScaler() if normalization_method == 'standard' else None)\n",
    "        if scaler is None:\n",
    "            raise ValueError(\"normalization_method deve ser 'minmax' ou 'standard'\")\n",
    "        df[value_cols] = scaler.fit_transform(df[value_cols])\n",
    "        self.norm_objects[normalization_method] = {'value_cols': value_cols, 'scaler': scaler}\n",
    "        self.df_base = df\n",
    "        return self.df_base\n",
    "\n",
    "    def normalize_splits(self, value_cols: List[str], normalization_method: str = 'minmax') -> dict:\n",
    "        \"\"\"Normaliza os conjuntos de treino, valida√ß√£o e teste.\"\"\"\n",
    "        if not self.splits:\n",
    "            print(\"Nenhum conjunto dividido encontrado.\")\n",
    "            return {}\n",
    "        normalized_splits = {}\n",
    "        for split_name, split_df in self.splits.items():\n",
    "            self.df_base = split_df\n",
    "            normalized_df = self.normalize(value_cols=value_cols, normalization_method=normalization_method)\n",
    "            normalized_splits[split_name] = normalized_df\n",
    "        self.splits = normalized_splits\n",
    "        return normalized_splits\n",
    "\n",
    "    def denormalize(self, normalization_method: str = 'minmax') -> pd.DataFrame:\n",
    "        \"\"\"Reverte normaliza√ß√£o usando metadados salvos.\"\"\"\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            print(\"df_base vazio. Nada para denormalizar.\")\n",
    "            return self.df_base\n",
    "        info = self.norm_objects.get(normalization_method)\n",
    "        if not info:\n",
    "            print(f\"Nenhum scaler salvo para o m√©todo '{normalization_method}'.\")\n",
    "            return self.df_base\n",
    "        cols: List[str] = info['value_cols']\n",
    "        scaler = info['scaler']\n",
    "        df = self.df_base.copy()\n",
    "        try:\n",
    "            df[cols] = scaler.inverse_transform(df[cols])\n",
    "        except Exception as e:\n",
    "            print(f\"Falha ao denormalizar colunas {cols}: {e}\")\n",
    "            return self.df_base\n",
    "        self.df_base = df\n",
    "        return self.df_base\n",
    "\n",
    "    def save_df_base(self, filename: Optional[str] = None, compression: Optional[str] = None, partition_by: Optional[List[str]] = None) -> Optional[str]:\n",
    "        \"\"\"Salva self.df_base em Parquet dentro de data_dir/{model_name}.\"\"\"\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            print(\"df_base vazio. Nada para salvar.\")\n",
    "            return None\n",
    "        comp = compression\n",
    "        if comp is None:\n",
    "            try:\n",
    "                comp = PARQUET_COMPRESSION\n",
    "            except NameError:\n",
    "                comp = 'zstd'\n",
    "        filename = \"raw_dataset.parquet\"\n",
    "        out_path = os.path.join(self.save_dir, filename)\n",
    "        df = self.df_base.copy()\n",
    "        if 'datetime' in df.columns:\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "        try:\n",
    "            if partition_by:\n",
    "                df.to_parquet(out_path, engine='pyarrow', compression=comp, index=False, partition_cols=partition_by)\n",
    "            else:\n",
    "                df.to_parquet(out_path, engine='pyarrow', compression=comp, index=False)\n",
    "            print(f\"[SALVO] df_base: {len(df):,} linhas ‚Üí {out_path}\")\n",
    "            return out_path\n",
    "        except Exception as e:\n",
    "            print(f\"Falha ao salvar df_base em {out_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def split_train_val_test(self, train_size: float = 0.7, val_size: float = 0.15, test_size: float = 0.15, time_col: str = 'datetime') -> Optional[dict]:\n",
    "        \"\"\"Divide df_base em conjuntos de treino, valida√ß√£o e teste com base em time_col.\"\"\"\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            print(\"df_base vazio. Nada para dividir.\")\n",
    "            return None\n",
    "        if not np.isclose(train_size + val_size + test_size, 1.0):\n",
    "            print(\"train_size, val_size e test_size devem somar 1.0\")\n",
    "            return None\n",
    "        df = self.df_base.copy()\n",
    "        if time_col not in df.columns:\n",
    "            print(f\"Coluna de tempo '{time_col}' n√£o encontrada em df_base.\")\n",
    "            return None\n",
    "        df = df.sort_values(time_col).reset_index(drop=True)\n",
    "        n = len(df)\n",
    "        train_end = int(n * train_size)\n",
    "        val_end = train_end + int(n * val_size)\n",
    "        splits = {\n",
    "            'train': df.iloc[:train_end].reset_index(drop=True),\n",
    "            'val': df.iloc[train_end:val_end].reset_index(drop=True),\n",
    "            'test': df.iloc[val_end:].reset_index(drop=True),\n",
    "        }\n",
    "        for split_name, split_df in splits.items():\n",
    "            print(f\"[DIVIDIDO] {split_name}: {len(split_df):,} linhas\")\n",
    "        self.splits = splits\n",
    "        return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa7c121",
   "metadata": {},
   "source": [
    "## Preprocessamento do Modelo Linear\n",
    "\n",
    "Esse modelo deve ser√° contruido a partir de lags e leads passados como par√¢metros na fun√ß√£o, resultando na contru√ß√£o de novas colunas lead lag, assim gerando uma flat matrix 2D que ser√° usada no modelo linear\n",
    "\n",
    "Observa√ß√£o importante: lag e lead s√£o inteiros e representam o m√°ximo de passos; o pipeline expande para intervalos 1..N automaticamente. Por exemplo, lag=96 gera features com defasagens de 1 a 96; lead=96 gera alvos de 1 a 96.\n",
    "\n",
    "Os arquivos do modelo ser√£o salvos em TFrecords j√° que o modelo linear ser√° contru√≠do usando tensor flow\n",
    "\n",
    "No caso o Preprocessador do modelo linear ser√° igual ao pr√©-processador do MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dd3ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearPreprocessor(Preprocessor):\n",
    "    \"\"\"Pr√©-processador linear: gera matriz flat (lags/leads), exporta Parquet e TFRecords.\"\"\"\n",
    "\n",
    "    def build_flat_matrix(\n",
    "        self,\n",
    "        value_cols: Optional[List[str]] = None,\n",
    "        target_cols: Optional[List[str]] = None,\n",
    "        lags: Optional[int] = None,\n",
    "        leads: Optional[int] = None,\n",
    "        reduced_dim: Optional[int] = None,\n",
    "        mask_value: float = 0.0,\n",
    "        dropna: bool = True,\n",
    "        group_cols: Optional[List[str]] = None,\n",
    "        time_col: str = \"datetime\",\n",
    "    ) -> pd.DataFrame:\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            print(\"df_base vazio. Chame load_data() primeiro.\")\n",
    "            return self.df_base\n",
    "\n",
    "        df = self.df_base.copy()\n",
    "        feats = value_cols or self.feature_cols\n",
    "        tgts = target_cols or self.target_cols\n",
    "        if not feats:\n",
    "            raise ValueError(\"Nenhuma coluna de feature informada.\")\n",
    "        if not tgts:\n",
    "            raise ValueError(\"Nenhum target informado.\")\n",
    "\n",
    "        group_cols = group_cols or [c for c in [\"country\"] if c in df.columns]\n",
    "        if time_col not in df.columns:\n",
    "            raise ValueError(f\"Coluna temporal '{time_col}' n√£o encontrada no DataFrame.\")\n",
    "\n",
    "        # Ordena\n",
    "        sort_cols = (group_cols or []) + [time_col]\n",
    "        df = df.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "        if group_cols:\n",
    "            df[\"_group_id\"] = df[group_cols].astype(str).agg(\"_\".join, axis=1)\n",
    "        else:\n",
    "            df[\"_group_id\"] = \"global\"\n",
    "\n",
    "        lag_steps = list(range(1, (lags or self.lag or 0) + 1))\n",
    "        lead_steps = list(range(1, (leads or self.lead or 0) + 1))\n",
    "        new_cols = []\n",
    "\n",
    "        # Determina redu√ß√£o\n",
    "        if reduced_dim is not None and reduced_dim < len(lag_steps):\n",
    "            active_lags = lag_steps[:reduced_dim]\n",
    "            padded_lags = lag_steps[reduced_dim:]\n",
    "            print(f\"[‚ÑπÔ∏è] Reduzindo lags: usando {len(active_lags)} e mascarando {len(padded_lags)} restantes com {mask_value}\")\n",
    "        else:\n",
    "            active_lags = lag_steps\n",
    "            padded_lags = []\n",
    "\n",
    "        # ---- Lags ----\n",
    "        for col in feats:\n",
    "            if col not in df.columns:\n",
    "                print(f\"[WARN] Coluna de feature '{col}' n√£o encontrada.\")\n",
    "                continue\n",
    "            # Lags ativos\n",
    "            for k in active_lags:\n",
    "                cname = f\"{col}_lag{k}\"\n",
    "                df[cname] = df.groupby(\"_group_id\", group_keys=False, sort=False)[col].shift(k)\n",
    "                new_cols.append(cname)\n",
    "            # Lags mascarados (padding)\n",
    "            for k in padded_lags:\n",
    "                cname = f\"{col}_lag{k}\"\n",
    "                df[cname] = mask_value\n",
    "                new_cols.append(cname)\n",
    "\n",
    "        # ---- Leads ----\n",
    "        for tgt in tgts:\n",
    "            if tgt in df.columns:\n",
    "                for k in lead_steps:\n",
    "                    cname = f\"{tgt}_lead{k}\"\n",
    "                    df[cname] = df.groupby(\"_group_id\", group_keys=False, sort=False)[tgt].shift(-k)\n",
    "                    new_cols.append(cname)\n",
    "            else:\n",
    "                print(f\"[WARN] Target '{tgt}' n√£o encontrado. Ignorando leads.\")\n",
    "\n",
    "        # ---- Drop NA ----\n",
    "        if dropna and new_cols:\n",
    "            df = df.dropna(subset=[c for c in new_cols if mask_value not in df[c].unique()]).reset_index(drop=True)\n",
    "\n",
    "        df.drop(columns=[\"_group_id\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "        # Atualiza atributos\n",
    "        self.df_base = df\n",
    "        self.feature_cols.extend([c for c in new_cols if \"_lag\" in c and c not in self.feature_cols])\n",
    "        self.target_cols.extend([c for c in new_cols if \"_lead\" in c and c not in self.target_cols])\n",
    "\n",
    "        return self.df_base\n",
    "\n",
    "    def build_flat_matrices_splits(self, *args, **kwargs) -> Optional[dict]:\n",
    "        \"\"\"Constr√≥i matrizes flat para cada split (train/val/test).\"\"\"\n",
    "        if not self.splits:\n",
    "            print(\"Nenhum conjunto dividido encontrado.\")\n",
    "            return None\n",
    "        built_splits = {}\n",
    "        for split_name, split_df in self.splits.items():\n",
    "            self.df_base = split_df\n",
    "            built_df = self.build_flat_matrix(*args, **kwargs)\n",
    "            built_splits[split_name] = built_df\n",
    "        self.splits = built_splits\n",
    "        return built_splits\n",
    "\n",
    "    def save_tfrecords(\n",
    "        self,\n",
    "        output_basename: str = 'dataset',\n",
    "        shard_size: int = 100_000,\n",
    "        compression: Optional[str] = None,\n",
    "    ) -> Optional[List[str]]:\n",
    "        import os, json\n",
    "        import pandas as pd\n",
    "        import tensorflow as tf\n",
    "\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            print(\"df_base vazio. Nada para salvar em TFRecords.\")\n",
    "            return None\n",
    "\n",
    "        # --- Salvar Parquet intermedi√°rio ---\n",
    "        parquet_path = os.path.join(self.save_dir, f\"{output_basename}.parquet\")\n",
    "        try:\n",
    "            self.df_base.to_parquet(parquet_path, index=False)\n",
    "            print(f\"[üíæ] Parquet salvo em: {parquet_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Falha ao salvar Parquet: {e}\")\n",
    "\n",
    "        # --- Sele√ß√£o de colunas num√©ricas ---\n",
    "        numeric_cols = self.df_base.select_dtypes(include=[\"number\", \"bool\"]).columns\n",
    "        present_feats = [c for c in self.feature_cols if c in numeric_cols]\n",
    "        present_tgts = [c for c in self.target_cols if c in numeric_cols]\n",
    "        if not present_feats or not present_tgts:\n",
    "            print(\"Nenhuma feature/target v√°lida encontrada. Abortando export.\")\n",
    "            return None\n",
    "        \n",
    "        self.feature_cols = present_feats\n",
    "        self.target_cols = present_tgts\n",
    "\n",
    "        df = self.df_base.reset_index(drop=True)\n",
    "        X = df[present_feats].astype('float32').to_numpy(copy=False)\n",
    "        y = df[present_tgts].astype('float32').to_numpy(copy=False)\n",
    "        n = len(df)\n",
    "        x_dim, y_dim = X.shape[1], y.shape[1]\n",
    "\n",
    "        comp = compression or 'GZIP'\n",
    "        options = tf.io.TFRecordOptions(compression_type=comp)\n",
    "        paths: List[str] = []\n",
    "\n",
    "        def _float_feature(v):\n",
    "            return tf.train.Feature(float_list=tf.train.FloatList(value=v))\n",
    "\n",
    "        for shard_idx, start in enumerate(range(0, n, shard_size)):\n",
    "            end = min(start + shard_size, n)\n",
    "            shard_path = os.path.join(self.save_dir, f\"{output_basename}_{shard_idx}.tfrecord\")\n",
    "            with tf.io.TFRecordWriter(shard_path, options=options) as w:\n",
    "                for i in range(start, end):\n",
    "                    ex = tf.train.Example(features=tf.train.Features(feature={\n",
    "                        'x': _float_feature(X[i]),\n",
    "                        'y': _float_feature(y[i]),\n",
    "                    }))\n",
    "                    w.write(ex.SerializeToString())\n",
    "            paths.append(shard_path)\n",
    "\n",
    "        # --- Metadados ---\n",
    "        meta = {\n",
    "            'x_dim': int(x_dim),\n",
    "            'y_dim': int(y_dim),\n",
    "            'feature_cols': present_feats,\n",
    "            'target_cols': present_tgts,\n",
    "            'count': int(n),\n",
    "            'compression': comp,\n",
    "            'basename': output_basename,\n",
    "            'parquet_path': parquet_path,\n",
    "        }\n",
    "        try:\n",
    "            with open(os.path.join(self.save_dir, f\"{output_basename}.meta.json\"), 'w', encoding='utf-8') as f:\n",
    "                json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Falha ao salvar metadados: {e}\")\n",
    "\n",
    "        print(f\"[‚úÖ] TFRecords salvos ({len(paths)} shards) + Parquet em {self.save_dir}\")\n",
    "        return paths\n",
    "\n",
    "    def save_splits_tfrecords(\n",
    "        self,\n",
    "        output_basename: str = 'dataset',\n",
    "        shard_size: int = 100_000,\n",
    "        compression: Optional[str] = None,\n",
    "    ) -> Optional[dict]:\n",
    "        \"\"\"Salva TFRecords e Parquet para cada split (train/val/test).\"\"\"\n",
    "        if not self.splits:\n",
    "            print(\"Nenhum conjunto dividido encontrado.\")\n",
    "            return None\n",
    "        paths_dict = {}\n",
    "        for split_name, split_df in self.splits.items():\n",
    "            self.df_base = split_df\n",
    "            paths = self.save_tfrecords(\n",
    "                output_basename=f\"{output_basename}_{split_name}\",\n",
    "                shard_size=shard_size,\n",
    "                compression=compression,\n",
    "            )\n",
    "            paths_dict[split_name] = paths\n",
    "        return paths_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e82e82",
   "metadata": {},
   "source": [
    "## Preprocessamento do Modelo LSTM\n",
    "\n",
    "O preprocessador do LSTM deve ser capaz de gerar tensores de dimens√£o 3, no seguinte formato (n_batch, seq_len, features) e (n_batch, seq_len, features)  para alimenta√ß√£o do modelo e valida√ß√£o das m√©tricas do modelo\n",
    "\n",
    "Os arquivos do modelo ser√£o salvos em TFrecords j√° que o modelo linear ser√° contru√≠do usando tensor flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42af4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "\n",
    "class LSTMPreprocessor(Preprocessor):\n",
    "    \"\"\"Pr√©-processador sequencial para LSTM: gera janelas 3D (N, seq_len, features) e Y (N, lead, targets).\"\"\"\n",
    "\n",
    "    # =====================================================\n",
    "    # BUILD MATRIX\n",
    "    # =====================================================\n",
    "    def build_sequence_matrix(\n",
    "        self,\n",
    "        value_cols: Optional[List[str]] = None,\n",
    "        target_cols: Optional[List[str]] = None,\n",
    "        seq_len: Optional[int] = None,\n",
    "        lead: Optional[int] = None,\n",
    "        group_cols: Optional[List[str]] = None,\n",
    "        time_col: str = \"datetime\",\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Constr√≥i tensores X (entradas) e Y (alvos) para modelo LSTM multivariado.\n",
    "        Agora suporta m√∫ltiplos passos de previs√£o (lead > 1).\n",
    "        \"\"\"\n",
    "        if self.df_base is None or self.df_base.empty:\n",
    "            raise ValueError(\"df_base vazio. Chame load_data() primeiro.\")\n",
    "\n",
    "        df = self.df_base.copy()\n",
    "        feats = value_cols or self.feature_cols\n",
    "        tgts = target_cols or self.target_cols\n",
    "        if not feats:\n",
    "            raise ValueError(\"Nenhuma coluna de feature informada.\")\n",
    "        if not tgts:\n",
    "            raise ValueError(\"Nenhum target informado.\")\n",
    "        if time_col not in df.columns:\n",
    "            raise ValueError(f\"Coluna temporal '{time_col}' n√£o encontrada.\")\n",
    "\n",
    "        group_cols = group_cols or [c for c in [\"country\"] if c in df.columns]\n",
    "        sort_cols = (group_cols or []) + [time_col]\n",
    "        df = df.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "        if group_cols:\n",
    "            df[\"_group_id\"] = df[group_cols].astype(str).agg(\"_\".join, axis=1)\n",
    "        else:\n",
    "            df[\"_group_id\"] = \"global\"\n",
    "\n",
    "        seq_len = seq_len or getattr(self, \"seq_len\", 24)\n",
    "        lead = lead or getattr(self, \"lead\", 1)\n",
    "\n",
    "        X_list, Y_list = [], []\n",
    "        for gid, g in df.groupby(\"_group_id\", sort=False):\n",
    "            g = g.reset_index(drop=True)\n",
    "            if len(g) < seq_len + lead:\n",
    "                continue\n",
    "\n",
    "            X_src = g[feats].to_numpy(np.float32)\n",
    "            Y_src = g[tgts].to_numpy(np.float32)\n",
    "\n",
    "            # cria janelas deslizantes\n",
    "            for i in range(len(g) - seq_len - lead + 1):\n",
    "                x_win = X_src[i : i + seq_len]\n",
    "                y_seq = Y_src[i + seq_len : i + seq_len + lead]  # <--- multi-step\n",
    "                X_list.append(x_win)\n",
    "                Y_list.append(y_seq)\n",
    "\n",
    "        if not X_list:\n",
    "            print(\"[WARN] Nenhuma janela gerada.\")\n",
    "            return {}\n",
    "\n",
    "        X = np.stack(X_list)  # (N, seq_len, x_dim)\n",
    "        Y = np.stack(Y_list)  # (N, lead, y_dim)\n",
    "        print(f\"[JANELAS] X={X.shape}, Y={Y.shape}, seq_len={seq_len}, lead={lead}\")\n",
    "        self._seq_data = dict(\n",
    "            X=X, Y=Y, seq_len=seq_len, lead=lead, x_dim=X.shape[-1], y_dim=Y.shape[-1]\n",
    "        )\n",
    "        return self._seq_data\n",
    "\n",
    "    # =====================================================\n",
    "    # SAVE TFRECORDS\n",
    "    # =====================================================\n",
    "    def save_sequence_tfrecords(\n",
    "        self,\n",
    "        output_basename: str = \"lstm_dataset\",\n",
    "        shard_size: int = 50_000,\n",
    "        compression: str = \"GZIP\",\n",
    "    ) -> Optional[List[str]]:\n",
    "        \"\"\"Salva janelas (X,Y) como TFRecords comprimidos.\"\"\"\n",
    "        if not hasattr(self, \"_seq_data\"):\n",
    "            print(\"Nenhum dado sequencial encontrado. Execute build_sequence_matrix() antes.\")\n",
    "            return None\n",
    "\n",
    "        X, Y = self._seq_data[\"X\"], self._seq_data[\"Y\"]\n",
    "        seq_len, x_dim, lead, y_dim = (\n",
    "            self._seq_data[\"seq_len\"],\n",
    "            self._seq_data[\"x_dim\"],\n",
    "            self._seq_data[\"lead\"],\n",
    "            self._seq_data[\"y_dim\"],\n",
    "        )\n",
    "        n = len(X)\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "        options = tf.io.TFRecordOptions(compression_type=compression)\n",
    "        paths = []\n",
    "\n",
    "        def _bytes_feature(arr: np.ndarray) -> tf.train.Feature:\n",
    "            return tf.train.Feature(bytes_list=tf.train.BytesList(value=[arr.tobytes()]))\n",
    "\n",
    "        for shard_idx, start in enumerate(range(0, n, shard_size)):\n",
    "            end = min(start + shard_size, n)\n",
    "            shard_path = os.path.join(self.save_dir, f\"{output_basename}_{shard_idx}.tfrecord\")\n",
    "            with tf.io.TFRecordWriter(shard_path, options=options) as w:\n",
    "                for i in range(start, end):\n",
    "                    ex = tf.train.Example(\n",
    "                        features=tf.train.Features(\n",
    "                            feature={\n",
    "                                \"x_raw\": _bytes_feature(X[i]),\n",
    "                                \"y_raw\": _bytes_feature(Y[i]),\n",
    "                            }\n",
    "                        )\n",
    "                    )\n",
    "                    w.write(ex.SerializeToString())\n",
    "            paths.append(shard_path)\n",
    "\n",
    "        meta = {\n",
    "            \"seq_len\": seq_len,\n",
    "            \"lead\": lead,\n",
    "            \"x_dim\": x_dim,\n",
    "            \"y_dim\": y_dim,\n",
    "            \"compression\": compression,\n",
    "            \"count\": int(n),\n",
    "            \"basename\": output_basename,\n",
    "        }\n",
    "        with open(os.path.join(self.save_dir, f\"{output_basename}.meta.json\"), \"w\") as f:\n",
    "            json.dump(meta, f, indent=2)\n",
    "\n",
    "        print(f\"[‚úÖ] TFRecords salvos ({len(paths)} shards) em {self.save_dir} ‚Äî lead={lead}\")\n",
    "        return paths\n",
    "\n",
    "    # =====================================================\n",
    "    # PARSE + LOAD DATASETS\n",
    "    # =====================================================\n",
    "    @staticmethod\n",
    "    def parse_tfrecord(example_proto, seq_len: int, x_dim: int, lead: int, y_dim: int):\n",
    "        \"\"\"Fun√ß√£o para leitura dos TFRecords salvos (mant√©m formato 3D para Y).\"\"\"\n",
    "        features = {\n",
    "            \"x_raw\": tf.io.FixedLenFeature([], tf.string),\n",
    "            \"y_raw\": tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "        parsed = tf.io.parse_single_example(example_proto, features)\n",
    "        x = tf.io.decode_raw(parsed[\"x_raw\"], tf.float32)\n",
    "        y = tf.io.decode_raw(parsed[\"y_raw\"], tf.float32)\n",
    "        x = tf.reshape(x, [seq_len, x_dim])\n",
    "        y = tf.reshape(y, [lead, y_dim])  # <--- fixado\n",
    "        return x, y\n",
    "\n",
    "    @staticmethod\n",
    "    def load_sequence_dataset(\n",
    "        path_pattern: str,\n",
    "        seq_len: Optional[int] = None,\n",
    "        x_dim: Optional[int] = None,\n",
    "        y_dim: Optional[int] = None,\n",
    "        lead: Optional[int] = None,\n",
    "        batch_size: int = 256,\n",
    "        compression: str = \"GZIP\",\n",
    "        meta_path: Optional[str] = None,\n",
    "        return_meta: bool = False,\n",
    "    ) -> tf.data.Dataset | tuple:\n",
    "        \"\"\"Carrega os TFRecords como dataset pronto para treino.\"\"\"\n",
    "        files = tf.io.gfile.glob(path_pattern)\n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"Nenhum arquivo encontrado para padr√£o: {path_pattern}\")\n",
    "\n",
    "        # --- Meta resolution --- #\n",
    "        meta = None\n",
    "        if meta_path is None and files:\n",
    "            guess = re.sub(r\"_[0-9]+\\.tfrecord$\", \".meta.json\", files[0])\n",
    "            if tf.io.gfile.exists(guess):\n",
    "                meta_path = guess\n",
    "        if meta_path and tf.io.gfile.exists(meta_path):\n",
    "            with tf.io.gfile.GFile(meta_path, \"r\") as f:\n",
    "                meta = json.load(f)\n",
    "\n",
    "        if meta is not None:\n",
    "            seq_len = seq_len or int(meta.get(\"seq_len\"))\n",
    "            lead = lead or int(meta.get(\"lead\"))\n",
    "            x_dim = x_dim or int(meta.get(\"x_dim\"))\n",
    "            y_dim = y_dim or int(meta.get(\"y_dim\"))\n",
    "\n",
    "        if None in [seq_len, x_dim, y_dim, lead]:\n",
    "            raise ValueError(\"seq_len/x_dim/y_dim/lead ausentes e meta.json incompleto.\")\n",
    "\n",
    "        ds = tf.data.TFRecordDataset(files, compression_type=compression)\n",
    "        ds = ds.map(\n",
    "            lambda ex: LSTMPreprocessor.parse_tfrecord(ex, seq_len, x_dim, lead, y_dim),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE,\n",
    "        )\n",
    "        ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        print(f\"[DATASET] {len(files)} shards ‚Üí batch_size={batch_size} (seq_len={seq_len}, lead={lead})\")\n",
    "\n",
    "        if return_meta:\n",
    "            return ds, meta\n",
    "        return ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd309dd",
   "metadata": {},
   "source": [
    "## Preprocessamento do Modelo TFT (PyTorch)\n",
    "\n",
    "O preprocessador do LSTM deve ser capaz de gerar tensores de dimens√£o 3, no seguinte formato (n_batch, seq_len, features) e (n_batch, seq_len, features)  para alimenta√ß√£o do modelo e valida√ß√£o das m√©tricas do modelo\n",
    "\n",
    "Os arquivos do modelo ser√£o salvos em TFrecords j√° que o modelo linear ser√° contru√≠do usando tensor flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0a89f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "class TFTPreprocessor(Preprocessor):\n",
    "    \"\"\"\n",
    "    Preprocessador espec√≠fico para o modelo Temporal Fusion Transformer (PyTorch Forecasting).\n",
    "    Herdando de Preprocessor, apenas adiciona a etapa final de estrutura√ß√£o e salvamento\n",
    "    dos splits no formato compat√≠vel com o PyTorch Forecasting.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        model_name: str,\n",
    "        feature_cols: List[str],\n",
    "        target_cols: List[str],\n",
    "        country_list: List[str],\n",
    "        seq_len: int,\n",
    "        lead: int,\n",
    "    ):\n",
    "        # Corrigido: alinhar com assinatura de Preprocessor\n",
    "        super().__init__(\n",
    "            lag=seq_len,\n",
    "            lead=lead,\n",
    "            country_list=country_list,\n",
    "            model_name=model_name,\n",
    "            data_dir=data_dir,\n",
    "            feature_cols=feature_cols,\n",
    "            target_cols=target_cols,\n",
    "        )\n",
    "        self.seq_len = seq_len\n",
    "        self.lead = lead\n",
    "\n",
    "\n",
    "    def build_tft_parquets(\n",
    "        self,\n",
    "        group_cols: Optional[List[str]] = [\"country\"],\n",
    "        time_col: str = \"datetime\",\n",
    "        dropna: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Estrutura os splits existentes (j√° criados na classe-base) para uso no TFT e salva em parquet.\n",
    "        Simples e direto:\n",
    "        - Ordena por (group_cols + time_col)\n",
    "        - Opcionalmente remove nulos nas colunas cr√≠ticas [time_col] + group_cols + target_cols\n",
    "        - Define '_group_id' e calcula 'time_idx' por grupo via cumcount() (0..N-1 por s√©rie)\n",
    "        - Salva parquet por split\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"splits\") or not self.splits:\n",
    "            raise ValueError(\"Os splits ainda n√£o foram gerados. Execute split_train_val_test() primeiro.\")\n",
    "\n",
    "        for name, df in self.splits.items():\n",
    "            df = df.copy()\n",
    "            # tipos e ordena√ß√£o\n",
    "            df[time_col] = pd.to_datetime(df[time_col], utc=True)\n",
    "            sort_cols = (group_cols or []) + [time_col]\n",
    "            df = df.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "            # drop nulos b√°sico\n",
    "            if dropna:\n",
    "                subset_cols = ([time_col] if time_col else []) + (group_cols or []) + (self.target_cols or [])\n",
    "                present = [c for c in subset_cols if c in df.columns]\n",
    "                before = len(df)\n",
    "                df = df.dropna(subset=present).reset_index(drop=True)\n",
    "                if before - len(df) > 0:\n",
    "                    print(f\"üßπ Drop NA ({name}): {before - len(df)} linhas removidas nas colunas {present}.\")\n",
    "\n",
    "            # id de grupo e time_idx por grupo\n",
    "            if group_cols:\n",
    "                df[\"_group_id\"] = df[group_cols].astype(str).agg(\"_\".join, axis=1)\n",
    "            else:\n",
    "                df[\"_group_id\"] = \"global\"\n",
    "\n",
    "            # contador sequencial por grupo (n√£o global)\n",
    "            df[\"time_idx\"] = df.groupby(\"_group_id\").cumcount().astype(\"int64\")\n",
    "\n",
    "            # salvar parquet\n",
    "            path = os.path.join(self.data_dir, f\"tft_dataset_{name}.parquet\")\n",
    "            df.to_parquet(path, index=False)\n",
    "            print(f\"üíæ Split '{name}' salvo em {path} ({df.shape[0]} linhas, grupos={df['_group_id'].nunique()}, max local time_idx={df.groupby('_group_id')['time_idx'].max().max()}).\")\n",
    "\n",
    "\n",
    "    def load_tft_dataset(\n",
    "        self,\n",
    "        split_name: str,\n",
    "        target_col: str,\n",
    "        known_reals: Optional[List[str]] = None,\n",
    "        return_df: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Carrega o parquet salvo como DataFrame ou cria um TimeSeriesDataSet compat√≠vel com o TFT PyTorch.\n",
    "\n",
    "        Args:\n",
    "            split_name: 'train' | 'val' | 'test' (parte do nome do arquivo parquet gerado)\n",
    "            target_col: coluna alvo principal (string)\n",
    "            known_reals: lista de features conhecidas no tempo (overrides self.feature_cols quando fornecida)\n",
    "            return_df: se True retorna o DataFrame bruto em vez do TimeSeriesDataSet\n",
    "\n",
    "        Retorna:\n",
    "            DataFrame (quando return_df=True) ou TimeSeriesDataSet\n",
    "        \"\"\"\n",
    "        path = os.path.join(self.data_dir, f\"tft_dataset_{split_name}.parquet\")\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"Arquivo n√£o encontrado: {path}\")\n",
    "\n",
    "        df = pd.read_parquet(path)\n",
    "\n",
    "        if return_df:\n",
    "            print(f\"üì• Parquet '{split_name}' carregado ({len(df)} linhas) ‚Äî retornando DataFrame.\")\n",
    "            return df\n",
    "\n",
    "        # determina known/unknown reals\n",
    "        known_reals = known_reals or [c for c in (self.feature_cols or []) if c not in (self.target_cols or [])]\n",
    "\n",
    "        ds = TimeSeriesDataSet(\n",
    "            df,\n",
    "            time_idx=\"time_idx\",\n",
    "            target=target_col,\n",
    "            group_ids=[\"_group_id\"],\n",
    "            max_encoder_length=self.seq_len,\n",
    "            max_prediction_length=self.lead,\n",
    "            time_varying_known_reals=known_reals,\n",
    "            time_varying_unknown_reals=self.target_cols,\n",
    "            add_relative_time_idx=True,\n",
    "            add_target_scales=True,\n",
    "            add_encoder_length=True,\n",
    "        )\n",
    "        print(f\"üì¶ TimeSeriesDataSet ({split_name}) criado com {len(df)} amostras.\")\n",
    "        return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8ece38",
   "metadata": {},
   "source": [
    "# Cap√≠tulo 2 ‚Äî Constru√ß√£o dos Modelos\n",
    "\n",
    "A seguir, definimos construtores simples e eficientes para cada modelo (Linear, LSTM, TFT e TimesFM),\n",
    "prontos para uso em rotinas de otimiza√ß√£o de hiperpar√¢metros (por exemplo, Optuna). Cada construtor\n",
    "recebe um dicion√°rio de par√¢metros (`params`) e retorna um modelo compilado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556d5525",
   "metadata": {},
   "source": [
    "## Constru√ß√£o do Modelo Linear/MLP\n",
    "\n",
    "Objetivo: Criar um regressor simples (MLP), com capacidade de redu√ß√£o para um modelo apenas lienar - pela exclus√£o da camada de ativa√ß√£o - para prever `target_cols` a partir de `feature_cols`.\n",
    "\n",
    "Contrato r√°pido:\n",
    "- Entrada: vetor de tamanho `x_dim` (n√∫mero de features)\n",
    "- Sa√≠da: vetor de tamanho `y_dim` (n√∫mero de targets)\n",
    "- Par√¢metros (exemplos): hidden_units, activation, dropout, lr, l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727619dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "def build_linear_model(x_dim: int, y_dim: int, params: Dict[str, Any], linear: bool = False) -> keras.Model:\n",
    "    \"\"\"\n",
    "    Constr√≥i um modelo linear puro ou um MLP, dependendo do argumento `linear`.\n",
    "\n",
    "    params:\n",
    "      - hidden_units: List[int] (apenas usado se linear=False)\n",
    "      - activation: str (apenas usado se linear=False)\n",
    "      - dropout: float (0..1)\n",
    "      - l2: float (regulariza√ß√£o L2)\n",
    "      - lr: float (learning rate)\n",
    "    \"\"\"\n",
    "    hidden_units = params.get('hidden_units', [128, 64])\n",
    "    activation = params.get('activation', 'relu')\n",
    "    dropout = float(params.get('dropout', 0.0))\n",
    "    l2 = float(params.get('l2', 0.0))\n",
    "    lr = float(params.get('lr', 1e-3))\n",
    "    mask_value = params.get('mask_value', -999.0)\n",
    "\n",
    "    inputs = keras.Input(shape=(x_dim,), name='features')\n",
    "\n",
    "    x = layers.Masking(mask_value=mask_value, name='masking')(inputs)\n",
    "\n",
    "    if linear:\n",
    "        # Modelo puramente linear (sem ativa√ß√£o)\n",
    "        outputs = layers.Dense(\n",
    "            y_dim,\n",
    "            activation=None,\n",
    "            kernel_regularizer=keras.regularizers.l2(l2),\n",
    "            name='linear_output'\n",
    "        )(inputs)\n",
    "        model = keras.Model(inputs, outputs, name='linear_model_true')\n",
    "\n",
    "    else:\n",
    "        # Modelo MLP (n√£o linear)\n",
    "        x = inputs\n",
    "        for i, units in enumerate(hidden_units):\n",
    "            x = layers.Dense(\n",
    "                units,\n",
    "                activation=activation,\n",
    "                kernel_regularizer=keras.regularizers.l2(l2),\n",
    "                name=f'dense_{i}'\n",
    "            )(x)\n",
    "            if dropout > 0:\n",
    "                x = layers.Dropout(dropout, name=f'dropout_{i}')(x)\n",
    "        outputs = layers.Dense(y_dim, name='targets')(x)\n",
    "        model = keras.Model(inputs, outputs, name='mlp_model')\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# Fun√ß√µes auxiliares para carregar TFRecords\n",
    "def parse_tfrecord(example_proto, x_dim, y_dim):\n",
    "    feature_description = {\n",
    "        'x': tf.io.VarLenFeature(tf.float32),\n",
    "        'y': tf.io.VarLenFeature(tf.float32),\n",
    "    }\n",
    "    parsed = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    x = tf.sparse.to_dense(parsed['x'])\n",
    "    y = tf.sparse.to_dense(parsed['y'])\n",
    "    x = tf.reshape(x, [x_dim])\n",
    "    y = tf.reshape(y, [y_dim])\n",
    "    return x, y\n",
    "\n",
    "def load_tfrecord_dataset(\n",
    "    path_pattern: str,\n",
    "    x_dim: int | None = None,\n",
    "    y_dim: int | None = None,\n",
    "    batch_size: int = 64,\n",
    "    compression: str = 'GZIP',\n",
    "    meta_path: str | None = None,\n",
    "    return_meta: bool = False,\n",
    "):\n",
    "    import re, json\n",
    "    import tensorflow as tf\n",
    "\n",
    "    files = tf.io.gfile.glob(path_pattern)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"Nenhum TFRecord encontrado para o padr√£o: {path_pattern}\")\n",
    "\n",
    "    meta = None\n",
    "    # Tenta deduzir o caminho do meta.json a partir do primeiro shard\n",
    "    if meta_path is None:\n",
    "        guess = re.sub(r'_[0-9]+\\\\.tfrecord$', '.meta.json', files[0])\n",
    "        if tf.io.gfile.exists(guess):\n",
    "            meta_path = guess\n",
    "    if meta_path and tf.io.gfile.exists(meta_path):\n",
    "        try:\n",
    "            with tf.io.gfile.GFile(meta_path, 'r') as f:\n",
    "                meta = json.load(f)\n",
    "        except Exception:\n",
    "            meta = None\n",
    "\n",
    "    # Resolve dimens√µes a partir do meta quando n√£o fornecidas\n",
    "    if x_dim is None and meta is not None:\n",
    "        x_dim = int(meta.get('x_dim')) if meta.get('x_dim') is not None else None\n",
    "    if y_dim is None and meta is not None:\n",
    "        y_dim = int(meta.get('y_dim')) if meta.get('y_dim') is not None else None\n",
    "\n",
    "    if x_dim is None or y_dim is None:\n",
    "        raise ValueError(\"x_dim/y_dim n√£o definidos e meta.json ausente ou incompleto.\")\n",
    "\n",
    "    ds = tf.data.TFRecordDataset(files, compression_type=compression)\n",
    "    ds = ds.map(lambda ex: parse_tfrecord(ex, x_dim, y_dim),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # Mensagem informativa\n",
    "    if meta is not None:\n",
    "        seq_len = meta.get('seq_len')  # Pode existir em alguns formatos de meta\n",
    "        if seq_len is not None:\n",
    "            print(f\"[DATASET] {len(files)} shards ‚Üí batch_size={batch_size} (x_dim={x_dim}, y_dim={y_dim}, seq_len={seq_len})\")\n",
    "        else:\n",
    "            print(f\"[DATASET] {len(files)} shards ‚Üí batch_size={batch_size} (x_dim={x_dim}, y_dim={y_dim})\")\n",
    "    else:\n",
    "        print(f\"[DATASET] {len(files)} shards carregados ‚Üí batch_size={batch_size}\")\n",
    "\n",
    "    if return_meta:\n",
    "        return ds, (meta or {})\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0c1b3c",
   "metadata": {},
   "source": [
    "## Constru√ß√£o do Modelo LSTM\n",
    "\n",
    "Objetivo: um regressor denso simples (MLP) para prever `target_cols` a partir de `feature_cols`.\n",
    "\n",
    "Contrato r√°pido:\n",
    "- Entrada: vetor de tamanho `x_dim` (n√∫mero de features)\n",
    "- Sa√≠da: vetor de tamanho `y_dim` (n√∫mero de targets)\n",
    "- Par√¢metros (exemplos): hidden_units, activation, dropout, lr, l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b6c091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "def build_lstm_model(seq_len: int, x_dim: int, y_dim: int, params: Dict[str, Any]) -> keras.Model:\n",
    "    \"\"\"\n",
    "    LSTM para regress√£o multivariada temporal com suporte a m√°scara para valores nulos/padding.\n",
    "\n",
    "    - As entradas devem conter `NaN` ou um valor sentinel (ex.: 0.0) para timesteps a mascarar.\n",
    "    - Camadas LSTM automaticamente ignoram esses timesteps durante o treinamento.\n",
    "    \"\"\"\n",
    "\n",
    "    lstm_units = params.get('lstm_units', [128, 64])\n",
    "    dense_units = params.get('dense_units', [128])\n",
    "    dropout = float(params.get('dropout', 0.1))\n",
    "    rec_dropout = float(params.get('rec_dropout', 0.0))\n",
    "    act = params.get('act', 'relu')\n",
    "    lr = float(params.get('lr', 1e-3))\n",
    "    l2 = float(params.get('l2', 0.0))\n",
    "    layer_norm = bool(params.get('layer_norm', True))\n",
    "    mask_value = float(params.get('mask_value', 0.0))  # sentinel for masking\n",
    "\n",
    "    # --- Inputs & Mask ---\n",
    "    inputs = keras.Input(shape=(None, x_dim), name='sequence_input')\n",
    "\n",
    "    # Replace NaNs safely within a Lambda layer\n",
    "    x = layers.Lambda(\n",
    "        lambda v: tf.where(tf.math.is_nan(v), tf.zeros_like(v), v),\n",
    "        output_shape=lambda input_shape: input_shape,\n",
    "        name=\"replace_nans\"\n",
    "    )(inputs)\n",
    "\n",
    "\n",
    "    # --- LSTM stack ---\n",
    "    for i, units in enumerate(lstm_units):\n",
    "        return_seq = i < len(lstm_units) - 1\n",
    "        x = layers.LSTM(\n",
    "            units,\n",
    "            return_sequences=return_seq,\n",
    "            dropout=dropout,\n",
    "            recurrent_dropout=rec_dropout,\n",
    "            kernel_regularizer=keras.regularizers.l2(l2),\n",
    "            name=f'lstm_{i}'\n",
    "        )(x)\n",
    "        if layer_norm:\n",
    "            x = layers.LayerNormalization(name=f'ln_{i}')(x)\n",
    "\n",
    "    # --- Dense layers ---\n",
    "    for i, units in enumerate(dense_units):\n",
    "        x = layers.Dense(units, activation=act, name=f'dense_{i}')(x)\n",
    "        if dropout > 0:\n",
    "            x = layers.Dropout(dropout, name=f'dropout_{i}')(x)\n",
    "\n",
    "    outputs = layers.Dense(y_dim, name='output')(x)\n",
    "\n",
    "    # --- Compile ---\n",
    "    model = keras.Model(inputs, outputs, name='lstm_regressor')\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=lr, clipnorm=1.0)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "    return model\n",
    "def parse_seq_tfrecord(example_proto, seq_len, x_dim, y_dim):\n",
    "    \"\"\"\n",
    "    Faz o parsing de TFRecords com dados 3D salvos em bytes.\n",
    "    Espera features:\n",
    "        'x_raw': sequ√™ncia de entrada (float32 bytes)\n",
    "        'y_raw': target (float32 bytes)\n",
    "    \"\"\"\n",
    "    feature_description = {\n",
    "        'x_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "        'y_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    parsed = tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "    x = tf.io.decode_raw(parsed['x_raw'], tf.float32)\n",
    "    y = tf.io.decode_raw(parsed['y_raw'], tf.float32)\n",
    "\n",
    "    x = tf.reshape(x, [seq_len, x_dim])\n",
    "    y = tf.reshape(y, [y_dim])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def load_seq_tfrecord_dataset(path_pattern, seq_len, x_dim, y_dim, batch_size=64, compression='GZIP'):\n",
    "    \"\"\"\n",
    "    Carrega TFRecords sequenciais e retorna um tf.data.Dataset pronto para treino.\n",
    "\n",
    "    Cada exemplo cont√©m:\n",
    "        X.shape = (seq_len, x_dim)\n",
    "        Y.shape = (y_dim,)\n",
    "    \"\"\"\n",
    "    files = tf.io.gfile.glob(path_pattern)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"Nenhum arquivo TFRecord encontrado em {path_pattern}\")\n",
    "\n",
    "    ds = tf.data.TFRecordDataset(files, compression_type=compression)\n",
    "    ds = ds.map(\n",
    "        lambda ex: parse_seq_tfrecord(ex, seq_len, x_dim, y_dim),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    print(f\"[DATASET] {len(files)} shards carregados | batch_size={batch_size}\")\n",
    "    return ds\n",
    "\n",
    "def save_model(model, path: str):\n",
    "    \"\"\"\n",
    "    Salva um modelo TensorFlow (.keras/.h5) ou PyTorch (.pt/.pth).\n",
    "    Detecta o tipo automaticamente.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    \n",
    "    if isinstance(model, tf.keras.Model):\n",
    "        if not path.endswith(('.keras', '.h5')):\n",
    "            path += \".keras\"\n",
    "        model.save(path)\n",
    "        print(f\"‚úÖ Modelo TensorFlow salvo em: {path}\")\n",
    "        \n",
    "    elif isinstance(model, torch.nn.Module):\n",
    "        if not path.endswith(('.pt', '.pth')):\n",
    "            path += \".pt\"\n",
    "        torch.save(model.state_dict(), path)\n",
    "        print(f\"‚úÖ Modelo PyTorch salvo em: {path}\")\n",
    "        \n",
    "    else:\n",
    "        raise TypeError(\"‚ùå Tipo de modelo n√£o suportado. Deve ser TensorFlow (keras.Model) ou PyTorch (nn.Module).\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b72a517",
   "metadata": {},
   "source": [
    "## Constru√ß√£o do Modelo TFT (Temporal Fusion Transformer)\n",
    "\n",
    "**Objetivo:** prever `target_cols` a partir de `feature_cols` utilizando a implementa√ß√£o oficial `keras_tft`,  \n",
    "que integra **sele√ß√£o de vari√°veis din√¢micas**, **blocos LSTM**, **aten√ß√£o temporal multi-cabe√ßas** e **gating residual** em um √∫nico modelo interpretable.\n",
    "\n",
    "**Contrato r√°pido:**\n",
    "- **Entrada:** `tf.data.Dataset` com tensores no formato `(batch, seq_len, x_dim)`  \n",
    "- **Sa√≠da:** tensor cont√≠nuo de tamanho `y_dim` *(ou `dec_len √ó y_dim` para horizontes m√∫ltiplos)*\n",
    "\n",
    "**Par√¢metros (exemplos):**  \n",
    "`hidden_size` (tamanho interno das camadas GRN) ¬∑ `lstm_layers` ¬∑ `num_heads` (aten√ß√£o) ¬∑ `dropout` ¬∑ `learning_rate` ¬∑ `output_size` ¬∑ `seq_len`\n",
    "\n",
    "**Componentes internos (`keras_tft`):**  \n",
    "Variable Selection Network ‚Üí LSTM Encoder/Decoder ‚Üí Multi-Head Temporal Attention ‚Üí Gated Residual Network ‚Üí Camada de proje√ß√£o final\n",
    "\n",
    "**Compatibilidade:**  \n",
    "Totalmente compat√≠vel com o pipeline atual de TFRecords do LSTM, recebendo o mesmo formato de dados  \n",
    "(`(batch, seq_len, features)`), permitindo substitui√ß√£o direta do modelo sem alterar o pr√©-processamento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce31bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import torch\n",
    "from torch import nn\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "\n",
    "def build_tft_model(\n",
    "    params: Dict[str, Any]\n",
    "):\n",
    "    \"\"\"\n",
    "    Constr√≥i um Temporal Fusion Transformer (TFT) com par√¢metros configur√°veis.\n",
    "\n",
    "    Args:\n",
    "        x_dim: n√∫mero de features de entrada\n",
    "        y_dim: n√∫mero de targets\n",
    "        seq_len: tamanho da sequ√™ncia temporal\n",
    "        params: dicion√°rio de hiperpar√¢metros (hidden_size, dropout, lstm_layers, etc.)\n",
    "        max_encoder_length: tamanho da janela passada (encoder)\n",
    "        max_prediction_length: tamanho do horizonte de previs√£o (decoder)\n",
    "    \"\"\"\n",
    "\n",
    "    from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "    hidden_size = int(params.get(\"hidden_size\", 64))\n",
    "    dropout = float(params.get(\"dropout\", 0.1))\n",
    "    lstm_layers = int(params.get(\"lstm_layers\", 1))\n",
    "    attention_head_size = int(params.get(\"num_heads\", 4))\n",
    "    lr = float(params.get(\"lr\", 1e-3))\n",
    "\n",
    "    model = TemporalFusionTransformer.from_dataset(\n",
    "        params[\"dataset\"],  # dataset preparado via TimeSeriesDataSet\n",
    "        learning_rate=lr,\n",
    "        hidden_size=hidden_size,\n",
    "        dropout=dropout,\n",
    "        lstm_layers=lstm_layers,\n",
    "        attention_head_size=attention_head_size,\n",
    "        loss=QuantileLoss([0.5]),\n",
    "        log_interval=10,\n",
    "        log_val_interval=1\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afbf736",
   "metadata": {},
   "source": [
    "# Cap√≠tulo 3 - Contru√ß√£o da Pipelines de dados dos modelos\n",
    "\n",
    "A fun√ß√£o de pipeline organiza o fluxo de dados para, de forma mais concisa e organizada, treinar o modelo, sendo capaz de mostrar a progress√£o das perdas a medida que as √©pocas de treinamento passam - Esse display est√© dispon√≠vel no notebook \"Resultados\"\n",
    "\n",
    "O resultado da pipeline √© um gr√°fico com a evolu√ß√£o de todas as m√©tricas e o salvamento do modelo treinado dentro da pasta ./modelo/{Nome_Problema}/{Nome_Modelo}\n",
    "\n",
    "Assim podendo ser facilmente reutilizado futuramente para um notebook comparativo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d03c22",
   "metadata": {},
   "source": [
    "## Pipeline dos Modelos Lineares\n",
    "\n",
    "Pipeline de preprocessamento e de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e97fb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping as TFEarlyStopping, ReduceLROnPlateau as TFReduceLROnPlateau\n",
    "\n",
    "\n",
    "def linear_preproccess_pipeline(\n",
    "    data_dir: str,\n",
    "    country_list: List[str],\n",
    "    feature_cols: List[str],\n",
    "    target_cols: List[str],\n",
    "    lag: int,\n",
    "    lead: int,\n",
    "    value_cols: List[str],\n",
    "    reduced_dim: Optional[int] = None,\n",
    "    mask_value: Optional[float] = -999.0,\n",
    ") -> Tuple[LinearPreprocessor, Dict[str, keras.Model]]:\n",
    "    \"\"\"\n",
    "    Pipeline completa de pr√©-processamento e treinamento de 3 modelos lineares\n",
    "    (simple, medium, deep) para compara√ß√£o direta de desempenho.\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------\n",
    "    # Pr√©-processamento\n",
    "    # ----------------------------\n",
    "    preproc = LinearPreprocessor(\n",
    "        data_dir=data_dir,\n",
    "        model_name='linear_model',\n",
    "        feature_cols=feature_cols,\n",
    "        target_cols=target_cols,\n",
    "        lag=lag,\n",
    "        lead=lead,\n",
    "        country_list=country_list\n",
    "    )\n",
    "\n",
    "    preproc.load_data()\n",
    "    preproc.encode(encode_cols='datetime', encode_method='time_cycle')\n",
    "    preproc.encode(encode_cols='country', encode_method='label')\n",
    "    preproc.split_train_val_test(train_size=0.8, val_size=0.1, test_size=0.1, time_col='datetime')\n",
    "    preproc.normalize_splits(value_cols=value_cols, normalization_method='minmax')\n",
    "    preproc.build_flat_matrices_splits(\n",
    "        value_cols=value_cols,\n",
    "        target_cols=target_cols,\n",
    "        dropna=True,\n",
    "        group_cols=['country'],\n",
    "        time_col='datetime',\n",
    "        reduced_dim=reduced_dim,\n",
    "        mask_value=mask_value\n",
    "    )\n",
    "    preproc.save_splits_tfrecords(output_basename='linear_dataset', shard_size=1000, compression='GZIP')\n",
    "    print(\"‚úÖ Pr√©-processamento linear conclu√≠do.\")\n",
    "\n",
    "    return preproc\n",
    "\n",
    "\n",
    "def linear_train_pipeline(\n",
    "    problem_name: str,\n",
    "    data_dir: str,\n",
    "    batch_size: int = 128,\n",
    "    configs: Dict[str, Dict[str, Any]] = None\n",
    "):\n",
    "    # ----------------------------\n",
    "    # TFRecord datasets\n",
    "    # ----------------------------\n",
    "    meta_path = os.path.join(data_dir, \"linear_dataset_train.meta.json\")\n",
    "    if not os.path.exists(meta_path):\n",
    "        raise FileNotFoundError(f\"‚ùå Arquivo de metadados n√£o encontrado: {meta_path}\")\n",
    "\n",
    "    with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    x_dim = int(meta[\"x_dim\"])\n",
    "    y_dim = int(meta[\"y_dim\"])\n",
    "\n",
    "    dataset_train = load_tfrecord_dataset(\n",
    "        path_pattern=os.path.join(data_dir, 'linear_dataset_train*.tfrecord'),\n",
    "        x_dim=x_dim, y_dim=y_dim, batch_size=batch_size\n",
    "    )\n",
    "    dataset_val = load_tfrecord_dataset(\n",
    "        path_pattern=os.path.join(data_dir, 'linear_dataset_val*.tfrecord'),\n",
    "        x_dim=x_dim, y_dim=y_dim, batch_size=batch_size\n",
    "    )\n",
    "    print(\"üì¶ Dataset TFRecord carregado para treinamento.\")\n",
    "\n",
    "    histories = {}\n",
    "    models = {}\n",
    "\n",
    "    early_stopping = TFEarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=True, verbose=0)\n",
    "    reduce_lr = TFReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=6, min_lr=1e-6, verbose=0)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Treinamento de cada modelo\n",
    "    # ----------------------------\n",
    "    for name, params in configs.items():\n",
    "        print(f\"\\nüöÄ Treinando modelo {name}...\")\n",
    "        if params[\"linear\"]:\n",
    "            # Modelo Linear\n",
    "            model = build_linear_model(x_dim=x_dim, y_dim=y_dim, params=params, linear=True)\n",
    "        else:\n",
    "            # Modelo MLP\n",
    "            model = build_linear_model(x_dim=x_dim, y_dim=y_dim, params=params, linear=False)\n",
    "        hist = model.fit(\n",
    "            dataset_train,\n",
    "            validation_data=dataset_val,\n",
    "            epochs=100,\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=2\n",
    "        )\n",
    "\n",
    "        histories[name] = hist\n",
    "        models[name] = model\n",
    "        print(f\"‚úÖ {name} conclu√≠do - Val Loss: {min(hist.history['val_loss']):.6f}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Salvando modelos\n",
    "    # ----------------------------\n",
    "    for name, model in models.items():\n",
    "        # Salvando modelo no path /modelos/{nome do problema}/{nome do modelo}\n",
    "        save_model(model, path = f\"./modelos/{problem_name}/{name}\")\n",
    "    \n",
    "    \n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6250a234",
   "metadata": {},
   "source": [
    "## Pipelines dos modelos LSTM\n",
    "Implementa√ß√£o e uso dos preprocessors e treinadores LSTM para s√©ries temporais (janelas seq_len e lead)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f66f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping as TFEarlyStopping, ReduceLROnPlateau as TFReduceLROnPlateau\n",
    "\n",
    "\n",
    "def lstm_preproccess_pipeline(\n",
    "    data_dir: str,\n",
    "    country_list: List[str],\n",
    "    feature_cols: List[str],\n",
    "    target_cols: List[str],\n",
    "    seq_len: int,\n",
    "    lead: int,\n",
    "    value_cols: List[str]\n",
    ") -> Tuple[LSTMPreprocessor, Dict[str, keras.Model]]:\n",
    "    \"\"\"\n",
    "    Pipeline completa de pr√©-processamento e treinamento de 3 modelos LSTM\n",
    "    em diferentes escalas de complexidade (simple, medium, deep).\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------\n",
    "    # Pr√©-processamento\n",
    "    # ----------------------------\n",
    "    preproc = LSTMPreprocessor(\n",
    "        data_dir=data_dir,\n",
    "        model_name=\"lstm_model\",\n",
    "        feature_cols=feature_cols,\n",
    "        target_cols=target_cols,\n",
    "        country_list=country_list,\n",
    "        lag=seq_len,\n",
    "        lead=lead,\n",
    "    )\n",
    "\n",
    "    preproc.load_data()\n",
    "    preproc.encode(encode_cols=\"datetime\", encode_method=\"time_cycle\")\n",
    "    preproc.encode(encode_cols=\"country\", encode_method=\"label\")\n",
    "    preproc.split_train_val_test(train_size=0.6, val_size=0.2, test_size=0.2, time_col=\"datetime\")\n",
    "    preproc.normalize_splits(value_cols=value_cols, normalization_method=\"minmax\")\n",
    "\n",
    "    # Constr√≥i janelas e salva TFRecords para cada split\n",
    "    for split_name, split_df in preproc.splits.items():\n",
    "        preproc.df_base = split_df\n",
    "        preproc.build_sequence_matrix(\n",
    "            value_cols=value_cols,\n",
    "            target_cols=target_cols,\n",
    "            seq_len=seq_len,\n",
    "            lead=lead,\n",
    "            group_cols=[\"country\"],\n",
    "            time_col=\"datetime\",\n",
    "        )\n",
    "        preproc.save_sequence_tfrecords(\n",
    "            output_basename=f\"lstm_dataset_{split_name}\", shard_size=1000, compression=\"GZIP\"\n",
    "        )\n",
    "    print(\"‚úÖ Pr√©-processamento sequencial conclu√≠do.\")\n",
    "    return preproc\n",
    "\n",
    "def lstm_train_pipeline(\n",
    "    problem_name: str,\n",
    "    data_dir: str,\n",
    "    seq_len: int,\n",
    "    batch_size: int = 128,\n",
    "    configs: Dict[str, Dict[str, Any]] = None\n",
    ") -> Tuple[LSTMPreprocessor, Dict[str, keras.Model]]:\n",
    "    # ----------------------------\n",
    "    # TFRecord datasets\n",
    "    # ----------------------------\n",
    "    meta_path = os.path.join(data_dir, \"lstm_dataset_train.meta.json\")\n",
    "    if not os.path.exists(meta_path):\n",
    "        raise FileNotFoundError(f\"‚ùå Arquivo de metadados n√£o encontrado: {meta_path}\")\n",
    "\n",
    "    with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    x_dim = int(meta[\"x_dim\"])\n",
    "    y_dim = int(meta[\"y_dim\"])\n",
    "\n",
    "    dataset_train = LSTMPreprocessor.load_sequence_dataset(\n",
    "        path_pattern=os.path.join(data_dir, \"lstm_dataset_train*.tfrecord\"),\n",
    "        seq_len=seq_len,\n",
    "        x_dim=x_dim,\n",
    "        y_dim=y_dim,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    dataset_val = LSTMPreprocessor.load_sequence_dataset(\n",
    "        path_pattern=os.path.join(data_dir, \"lstm_dataset_val*.tfrecord\"),\n",
    "        seq_len=seq_len,\n",
    "        x_dim=x_dim,\n",
    "        y_dim=y_dim,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    print(\"üì¶ Dataset TFRecord carregado para treinamento.\")\n",
    "\n",
    "    histories = {}\n",
    "    models = {}\n",
    "\n",
    "    early_stopping = TFEarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=True, verbose=0)\n",
    "    reduce_lr = TFReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=6, min_lr=1e-6, verbose=0)\n",
    "\n",
    "    for name, params in configs.items():\n",
    "        print(f\"\\nüöÄ Treinando modelo {name}...\")\n",
    "        model = build_lstm_model(seq_len=seq_len, x_dim=x_dim, y_dim=y_dim, params=params)\n",
    "        hist = model.fit(\n",
    "            dataset_train,\n",
    "            validation_data=dataset_val,\n",
    "            epochs=100,\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=2,\n",
    "        )\n",
    "\n",
    "        histories[name] = hist\n",
    "        models[name] = model\n",
    "\n",
    "        print(f\"‚úÖ {name} conclu√≠do - Val Loss: {min(hist.history['val_loss']):.6f}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Salvando modelos\n",
    "    # ----------------------------\n",
    "    for name, model in models.items():\n",
    "        # Salvando modelo no path /modelos/{nome do problema}/{nome do modelo}\n",
    "        save_model(model, path = f\"./modelos/{problem_name}/{name}\")\n",
    "    \n",
    "    \n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be66287c",
   "metadata": {},
   "source": [
    "## Pipelines dos modelos TFT\n",
    "Pr√©-processamento em parquet e treino com PyTorch Forecasting (Temporal Fusion Transformer) via Lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e31312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Dict, Any, List\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import EarlyStopping as LGEarlyStopping, LearningRateMonitor as LGLearningRateMonitor, ModelCheckpoint as LGModelCheckpoint\n",
    "\n",
    "\n",
    "def tft_preproccess_pipeline(\n",
    "    data_dir: str,\n",
    "    country_list: List[str],\n",
    "    feature_cols: List[str],\n",
    "    target_cols: List[str],\n",
    "    seq_len: int,\n",
    "    lead: int,\n",
    "    value_cols: List[str]\n",
    ") -> Tuple[TFTPreprocessor, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Pipeline completa de pr√©-processamento para TFT.\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------\n",
    "    # Pr√©-processamento\n",
    "    # ----------------------------\n",
    "    preproc = TFTPreprocessor(\n",
    "        data_dir=data_dir,\n",
    "        model_name='linear_model',\n",
    "        feature_cols=feature_cols,\n",
    "        target_cols=target_cols,\n",
    "        seq_len=seq_len,\n",
    "        lead=lead,\n",
    "        country_list=country_list\n",
    "    )\n",
    "\n",
    "    preproc.load_data()\n",
    "    preproc.encode(encode_cols='datetime', encode_method='time_cycle')\n",
    "    preproc.encode(encode_cols='country', encode_method='label')\n",
    "    preproc.split_train_val_test(train_size=0.8, val_size=0.1, test_size=0.1, time_col='datetime')\n",
    "    preproc.normalize_splits(value_cols=value_cols, normalization_method='minmax')\n",
    "    preproc.build_tft_parquets(\n",
    "        group_cols=['country'],\n",
    "        time_col='datetime'\n",
    "    )\n",
    "    print(\"‚úÖ Pr√©-processamento tft conclu√≠do.\")\n",
    "\n",
    "    return preproc\n",
    "\n",
    "\n",
    "\n",
    "def tft_train_pipeline(\n",
    "    problem_name: str,\n",
    "    data_dir: str,\n",
    "    feature_cols: List[str],\n",
    "    target_cols: List[str],\n",
    "    seq_len: int,\n",
    "    lead: int,\n",
    "    batch_size: int = 128,\n",
    "    configs: Dict[str, Dict[str, Any]] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Treinamento de modelos TFT (Temporal Fusion Transformer) usando PyTorch Forecasting + Lightning.\n",
    "\n",
    "    - Consome os parquets gerados por TFTPreprocessor: tft_dataset_train.parquet e tft_dataset_val.parquet\n",
    "    - Cria TimeSeriesDataSet para treino/valida√ß√£o\n",
    "    - Constr√≥i o modelo via TemporalFusionTransformer.from_dataset\n",
    "    - Treina com EarlyStopping e salva checkpoints por preset\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------\n",
    "    # 1. Carregar dados pr√©-processados via TFTPreprocessor reutilizando load_tft_dataset\n",
    "    # ----------------------------\n",
    "    preproc = TFTPreprocessor(\n",
    "        data_dir=data_dir,\n",
    "        model_name='tft_model',\n",
    "        feature_cols=feature_cols,\n",
    "        target_cols=target_cols,\n",
    "        seq_len=seq_len,\n",
    "        lead=lead,\n",
    "        country_list=[],\n",
    "    )\n",
    "\n",
    "    # usa a fun√ß√£o para retornar DataFrames ‚Äî permite aplicar dtypes e criar TimeSeriesDataSet de forma consistente\n",
    "    df_train = preproc.load_tft_dataset('train', target_col=target_cols[0])\n",
    "    df_val = preproc.load_tft_dataset('val', target_col=target_cols[0])\n",
    "\n",
    "    # ----------------------------\n",
    "    # 2. TimeSeriesDataSet (encoder/decoder feitos internamente)\n",
    "    # ----------------------------\n",
    "\n",
    "    train_loader = df_train.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "    val_loader   = df_val.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "    print(f\"üì¶ Dados TFT ‚Äî batches: train={len(train_loader)} | val={len(val_loader)}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # 3. Treinamento por preset\n",
    "    # ----------------------------\n",
    "    models = {}\n",
    "    seed_everything(42)\n",
    "\n",
    "    accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    for name, params in (configs or {}).items():\n",
    "        print(f\"\\nüöÄ Treinando TFT preset: {name} [{accelerator}]\")\n",
    "\n",
    "        model = build_tft_model(\n",
    "            params={\n",
    "                **params,\n",
    "                \"dataset\": df_train,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        save_dir = os.path.join(\"modelos\", problem_name, \"TFT\", name)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        callbacks = [\n",
    "            LGEarlyStopping(monitor=\"val_loss\", patience=int(params.get(\"patience\", 5)), mode=\"min\"),\n",
    "            LGLearningRateMonitor(logging_interval=\"epoch\"),\n",
    "            LGModelCheckpoint(\n",
    "                dirpath=save_dir,\n",
    "                filename=\"best\",\n",
    "                monitor=\"val_loss\",\n",
    "                save_top_k=1,\n",
    "                mode=\"min\",\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        trainer = Trainer(\n",
    "            max_epochs=int(params.get(\"epochs\", 50)),\n",
    "            accelerator=accelerator,\n",
    "            devices=1,\n",
    "            callbacks=callbacks,\n",
    "            default_root_dir=save_dir,\n",
    "            log_every_n_steps=10,\n",
    "            logger=True,\n",
    "            precision=32,\n",
    "        )\n",
    "\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        print(f\"‚úÖ {name} conclu√≠do ‚Äî melhor checkpoint salvo em {save_dir}\")\n",
    "\n",
    "        models[name] = model\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b138dc",
   "metadata": {},
   "source": [
    "# Cap√≠tulo 4: Defini√ß√£o da estrutura dos modelos - N√£o foi feita otimiza√ß√£o de hiperparm\n",
    "Configura√ß√£o dos problemas (dados, features, janelas) e presets de hiperpar√¢metros para Linear/MLP, LSTM e TFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d8e826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, tensorflow as tf\n",
    "\n",
    "\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "# Configura√ß√µes dos modelos\n",
    "\n",
    "configs_linear = {\n",
    "    # \"linear_Simple\": {\n",
    "    #     \"linear\": True,\n",
    "    #     \"units\": [],\n",
    "    #     \"dropout\": 0.0,\n",
    "    #     \"lr\": 1e-3,\n",
    "    #     \"l2\": 0.0,\n",
    "    #     \"layer_norm\": False,\n",
    "    # },\n",
    "    \"linear_Medium\": {\n",
    "        \"linear\": True,\n",
    "        \"units\": [128, 64],\n",
    "        \"dropout\": 0.1,\n",
    "        \"lr\": 1e-3,\n",
    "        \"l2\": 1e-6,\n",
    "        \"act\": \"relu\",\n",
    "        \"layer_norm\": False,\n",
    "    },\n",
    "    # \"linear_Deep\": {\n",
    "    #     \"linear\": False,\n",
    "    #     \"units\": [256, 128, 64],\n",
    "    #     \"dropout\": 0.2,\n",
    "    #     \"lr\": 1e-3,\n",
    "    #     \"l2\": 1e-6,\n",
    "    #     \"act\": \"relu\",\n",
    "    #     \"layer_norm\": True,\n",
    "    # },\n",
    "}\n",
    "\n",
    "configs_mlp = {\n",
    "    # \"mlp_Simple\": {\n",
    "    #     \"linear\": False,\n",
    "    #     \"units\": [64],\n",
    "    #     \"dropout\": 0.05,\n",
    "    #     \"lr\": 1e-3,\n",
    "    #     \"l2\": 1e-6,\n",
    "    #     \"act\": \"relu\",\n",
    "    #     \"layer_norm\": False,\n",
    "    # },\n",
    "    \"mlp_Medium\": {\n",
    "        \"linear\": False,\n",
    "        \"units\": [128, 64],\n",
    "        \"dropout\": 0.1,\n",
    "        \"lr\": 1e-3,\n",
    "        \"l2\": 1e-6,\n",
    "        \"act\": \"relu\",\n",
    "        \"layer_norm\": False,\n",
    "    },\n",
    "    # \"mlp_Deep\": {\n",
    "    #     \"linear\": False,\n",
    "    #     \"units\": [256, 128, 64],\n",
    "    #     \"dropout\": 0.2,\n",
    "    #     \"lr\": 1e-3,\n",
    "    #     \"l2\": 1e-6,\n",
    "    #     \"act\": \"relu\",\n",
    "    #     \"layer_norm\": True,\n",
    "    # },\n",
    "}\n",
    "\n",
    "configs_lstm = {\n",
    "    # \"lstm_Simple\": {\n",
    "    #     \"lstm_units\": [64],\n",
    "    #     \"dense_units\": [64],\n",
    "    #     \"dropout\": 0.05,\n",
    "    #     \"rec_dropout\": 0.0,\n",
    "    #     \"act\": \"tanh\",\n",
    "    #     \"lr\": 1e-3,\n",
    "    #     \"l2\": 1e-6,\n",
    "    #     \"layer_norm\": False,\n",
    "    # },\n",
    "    \"lstm_Medium\": {\n",
    "        \"lstm_units\": [128, 64],\n",
    "        \"dense_units\": [64],\n",
    "        \"dropout\": 0 if gpu_devices else 0.15,\n",
    "        \"rec_dropout\": 0 if gpu_devices else 0.05,\n",
    "        \"act\": \"tanh\",\n",
    "        \"lr\": 1e-3,\n",
    "        \"l2\": 1e-6,\n",
    "        \"layer_norm\": True,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Presets de TFT compat√≠veis com build_tft_model (PyTorch Forecasting)\n",
    "# Campos utilizados: hidden_size, dropout, lstm_layers, num_heads, lr, epochs, patience\n",
    "config_tft = {\n",
    "    # \"tft_Simple\": {\n",
    "    #     \"hidden_size\": 64,\n",
    "    #     \"dropout\": 0.1,\n",
    "    #     \"lstm_layers\": 1,\n",
    "    #     \"num_heads\": 4,\n",
    "    #     \"lr\": 1e-3,\n",
    "    #     \"epochs\": 50,\n",
    "    #     \"patience\": 5,\n",
    "    # },\n",
    "    \"tft_Medium\": {\n",
    "        \"hidden_size\": 64,\n",
    "        \"lstm_layers\": 1,\n",
    "        \"num_heads\": 2,\n",
    "        \"dropout\": 0.2,\n",
    "        \"hidden_continuous_size\": 32,\n",
    "        \"attention_head_size\": 2,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"patience\": 20,\n",
    "        \"epochs\": 50,\n",
    "    }, \n",
    "    \n",
    "    # \"tft_Deep\": {\n",
    "    #     \"hidden_size\": 256,\n",
    "    #     \"dropout\": 0.2,\n",
    "    #     \"lstm_layers\": 2,\n",
    "    #     \"num_heads\": 8,\n",
    "    #     \"lr\": 1e-3,\n",
    "    #     \"epochs\": 100,\n",
    "    #     \"patience\": 10,\n",
    "    # },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af6da91",
   "metadata": {},
   "source": [
    "# Cap√≠tulo 5: Pr√©processamento de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bec8a927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[üíæ] Parquet salvo em: data/N1A/linear_dataset_test.parquet\n",
      "[‚úÖ] TFRecords salvos (4 shards) + Parquet em data/N1A\n",
      "‚úÖ Pr√©-processamento linear conclu√≠do.\n",
      "üß† Pr√©-processando dados do modelo LSTM\n",
      "[‚úÖ] TFRecords salvos (28 shards) + Parquet em data/N1B\n",
      "[DIVIDIDO] train: 20,948 linhas\n",
      "[DIVIDIDO] val: 6,982 linhas\n",
      "[DIVIDIDO] test: 6,984 linhas\n",
      "[JANELAS] X=(20637, 240, 1), Y=(20637, 72, 1), seq_len=240, lead=72\n",
      "[üíæ] Parquet salvo em: data/N1B/linear_dataset_val.parquet\n",
      "[‚úÖ] TFRecords salvos (4 shards) + Parquet em data/N1B\n",
      "[‚úÖ] TFRecords salvos (21 shards) em data/N1A ‚Äî lead=72\n",
      "[JANELAS] X=(6671, 240, 1), Y=(6671, 72, 1), seq_len=240, lead=72\n",
      "[‚úÖ] TFRecords salvos (7 shards) em data/N1A ‚Äî lead=72\n",
      "[JANELAS] X=(6673, 240, 1), Y=(6673, 72, 1), seq_len=240, lead=72\n",
      "[üíæ] Parquet salvo em: data/N1B/linear_dataset_test.parquet\n",
      "[‚úÖ] TFRecords salvos (4 shards) + Parquet em data/N1B\n",
      "‚úÖ Pr√©-processamento linear conclu√≠do.\n",
      "üß† Pr√©-processando dados do modelo LSTM\n",
      "[‚úÖ] TFRecords salvos (7 shards) em data/N1A ‚Äî lead=72\n",
      "‚úÖ Pr√©-processamento sequencial conclu√≠do.\n",
      "üß† Pr√©-processando dados do modelo TFT\n",
      "[DIVIDIDO] train: 27,931 linhas\n",
      "[DIVIDIDO] val: 3,491 linhas\n",
      "[DIVIDIDO] test: 3,492 linhas\n",
      "[DIVIDIDO] train: 20,948 linhas\n",
      "[DIVIDIDO] val: 6,982 linhas\n",
      "[DIVIDIDO] test: 6,984 linhas\n",
      "[JANELAS] X=(20637, 240, 1), Y=(20637, 72, 1), seq_len=240, lead=72\n",
      "üíæ Split 'train' salvo em data/N1A/tft_dataset_train.parquet (27931 linhas, grupos=1, max local time_idx=27930).\n",
      "üíæ Split 'val' salvo em data/N1A/tft_dataset_val.parquet (3491 linhas, grupos=1, max local time_idx=3490).\n",
      "üíæ Split 'test' salvo em data/N1A/tft_dataset_test.parquet (3492 linhas, grupos=1, max local time_idx=3491).\n",
      "‚úÖ Pr√©-processamento tft conclu√≠do.\n",
      "‚úÖ Finalizado N1A - mem√≥ria liberada\n",
      "------------------------------------------------------------\n",
      "üß© Resultado N1A: OK\n",
      "[üíæ] Parquet salvo em: data/N1C/linear_dataset_train.parquet\n",
      "[üíæ] Parquet salvo em: data/N2A/linear_dataset_train.parquet\n",
      "[‚úÖ] TFRecords salvos (21 shards) em data/N1B ‚Äî lead=72\n",
      "[‚ÑπÔ∏è] Reduzindo lags: usando 168 e mascarando 72 restantes com -999.0\n",
      "[JANELAS] X=(6671, 240, 1), Y=(6671, 72, 1), seq_len=240, lead=72\n",
      "[‚úÖ] TFRecords salvos (7 shards) em data/N1B ‚Äî lead=72\n",
      "[‚úÖ] TFRecords salvos (28 shards) + Parquet em data/N1C\n",
      "[JANELAS] X=(6673, 240, 1), Y=(6673, 72, 1), seq_len=240, lead=72\n",
      "[‚úÖ] TFRecords salvos (7 shards) em data/N1B ‚Äî lead=72\n",
      "‚úÖ Pr√©-processamento sequencial conclu√≠do.\n",
      "üß† Pr√©-processando dados do modelo TFT\n",
      "[DIVIDIDO] train: 27,931 linhas\n",
      "[DIVIDIDO] val: 3,491 linhas\n",
      "[DIVIDIDO] test: 3,492 linhas\n",
      "[üíæ] Parquet salvo em: data/N1C/linear_dataset_val.parquet\n",
      "[‚úÖ] TFRecords salvos (4 shards) + Parquet em data/N1C\n",
      "üíæ Split 'train' salvo em data/N1B/tft_dataset_train.parquet (27931 linhas, grupos=1, max local time_idx=27930).\n",
      "üíæ Split 'val' salvo em data/N1B/tft_dataset_val.parquet (3491 linhas, grupos=1, max local time_idx=3490).\n",
      "üíæ Split 'test' salvo em data/N1B/tft_dataset_test.parquet (3492 linhas, grupos=1, max local time_idx=3491).\n",
      "‚úÖ Pr√©-processamento tft conclu√≠do.\n",
      "‚úÖ Finalizado N1B - mem√≥ria liberada\n",
      "------------------------------------------------------------\n",
      "üß© Resultado N1B: OK\n",
      "[üíæ] Parquet salvo em: data/N1C/linear_dataset_test.parquet\n",
      "[‚úÖ] TFRecords salvos (4 shards) + Parquet em data/N1C\n",
      "‚úÖ Pr√©-processamento linear conclu√≠do.\n",
      "üß† Pr√©-processando dados do modelo LSTM\n",
      "[DIVIDIDO] train: 20,948 linhas\n",
      "[DIVIDIDO] val: 6,982 linhas\n",
      "[DIVIDIDO] test: 6,984 linhas\n",
      "[JANELAS] X=(20637, 240, 1), Y=(20637, 72, 1), seq_len=240, lead=72\n",
      "[‚úÖ] TFRecords salvos (84 shards) + Parquet em data/N2A\n",
      "[‚úÖ] TFRecords salvos (21 shards) em data/N1C ‚Äî lead=72\n",
      "[JANELAS] X=(6671, 240, 1), Y=(6671, 72, 1), seq_len=240, lead=72\n",
      "[‚úÖ] TFRecords salvos (7 shards) em data/N1C ‚Äî lead=72\n",
      "[üíæ] Parquet salvo em: data/N2A/linear_dataset_val.parquet\n",
      "[JANELAS] X=(6673, 240, 1), Y=(6673, 72, 1), seq_len=240, lead=72\n",
      "[‚úÖ] TFRecords salvos (7 shards) em data/N1C ‚Äî lead=72\n",
      "‚úÖ Pr√©-processamento sequencial conclu√≠do.\n",
      "üß† Pr√©-processando dados do modelo TFT\n",
      "[‚úÖ] TFRecords salvos (11 shards) + Parquet em data/N2A\n",
      "[DIVIDIDO] train: 27,931 linhas\n",
      "[DIVIDIDO] val: 3,491 linhas\n",
      "[DIVIDIDO] test: 3,492 linhas\n",
      "üíæ Split 'train' salvo em data/N1C/tft_dataset_train.parquet (27931 linhas, grupos=1, max local time_idx=27930).\n",
      "üíæ Split 'val' salvo em data/N1C/tft_dataset_val.parquet (3491 linhas, grupos=1, max local time_idx=3490).\n",
      "üíæ Split 'test' salvo em data/N1C/tft_dataset_test.parquet (3492 linhas, grupos=1, max local time_idx=3491).\n",
      "‚úÖ Pr√©-processamento tft conclu√≠do.\n",
      "‚úÖ Finalizado N1C - mem√≥ria liberada\n",
      "------------------------------------------------------------\n",
      "üß© Resultado N1C: OK\n",
      "[üíæ] Parquet salvo em: data/N2A/linear_dataset_test.parquet\n",
      "[‚úÖ] TFRecords salvos (11 shards) + Parquet em data/N2A\n",
      "‚úÖ Pr√©-processamento linear conclu√≠do.\n",
      "üß† Pr√©-processando dados do modelo LSTM\n",
      "[DIVIDIDO] train: 62,873 linhas\n",
      "[DIVIDIDO] val: 20,957 linhas\n",
      "[DIVIDIDO] test: 20,959 linhas\n",
      "[üíæ] Parquet salvo em: data/N2B/linear_dataset_train.parquet\n",
      "[JANELAS] X=(61940, 240, 1), Y=(61940, 72, 1), seq_len=240, lead=72\n",
      "[‚úÖ] TFRecords salvos (62 shards) em data/N2A ‚Äî lead=72\n",
      "[JANELAS] X=(20024, 240, 1), Y=(20024, 72, 1), seq_len=240, lead=72\n",
      "[üíæ] Parquet salvo em: data/N2C/linear_dataset_train.parquet\n",
      "[üíæ] Parquet salvo em: data/treinamento/linear_dataset_train.parquet\n",
      "[‚úÖ] TFRecords salvos (21 shards) em data/N2A ‚Äî lead=72\n",
      "[‚úÖ] TFRecords salvos (84 shards) + Parquet em data/N2B\n",
      "[JANELAS] X=(20026, 240, 1), Y=(20026, 72, 1), seq_len=240, lead=72\n",
      "[‚úÖ] TFRecords salvos (21 shards) em data/N2A ‚Äî lead=72\n",
      "‚úÖ Pr√©-processamento sequencial conclu√≠do.\n",
      "üß† Pr√©-processando dados do modelo TFT\n",
      "[üíæ] Parquet salvo em: data/N2B/linear_dataset_val.parquet\n",
      "[DIVIDIDO] train: 83,831 linhas\n",
      "[DIVIDIDO] val: 10,478 linhas\n",
      "[DIVIDIDO] test: 10,480 linhas\n",
      "[‚úÖ] TFRecords salvos (10 shards) + Parquet em data/N2B\n",
      "[‚úÖ] TFRecords salvos (83 shards) + Parquet em data/N2C\n",
      "üíæ Split 'train' salvo em data/N2A/tft_dataset_train.parquet (83831 linhas, grupos=3, max local time_idx=28018).\n",
      "üíæ Split 'val' salvo em data/N2A/tft_dataset_val.parquet (10478 linhas, grupos=3, max local time_idx=3506).\n",
      "[üíæ] Parquet salvo em: data/N2B/linear_dataset_test.parquet\n",
      "[üíæ] Parquet salvo em: data/N2C/linear_dataset_val.parquet\n",
      "[‚úÖ] TFRecords salvos (83 shards) + Parquet em data/treinamento\n",
      "üíæ Split 'test' salvo em data/N2A/tft_dataset_test.parquet (10480 linhas, grupos=3, max local time_idx=3513).\n",
      "‚úÖ Pr√©-processamento tft conclu√≠do.\n",
      "‚úÖ Finalizado N2A - mem√≥ria liberada\n",
      "------------------------------------------------------------\n",
      "üß© Resultado N2A: OK\n",
      "[‚úÖ] TFRecords salvos (10 shards) + Parquet em data/N2C\n",
      "[‚úÖ] TFRecords salvos (10 shards) + Parquet em data/N2B\n",
      "‚úÖ Pr√©-processamento linear conclu√≠do.\n",
      "üß† Pr√©-processando dados do modelo LSTM\n",
      "[DIVIDIDO] train: 62,873 linhas\n",
      "[DIVIDIDO] val: 20,957 linhas\n",
      "[DIVIDIDO] test: 20,959 linhas\n",
      "[üíæ] Parquet salvo em: data/treinamento/linear_dataset_val.parquet\n",
      "[üíæ] Parquet salvo em: data/N2C/linear_dataset_test.parquet\n",
      "[JANELAS] X=(61940, 240, 1), Y=(61940, 72, 1), seq_len=240, lead=72\n",
      "[‚úÖ] TFRecords salvos (10 shards) + Parquet em data/treinamento\n",
      "[‚úÖ] TFRecords salvos (10 shards) + Parquet em data/N2C\n",
      "‚úÖ Pr√©-processamento linear conclu√≠do.\n",
      "üß† Pr√©-processando dados do modelo LSTM\n",
      "[üíæ] Parquet salvo em: data/treinamento/linear_dataset_test.parquet\n",
      "[DIVIDIDO] train: 62,873 linhas\n",
      "[DIVIDIDO] val: 20,957 linhas\n",
      "[DIVIDIDO] test: 20,959 linhas\n",
      "[‚úÖ] TFRecords salvos (10 shards) + Parquet em data/treinamento\n",
      "‚úÖ Pr√©-processamento linear conclu√≠do.\n",
      "üß† Pr√©-processando dados do modelo LSTM\n",
      "[JANELAS] X=(61940, 240, 1), Y=(61940, 72, 1), seq_len=240, lead=72\n",
      "[DIVIDIDO] train: 62,873 linhas\n",
      "[DIVIDIDO] val: 20,957 linhas\n",
      "[DIVIDIDO] test: 20,959 linhas\n",
      "[‚úÖ] TFRecords salvos (62 shards) em data/N2B ‚Äî lead=72\n",
      "[JANELAS] X=(61940, 240, 1), Y=(61940, 72, 1), seq_len=240, lead=72\n",
      "[JANELAS] X=(20024, 240, 1), Y=(20024, 72, 1), seq_len=240, lead=72\n",
      "[‚úÖ] TFRecords salvos (21 shards) em data/N2B ‚Äî lead=72\n",
      "[‚úÖ] TFRecords salvos (62 shards) em data/N2C ‚Äî lead=72\n",
      "[JANELAS] X=(20026, 240, 1), Y=(20026, 72, 1), seq_len=240, lead=72\n",
      "[JANELAS] X=(20024, 240, 1), Y=(20024, 72, 1), seq_len=240, lead=72\n",
      "[‚úÖ] TFRecords salvos (21 shards) em data/N2B ‚Äî lead=72\n",
      "‚úÖ Pr√©-processamento sequencial conclu√≠do.\n",
      "üß† Pr√©-processando dados do modelo TFT\n",
      "[‚úÖ] TFRecords salvos (21 shards) em data/N2C ‚Äî lead=72\n",
      "[JANELAS] X=(20026, 240, 1), Y=(20026, 72, 1), seq_len=240, lead=72\n",
      "[‚úÖ] TFRecords salvos (62 shards) em data/treinamento ‚Äî lead=72\n",
      "[DIVIDIDO] train: 83,831 linhas\n",
      "[DIVIDIDO] val: 10,478 linhas\n",
      "[DIVIDIDO] test: 10,480 linhas\n",
      "[JANELAS] X=(20024, 240, 1), Y=(20024, 72, 1), seq_len=240, lead=72\n",
      "[‚úÖ] TFRecords salvos (21 shards) em data/N2C ‚Äî lead=72\n",
      "‚úÖ Pr√©-processamento sequencial conclu√≠do.\n",
      "üß† Pr√©-processando dados do modelo TFT\n",
      "üíæ Split 'train' salvo em data/N2B/tft_dataset_train.parquet (83831 linhas, grupos=3, max local time_idx=28018).\n",
      "üíæ Split 'val' salvo em data/N2B/tft_dataset_val.parquet (10478 linhas, grupos=3, max local time_idx=3506).\n",
      "[‚úÖ] TFRecords salvos (21 shards) em data/treinamento ‚Äî lead=72\n",
      "üíæ Split 'test' salvo em data/N2B/tft_dataset_test.parquet (10480 linhas, grupos=3, max local time_idx=3513).\n",
      "‚úÖ Pr√©-processamento tft conclu√≠do.\n",
      "‚úÖ Finalizado N2B - mem√≥ria liberada\n",
      "------------------------------------------------------------\n",
      "üß© Resultado N2B: OK\n",
      "[JANELAS] X=(20026, 240, 1), Y=(20026, 72, 1), seq_len=240, lead=72\n",
      "[DIVIDIDO] train: 83,831 linhas\n",
      "[DIVIDIDO] val: 10,478 linhas\n",
      "[DIVIDIDO] test: 10,480 linhas\n",
      "üíæ Split 'train' salvo em data/N2C/tft_dataset_train.parquet (83831 linhas, grupos=3, max local time_idx=28018).\n",
      "üíæ Split 'val' salvo em data/N2C/tft_dataset_val.parquet (10478 linhas, grupos=3, max local time_idx=3506).\n",
      "üíæ Split 'test' salvo em data/N2C/tft_dataset_test.parquet (10480 linhas, grupos=3, max local time_idx=3513).\n",
      "‚úÖ Pr√©-processamento tft conclu√≠do.\n",
      "‚úÖ Finalizado N2C - mem√≥ria liberada\n",
      "------------------------------------------------------------\n",
      "üß© Resultado N2C: OK\n",
      "[‚úÖ] TFRecords salvos (21 shards) em data/treinamento ‚Äî lead=72\n",
      "‚úÖ Pr√©-processamento sequencial conclu√≠do.\n",
      "üß† Pr√©-processando dados do modelo TFT\n",
      "[DIVIDIDO] train: 83,831 linhas\n",
      "[DIVIDIDO] val: 10,478 linhas\n",
      "[DIVIDIDO] test: 10,480 linhas\n",
      "üíæ Split 'train' salvo em data/treinamento/tft_dataset_train.parquet (83831 linhas, grupos=3, max local time_idx=28018).\n",
      "üíæ Split 'val' salvo em data/treinamento/tft_dataset_val.parquet (10478 linhas, grupos=3, max local time_idx=3506).\n",
      "üíæ Split 'test' salvo em data/treinamento/tft_dataset_test.parquet (10480 linhas, grupos=3, max local time_idx=3513).\n",
      "‚úÖ Pr√©-processamento tft conclu√≠do.\n",
      "‚úÖ Finalizado treinamento - mem√≥ria liberada\n",
      "------------------------------------------------------------\n",
      "üß© Resultado treinamento: OK\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "\n",
    "\n",
    "preprocess_collector = {}\n",
    "def run_preprocessing(cfg):\n",
    "    \"\"\"Executa o pipeline completo de pr√©-processamento para um problema.\"\"\"\n",
    "    name = cfg[\"name\"]\n",
    "    try:\n",
    "        print(f\"\\nüöÄ Executando Preprocessamento do problema {name} ...\")\n",
    "\n",
    "        print(\"üß† Pr√©-processando dados do modelo linear/MLP\")\n",
    "        preproc_lin = linear_preproccess_pipeline(\n",
    "            data_dir=cfg[\"data_dir\"],\n",
    "            feature_cols=cfg[\"feats\"],\n",
    "            target_cols=cfg[\"tgts\"],\n",
    "            lag=cfg[\"lag\"],\n",
    "            lead=cfg[\"lead\"],\n",
    "            value_cols=cfg[\"vals\"],\n",
    "            country_list=cfg[\"countries\"],\n",
    "            reduced_dim=cfg.get(\"reduced_dim\", None),\n",
    "            mask_value=cfg.get(\"mask_value\", -999.0),\n",
    "        )\n",
    "\n",
    "        del preproc_lin\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"üß† Pr√©-processando dados do modelo LSTM\")\n",
    "        preproc_lstm = lstm_preproccess_pipeline(\n",
    "            data_dir=cfg[\"data_dir\"],\n",
    "            feature_cols=cfg[\"feats\"],\n",
    "            target_cols=cfg[\"tgts\"],\n",
    "            seq_len=cfg[\"lag\"],\n",
    "            lead=cfg[\"lead\"],\n",
    "            value_cols=cfg[\"vals\"],\n",
    "            country_list=cfg[\"countries\"],\n",
    "        )\n",
    "        del preproc_lstm\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "        # === Adicionado: pr√©-processamento TFT ===\n",
    "        print(\"üß† Pr√©-processando dados do modelo TFT\")\n",
    "        preproc_tft = tft_preproccess_pipeline(\n",
    "            data_dir=cfg[\"data_dir\"],\n",
    "            feature_cols=cfg[\"feats\"],\n",
    "            target_cols=cfg[\"tgts\"],\n",
    "            seq_len=cfg[\"lag\"],\n",
    "            lead=cfg[\"lead\"],\n",
    "            value_cols=cfg[\"vals\"],\n",
    "            country_list=cfg[\"countries\"],\n",
    "        )\n",
    "        del preproc_tft\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "        print(f\"‚úÖ Finalizado {name} - mem√≥ria liberada\\n{'-'*60}\")\n",
    "        return (name, \"OK\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro em {name}: {e}\")\n",
    "        return (name, f\"ERRO: {e}\")\n",
    "\n",
    "\n",
    "# ================================\n",
    "# Execu√ß√£o paralela\n",
    "# ================================\n",
    "MAX_WORKERS = min(8, len(problemas))  # ajuste conforme n√∫cleos / VRAM dispon√≠vel\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = [executor.submit(run_preprocessing, cfg) for cfg in problemas]\n",
    "\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        name, status = future.result()\n",
    "        print(f\"üß© Resultado {name}: {status}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e510a9f",
   "metadata": {},
   "source": [
    "# Cap√≠tulo 6: Treinamento dos modelos\n",
    "Este cap√≠tulo executa, por problema: Linear/MLP (configs_linear), MLP (configs_mlp), LSTM (configs_lstm) e TFT (config_tft), liberando mem√≥ria entre execu√ß√µes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2222b57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Iniciando treinamento dos modelos ...\n",
      "================================================================================\n",
      "Modelo Linear...\n",
      "[DATASET] 83 shards carregados ‚Üí batch_size=256\n",
      "[DATASET] 10 shards carregados ‚Üí batch_size=256\n",
      "üì¶ Dataset TFRecord carregado para treinamento.\n",
      "\n",
      "üöÄ Treinando modelo linear_Medium...\n",
      "Epoch 1/100\n",
      "324/324 - 3s - 9ms/step - loss: 0.0359 - mae: 0.1182 - val_loss: 0.0164 - val_mae: 0.0920 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0092 - mae: 0.0643 - val_loss: 0.0079 - val_mae: 0.0612 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0069 - mae: 0.0534 - val_loss: 0.0077 - val_mae: 0.0601 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0064 - mae: 0.0509 - val_loss: 0.0076 - val_mae: 0.0601 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0056 - mae: 0.0479 - val_loss: 0.0091 - val_mae: 0.0662 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0056 - mae: 0.0471 - val_loss: 0.0095 - val_mae: 0.0672 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0051 - mae: 0.0443 - val_loss: 0.0101 - val_mae: 0.0687 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0048 - mae: 0.0421 - val_loss: 0.0102 - val_mae: 0.0695 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0045 - mae: 0.0405 - val_loss: 0.0097 - val_mae: 0.0685 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0042 - mae: 0.0392 - val_loss: 0.0089 - val_mae: 0.0640 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0033 - mae: 0.0345 - val_loss: 0.0043 - val_mae: 0.0426 - learning_rate: 5.0000e-04\n",
      "Epoch 12/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0032 - mae: 0.0337 - val_loss: 0.0025 - val_mae: 0.0322 - learning_rate: 5.0000e-04\n",
      "Epoch 13/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0023 - mae: 0.0299 - val_loss: 0.0029 - val_mae: 0.0347 - learning_rate: 5.0000e-04\n",
      "Epoch 14/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0026 - mae: 0.0319 - val_loss: 0.0031 - val_mae: 0.0361 - learning_rate: 5.0000e-04\n",
      "Epoch 15/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0027 - mae: 0.0327 - val_loss: 0.0033 - val_mae: 0.0374 - learning_rate: 5.0000e-04\n",
      "Epoch 16/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0028 - mae: 0.0330 - val_loss: 0.0035 - val_mae: 0.0385 - learning_rate: 5.0000e-04\n",
      "Epoch 17/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0028 - mae: 0.0332 - val_loss: 0.0037 - val_mae: 0.0404 - learning_rate: 5.0000e-04\n",
      "Epoch 18/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0028 - mae: 0.0332 - val_loss: 0.0038 - val_mae: 0.0413 - learning_rate: 5.0000e-04\n",
      "Epoch 19/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0018 - mae: 0.0258 - val_loss: 0.0018 - val_mae: 0.0277 - learning_rate: 2.5000e-04\n",
      "Epoch 20/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0015 - mae: 0.0240 - val_loss: 0.0016 - val_mae: 0.0256 - learning_rate: 2.5000e-04\n",
      "Epoch 21/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0015 - mae: 0.0235 - val_loss: 0.0017 - val_mae: 0.0259 - learning_rate: 2.5000e-04\n",
      "Epoch 22/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0015 - mae: 0.0241 - val_loss: 0.0017 - val_mae: 0.0262 - learning_rate: 2.5000e-04\n",
      "Epoch 23/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0016 - mae: 0.0248 - val_loss: 0.0018 - val_mae: 0.0263 - learning_rate: 2.5000e-04\n",
      "Epoch 24/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0017 - mae: 0.0252 - val_loss: 0.0018 - val_mae: 0.0263 - learning_rate: 2.5000e-04\n",
      "Epoch 25/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0017 - mae: 0.0254 - val_loss: 0.0018 - val_mae: 0.0266 - learning_rate: 2.5000e-04\n",
      "Epoch 26/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0017 - mae: 0.0255 - val_loss: 0.0018 - val_mae: 0.0268 - learning_rate: 2.5000e-04\n",
      "Epoch 27/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0013 - mae: 0.0220 - val_loss: 0.0016 - val_mae: 0.0257 - learning_rate: 1.2500e-04\n",
      "Epoch 28/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0013 - mae: 0.0215 - val_loss: 0.0016 - val_mae: 0.0251 - learning_rate: 1.2500e-04\n",
      "Epoch 29/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0013 - mae: 0.0215 - val_loss: 0.0016 - val_mae: 0.0249 - learning_rate: 1.2500e-04\n",
      "Epoch 30/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0013 - mae: 0.0216 - val_loss: 0.0016 - val_mae: 0.0249 - learning_rate: 1.2500e-04\n",
      "Epoch 31/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0013 - mae: 0.0217 - val_loss: 0.0015 - val_mae: 0.0248 - learning_rate: 1.2500e-04\n",
      "Epoch 32/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0013 - mae: 0.0218 - val_loss: 0.0015 - val_mae: 0.0247 - learning_rate: 1.2500e-04\n",
      "Epoch 33/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0013 - mae: 0.0218 - val_loss: 0.0015 - val_mae: 0.0246 - learning_rate: 1.2500e-04\n",
      "Epoch 34/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0013 - mae: 0.0218 - val_loss: 0.0015 - val_mae: 0.0246 - learning_rate: 1.2500e-04\n",
      "Epoch 35/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0013 - mae: 0.0217 - val_loss: 0.0015 - val_mae: 0.0245 - learning_rate: 1.2500e-04\n",
      "Epoch 36/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0013 - mae: 0.0217 - val_loss: 0.0015 - val_mae: 0.0245 - learning_rate: 1.2500e-04\n",
      "Epoch 37/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0013 - mae: 0.0216 - val_loss: 0.0015 - val_mae: 0.0245 - learning_rate: 1.2500e-04\n",
      "Epoch 38/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0013 - mae: 0.0216 - val_loss: 0.0015 - val_mae: 0.0244 - learning_rate: 1.2500e-04\n",
      "Epoch 39/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0012 - mae: 0.0205 - val_loss: 0.0015 - val_mae: 0.0244 - learning_rate: 6.2500e-05\n",
      "Epoch 40/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0012 - mae: 0.0204 - val_loss: 0.0015 - val_mae: 0.0242 - learning_rate: 6.2500e-05\n",
      "Epoch 41/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0012 - mae: 0.0204 - val_loss: 0.0015 - val_mae: 0.0242 - learning_rate: 6.2500e-05\n",
      "Epoch 42/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0012 - mae: 0.0204 - val_loss: 0.0015 - val_mae: 0.0241 - learning_rate: 6.2500e-05\n",
      "Epoch 43/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0012 - mae: 0.0204 - val_loss: 0.0015 - val_mae: 0.0241 - learning_rate: 6.2500e-05\n",
      "Epoch 44/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0012 - mae: 0.0203 - val_loss: 0.0015 - val_mae: 0.0241 - learning_rate: 6.2500e-05\n",
      "Epoch 45/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0012 - mae: 0.0201 - val_loss: 0.0014 - val_mae: 0.0232 - learning_rate: 3.1250e-05\n",
      "Epoch 46/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0012 - mae: 0.0201 - val_loss: 0.0014 - val_mae: 0.0232 - learning_rate: 3.1250e-05\n",
      "Epoch 47/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0012 - mae: 0.0200 - val_loss: 0.0014 - val_mae: 0.0232 - learning_rate: 3.1250e-05\n",
      "Epoch 48/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0012 - mae: 0.0200 - val_loss: 0.0014 - val_mae: 0.0232 - learning_rate: 3.1250e-05\n",
      "Epoch 49/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0012 - mae: 0.0200 - val_loss: 0.0014 - val_mae: 0.0231 - learning_rate: 3.1250e-05\n",
      "Epoch 50/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0012 - mae: 0.0200 - val_loss: 0.0014 - val_mae: 0.0231 - learning_rate: 3.1250e-05\n",
      "Epoch 51/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0012 - mae: 0.0200 - val_loss: 0.0014 - val_mae: 0.0231 - learning_rate: 3.1250e-05\n",
      "Epoch 52/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0199 - val_loss: 0.0014 - val_mae: 0.0229 - learning_rate: 1.5625e-05\n",
      "Epoch 53/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0199 - val_loss: 0.0014 - val_mae: 0.0228 - learning_rate: 1.5625e-05\n",
      "Epoch 54/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0199 - val_loss: 0.0014 - val_mae: 0.0228 - learning_rate: 1.5625e-05\n",
      "Epoch 55/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0199 - val_loss: 0.0014 - val_mae: 0.0228 - learning_rate: 1.5625e-05\n",
      "Epoch 56/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0199 - val_loss: 0.0014 - val_mae: 0.0228 - learning_rate: 1.5625e-05\n",
      "Epoch 57/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0198 - val_loss: 0.0014 - val_mae: 0.0228 - learning_rate: 1.5625e-05\n",
      "Epoch 58/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0198 - val_loss: 0.0014 - val_mae: 0.0228 - learning_rate: 7.8125e-06\n",
      "Epoch 59/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0198 - val_loss: 0.0014 - val_mae: 0.0228 - learning_rate: 7.8125e-06\n",
      "Epoch 60/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0198 - val_loss: 0.0014 - val_mae: 0.0228 - learning_rate: 7.8125e-06\n",
      "Epoch 61/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0198 - val_loss: 0.0014 - val_mae: 0.0228 - learning_rate: 7.8125e-06\n",
      "Epoch 62/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0198 - val_loss: 0.0014 - val_mae: 0.0227 - learning_rate: 7.8125e-06\n",
      "Epoch 63/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0198 - val_loss: 0.0014 - val_mae: 0.0227 - learning_rate: 7.8125e-06\n",
      "Epoch 64/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0198 - val_loss: 0.0014 - val_mae: 0.0227 - learning_rate: 3.9063e-06\n",
      "Epoch 65/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0198 - val_loss: 0.0014 - val_mae: 0.0227 - learning_rate: 3.9063e-06\n",
      "Epoch 66/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0198 - val_loss: 0.0014 - val_mae: 0.0227 - learning_rate: 3.9063e-06\n",
      "Epoch 67/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0198 - val_loss: 0.0014 - val_mae: 0.0227 - learning_rate: 3.9063e-06\n",
      "Epoch 68/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0198 - val_loss: 0.0014 - val_mae: 0.0227 - learning_rate: 3.9063e-06\n",
      "Epoch 69/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0198 - val_loss: 0.0014 - val_mae: 0.0227 - learning_rate: 3.9063e-06\n",
      "Epoch 70/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0198 - val_loss: 0.0014 - val_mae: 0.0226 - learning_rate: 1.9531e-06\n",
      "Epoch 71/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0198 - val_loss: 0.0014 - val_mae: 0.0226 - learning_rate: 1.9531e-06\n",
      "Epoch 72/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0198 - val_loss: 0.0014 - val_mae: 0.0226 - learning_rate: 1.9531e-06\n",
      "Epoch 73/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0198 - val_loss: 0.0014 - val_mae: 0.0226 - learning_rate: 1.9531e-06\n",
      "Epoch 74/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0198 - val_loss: 0.0014 - val_mae: 0.0226 - learning_rate: 1.9531e-06\n",
      "Epoch 75/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0198 - val_loss: 0.0014 - val_mae: 0.0226 - learning_rate: 1.9531e-06\n",
      "Epoch 76/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0198 - val_loss: 0.0014 - val_mae: 0.0226 - learning_rate: 1.0000e-06\n",
      "Epoch 77/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0197 - val_loss: 0.0014 - val_mae: 0.0226 - learning_rate: 1.0000e-06\n",
      "Epoch 78/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0197 - val_loss: 0.0014 - val_mae: 0.0226 - learning_rate: 1.0000e-06\n",
      "Epoch 79/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0197 - val_loss: 0.0014 - val_mae: 0.0226 - learning_rate: 1.0000e-06\n",
      "Epoch 80/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0197 - val_loss: 0.0014 - val_mae: 0.0226 - learning_rate: 1.0000e-06\n",
      "Epoch 81/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0197 - val_loss: 0.0014 - val_mae: 0.0226 - learning_rate: 1.0000e-06\n",
      "Epoch 82/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0197 - val_loss: 0.0014 - val_mae: 0.0226 - learning_rate: 1.0000e-06\n",
      "Epoch 83/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0197 - val_loss: 0.0014 - val_mae: 0.0226 - learning_rate: 1.0000e-06\n",
      "Epoch 84/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0197 - val_loss: 0.0014 - val_mae: 0.0226 - learning_rate: 1.0000e-06\n",
      "Epoch 85/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0197 - val_loss: 0.0014 - val_mae: 0.0226 - learning_rate: 1.0000e-06\n",
      "Epoch 86/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0197 - val_loss: 0.0014 - val_mae: 0.0226 - learning_rate: 1.0000e-06\n",
      "Epoch 87/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0197 - val_loss: 0.0014 - val_mae: 0.0226 - learning_rate: 1.0000e-06\n",
      "Epoch 88/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0197 - val_loss: 0.0014 - val_mae: 0.0226 - learning_rate: 1.0000e-06\n",
      "Epoch 89/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0197 - val_loss: 0.0014 - val_mae: 0.0226 - learning_rate: 1.0000e-06\n",
      "Epoch 90/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0197 - val_loss: 0.0014 - val_mae: 0.0226 - learning_rate: 1.0000e-06\n",
      "Epoch 91/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0197 - val_loss: 0.0014 - val_mae: 0.0226 - learning_rate: 1.0000e-06\n",
      "Epoch 92/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0197 - val_loss: 0.0014 - val_mae: 0.0226 - learning_rate: 1.0000e-06\n",
      "Epoch 93/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0197 - val_loss: 0.0014 - val_mae: 0.0226 - learning_rate: 1.0000e-06\n",
      "Epoch 94/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0197 - val_loss: 0.0014 - val_mae: 0.0226 - learning_rate: 1.0000e-06\n",
      "Epoch 95/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0197 - val_loss: 0.0014 - val_mae: 0.0226 - learning_rate: 1.0000e-06\n",
      "Epoch 96/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0197 - val_loss: 0.0014 - val_mae: 0.0226 - learning_rate: 1.0000e-06\n",
      "Epoch 97/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0197 - val_loss: 0.0014 - val_mae: 0.0226 - learning_rate: 1.0000e-06\n",
      "Epoch 98/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0197 - val_loss: 0.0014 - val_mae: 0.0226 - learning_rate: 1.0000e-06\n",
      "Epoch 99/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0197 - val_loss: 0.0014 - val_mae: 0.0226 - learning_rate: 1.0000e-06\n",
      "Epoch 100/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0011 - mae: 0.0197 - val_loss: 0.0014 - val_mae: 0.0226 - learning_rate: 1.0000e-06\n",
      "‚úÖ linear_Medium conclu√≠do - Val Loss: 0.001361\n",
      "‚úÖ Modelo TensorFlow salvo em: ./modelos/treinamento/linear_Medium.keras\n",
      "================================================================================\n",
      "Modelo MLP...\n",
      "[DATASET] 83 shards carregados ‚Üí batch_size=256\n",
      "[DATASET] 10 shards carregados ‚Üí batch_size=256\n",
      "üì¶ Dataset TFRecord carregado para treinamento.\n",
      "\n",
      "üöÄ Treinando modelo mlp_Medium...\n",
      "Epoch 1/100\n",
      "324/324 - 5s - 15ms/step - loss: 0.0254 - mae: 0.0887 - val_loss: 0.0039 - val_mae: 0.0450 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0075 - mae: 0.0575 - val_loss: 0.0038 - val_mae: 0.0434 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0064 - mae: 0.0523 - val_loss: 0.0050 - val_mae: 0.0479 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0046 - mae: 0.0451 - val_loss: 0.0074 - val_mae: 0.0586 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0046 - mae: 0.0446 - val_loss: 0.0203 - val_mae: 0.1026 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0085 - mae: 0.0599 - val_loss: 0.0055 - val_mae: 0.0491 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0065 - mae: 0.0545 - val_loss: 0.0145 - val_mae: 0.0834 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0050 - mae: 0.0467 - val_loss: 0.0053 - val_mae: 0.0491 - learning_rate: 5.0000e-04\n",
      "Epoch 9/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0041 - mae: 0.0426 - val_loss: 0.0057 - val_mae: 0.0518 - learning_rate: 5.0000e-04\n",
      "Epoch 10/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0038 - mae: 0.0412 - val_loss: 0.0061 - val_mae: 0.0544 - learning_rate: 5.0000e-04\n",
      "Epoch 11/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0037 - mae: 0.0403 - val_loss: 0.0066 - val_mae: 0.0571 - learning_rate: 5.0000e-04\n",
      "Epoch 12/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0036 - mae: 0.0399 - val_loss: 0.0063 - val_mae: 0.0559 - learning_rate: 5.0000e-04\n",
      "Epoch 13/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0035 - mae: 0.0395 - val_loss: 0.0075 - val_mae: 0.0612 - learning_rate: 5.0000e-04\n",
      "Epoch 14/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0033 - mae: 0.0384 - val_loss: 0.0047 - val_mae: 0.0482 - learning_rate: 2.5000e-04\n",
      "Epoch 15/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0031 - mae: 0.0375 - val_loss: 0.0048 - val_mae: 0.0486 - learning_rate: 2.5000e-04\n",
      "Epoch 16/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0031 - mae: 0.0372 - val_loss: 0.0046 - val_mae: 0.0480 - learning_rate: 2.5000e-04\n",
      "Epoch 17/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0030 - mae: 0.0371 - val_loss: 0.0047 - val_mae: 0.0486 - learning_rate: 2.5000e-04\n",
      "Epoch 18/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0030 - mae: 0.0369 - val_loss: 0.0048 - val_mae: 0.0491 - learning_rate: 2.5000e-04\n",
      "Epoch 19/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0029 - mae: 0.0365 - val_loss: 0.0047 - val_mae: 0.0488 - learning_rate: 2.5000e-04\n",
      "Epoch 20/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0029 - mae: 0.0359 - val_loss: 0.0036 - val_mae: 0.0425 - learning_rate: 1.2500e-04\n",
      "Epoch 21/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0028 - mae: 0.0358 - val_loss: 0.0038 - val_mae: 0.0436 - learning_rate: 1.2500e-04\n",
      "Epoch 22/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0028 - mae: 0.0355 - val_loss: 0.0038 - val_mae: 0.0439 - learning_rate: 1.2500e-04\n",
      "Epoch 23/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0028 - mae: 0.0353 - val_loss: 0.0037 - val_mae: 0.0436 - learning_rate: 1.2500e-04\n",
      "Epoch 24/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0027 - mae: 0.0351 - val_loss: 0.0039 - val_mae: 0.0446 - learning_rate: 1.2500e-04\n",
      "Epoch 25/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0027 - mae: 0.0349 - val_loss: 0.0037 - val_mae: 0.0438 - learning_rate: 1.2500e-04\n",
      "Epoch 26/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0027 - mae: 0.0347 - val_loss: 0.0037 - val_mae: 0.0436 - learning_rate: 1.2500e-04\n",
      "Epoch 27/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0026 - mae: 0.0344 - val_loss: 0.0031 - val_mae: 0.0401 - learning_rate: 6.2500e-05\n",
      "Epoch 28/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0026 - mae: 0.0344 - val_loss: 0.0031 - val_mae: 0.0404 - learning_rate: 6.2500e-05\n",
      "Epoch 29/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0026 - mae: 0.0342 - val_loss: 0.0032 - val_mae: 0.0407 - learning_rate: 6.2500e-05\n",
      "Epoch 30/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0026 - mae: 0.0340 - val_loss: 0.0031 - val_mae: 0.0405 - learning_rate: 6.2500e-05\n",
      "Epoch 31/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0026 - mae: 0.0339 - val_loss: 0.0031 - val_mae: 0.0406 - learning_rate: 6.2500e-05\n",
      "Epoch 32/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0025 - mae: 0.0337 - val_loss: 0.0033 - val_mae: 0.0416 - learning_rate: 6.2500e-05\n",
      "Epoch 33/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0025 - mae: 0.0337 - val_loss: 0.0032 - val_mae: 0.0412 - learning_rate: 6.2500e-05\n",
      "Epoch 34/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0025 - mae: 0.0336 - val_loss: 0.0029 - val_mae: 0.0395 - learning_rate: 3.1250e-05\n",
      "Epoch 35/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0025 - mae: 0.0335 - val_loss: 0.0030 - val_mae: 0.0398 - learning_rate: 3.1250e-05\n",
      "Epoch 36/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0025 - mae: 0.0333 - val_loss: 0.0030 - val_mae: 0.0398 - learning_rate: 3.1250e-05\n",
      "Epoch 37/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0025 - mae: 0.0332 - val_loss: 0.0030 - val_mae: 0.0402 - learning_rate: 3.1250e-05\n",
      "Epoch 38/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0025 - mae: 0.0333 - val_loss: 0.0030 - val_mae: 0.0400 - learning_rate: 3.1250e-05\n",
      "Epoch 39/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0024 - mae: 0.0331 - val_loss: 0.0030 - val_mae: 0.0403 - learning_rate: 3.1250e-05\n",
      "Epoch 40/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0025 - mae: 0.0331 - val_loss: 0.0030 - val_mae: 0.0404 - learning_rate: 3.1250e-05\n",
      "Epoch 41/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0024 - mae: 0.0331 - val_loss: 0.0030 - val_mae: 0.0406 - learning_rate: 1.5625e-05\n",
      "Epoch 42/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0024 - mae: 0.0330 - val_loss: 0.0030 - val_mae: 0.0405 - learning_rate: 1.5625e-05\n",
      "Epoch 43/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0024 - mae: 0.0330 - val_loss: 0.0030 - val_mae: 0.0406 - learning_rate: 1.5625e-05\n",
      "Epoch 44/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0024 - mae: 0.0329 - val_loss: 0.0030 - val_mae: 0.0407 - learning_rate: 1.5625e-05\n",
      "Epoch 45/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0024 - mae: 0.0328 - val_loss: 0.0030 - val_mae: 0.0406 - learning_rate: 1.5625e-05\n",
      "Epoch 46/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0024 - mae: 0.0328 - val_loss: 0.0030 - val_mae: 0.0408 - learning_rate: 1.5625e-05\n",
      "Epoch 47/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0024 - mae: 0.0327 - val_loss: 0.0031 - val_mae: 0.0418 - learning_rate: 7.8125e-06\n",
      "Epoch 48/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0024 - mae: 0.0327 - val_loss: 0.0031 - val_mae: 0.0419 - learning_rate: 7.8125e-06\n",
      "Epoch 49/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0024 - mae: 0.0327 - val_loss: 0.0031 - val_mae: 0.0419 - learning_rate: 7.8125e-06\n",
      "Epoch 50/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0024 - mae: 0.0327 - val_loss: 0.0031 - val_mae: 0.0419 - learning_rate: 7.8125e-06\n",
      "Epoch 51/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0024 - mae: 0.0327 - val_loss: 0.0031 - val_mae: 0.0420 - learning_rate: 7.8125e-06\n",
      "Epoch 52/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0024 - mae: 0.0327 - val_loss: 0.0031 - val_mae: 0.0419 - learning_rate: 7.8125e-06\n",
      "Epoch 53/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0024 - mae: 0.0326 - val_loss: 0.0032 - val_mae: 0.0425 - learning_rate: 3.9063e-06\n",
      "Epoch 54/100\n",
      "324/324 - 1s - 2ms/step - loss: 0.0023 - mae: 0.0325 - val_loss: 0.0032 - val_mae: 0.0426 - learning_rate: 3.9063e-06\n",
      "‚úÖ mlp_Medium conclu√≠do - Val Loss: 0.002936\n",
      "‚úÖ Modelo TensorFlow salvo em: ./modelos/treinamento/mlp_Medium.keras\n",
      "================================================================================\n",
      "Modelo LSTM...\n",
      "[DATASET] 62 shards ‚Üí batch_size=256 (seq_len=240, lead=72)\n",
      "[DATASET] 21 shards ‚Üí batch_size=256 (seq_len=240, lead=72)\n",
      "üì¶ Dataset TFRecord carregado para treinamento.\n",
      "\n",
      "üöÄ Treinando modelo lstm_Medium...\n",
      "Epoch 1/100\n",
      "242/242 - 10s - 41ms/step - loss: 0.0500 - mae: 0.1519 - val_loss: 0.0057 - val_mae: 0.0603 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0177 - mae: 0.1059 - val_loss: 0.0085 - val_mae: 0.0660 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0088 - mae: 0.0695 - val_loss: 0.0094 - val_mae: 0.0845 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0080 - mae: 0.0686 - val_loss: 0.0105 - val_mae: 0.0901 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0086 - mae: 0.0699 - val_loss: 0.0082 - val_mae: 0.0788 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0089 - mae: 0.0716 - val_loss: 0.0065 - val_mae: 0.0679 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0064 - mae: 0.0606 - val_loss: 0.0044 - val_mae: 0.0476 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0047 - mae: 0.0504 - val_loss: 0.0045 - val_mae: 0.0470 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0071 - mae: 0.0642 - val_loss: 0.0087 - val_mae: 0.0795 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0098 - mae: 0.0771 - val_loss: 0.0052 - val_mae: 0.0542 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0057 - mae: 0.0547 - val_loss: 0.0044 - val_mae: 0.0546 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0058 - mae: 0.0563 - val_loss: 0.0053 - val_mae: 0.0614 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0111 - mae: 0.0747 - val_loss: 0.0136 - val_mae: 0.0907 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0078 - mae: 0.0615 - val_loss: 0.0047 - val_mae: 0.0466 - learning_rate: 5.0000e-04\n",
      "Epoch 15/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0040 - mae: 0.0457 - val_loss: 0.0031 - val_mae: 0.0393 - learning_rate: 5.0000e-04\n",
      "Epoch 16/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0038 - mae: 0.0442 - val_loss: 0.0034 - val_mae: 0.0431 - learning_rate: 5.0000e-04\n",
      "Epoch 17/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0038 - mae: 0.0441 - val_loss: 0.0035 - val_mae: 0.0450 - learning_rate: 5.0000e-04\n",
      "Epoch 18/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0038 - mae: 0.0443 - val_loss: 0.0045 - val_mae: 0.0549 - learning_rate: 5.0000e-04\n",
      "Epoch 19/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0037 - mae: 0.0438 - val_loss: 0.0055 - val_mae: 0.0622 - learning_rate: 5.0000e-04\n",
      "Epoch 20/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0041 - mae: 0.0463 - val_loss: 0.0043 - val_mae: 0.0534 - learning_rate: 5.0000e-04\n",
      "Epoch 21/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0036 - mae: 0.0436 - val_loss: 0.0042 - val_mae: 0.0515 - learning_rate: 5.0000e-04\n",
      "Epoch 22/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0033 - mae: 0.0420 - val_loss: 0.0031 - val_mae: 0.0415 - learning_rate: 2.5000e-04\n",
      "Epoch 23/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0032 - mae: 0.0408 - val_loss: 0.0038 - val_mae: 0.0446 - learning_rate: 2.5000e-04\n",
      "Epoch 24/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0039 - mae: 0.0447 - val_loss: 0.0035 - val_mae: 0.0435 - learning_rate: 2.5000e-04\n",
      "Epoch 25/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0041 - mae: 0.0462 - val_loss: 0.0029 - val_mae: 0.0390 - learning_rate: 2.5000e-04\n",
      "Epoch 26/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0033 - mae: 0.0408 - val_loss: 0.0032 - val_mae: 0.0418 - learning_rate: 2.5000e-04\n",
      "Epoch 27/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0033 - mae: 0.0413 - val_loss: 0.0033 - val_mae: 0.0430 - learning_rate: 2.5000e-04\n",
      "Epoch 28/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0034 - mae: 0.0416 - val_loss: 0.0033 - val_mae: 0.0435 - learning_rate: 2.5000e-04\n",
      "Epoch 29/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0034 - mae: 0.0418 - val_loss: 0.0033 - val_mae: 0.0440 - learning_rate: 2.5000e-04\n",
      "Epoch 30/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0033 - mae: 0.0418 - val_loss: 0.0033 - val_mae: 0.0446 - learning_rate: 2.5000e-04\n",
      "Epoch 31/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0033 - mae: 0.0419 - val_loss: 0.0033 - val_mae: 0.0453 - learning_rate: 2.5000e-04\n",
      "Epoch 32/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0031 - mae: 0.0395 - val_loss: 0.0028 - val_mae: 0.0376 - learning_rate: 1.2500e-04\n",
      "Epoch 33/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0030 - mae: 0.0392 - val_loss: 0.0028 - val_mae: 0.0380 - learning_rate: 1.2500e-04\n",
      "Epoch 34/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0030 - mae: 0.0394 - val_loss: 0.0028 - val_mae: 0.0385 - learning_rate: 1.2500e-04\n",
      "Epoch 35/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0030 - mae: 0.0395 - val_loss: 0.0029 - val_mae: 0.0390 - learning_rate: 1.2500e-04\n",
      "Epoch 36/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0030 - mae: 0.0397 - val_loss: 0.0029 - val_mae: 0.0394 - learning_rate: 1.2500e-04\n",
      "Epoch 37/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0031 - mae: 0.0398 - val_loss: 0.0029 - val_mae: 0.0397 - learning_rate: 1.2500e-04\n",
      "Epoch 38/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0031 - mae: 0.0399 - val_loss: 0.0029 - val_mae: 0.0399 - learning_rate: 1.2500e-04\n",
      "Epoch 39/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0030 - mae: 0.0391 - val_loss: 0.0026 - val_mae: 0.0361 - learning_rate: 6.2500e-05\n",
      "Epoch 40/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0029 - mae: 0.0384 - val_loss: 0.0026 - val_mae: 0.0361 - learning_rate: 6.2500e-05\n",
      "Epoch 41/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0029 - mae: 0.0384 - val_loss: 0.0026 - val_mae: 0.0361 - learning_rate: 6.2500e-05\n",
      "Epoch 42/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0029 - mae: 0.0384 - val_loss: 0.0026 - val_mae: 0.0361 - learning_rate: 6.2500e-05\n",
      "Epoch 43/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0029 - mae: 0.0384 - val_loss: 0.0026 - val_mae: 0.0361 - learning_rate: 6.2500e-05\n",
      "Epoch 44/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0029 - mae: 0.0384 - val_loss: 0.0026 - val_mae: 0.0362 - learning_rate: 6.2500e-05\n",
      "Epoch 45/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0029 - mae: 0.0384 - val_loss: 0.0026 - val_mae: 0.0362 - learning_rate: 6.2500e-05\n",
      "Epoch 46/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0029 - mae: 0.0381 - val_loss: 0.0026 - val_mae: 0.0359 - learning_rate: 3.1250e-05\n",
      "Epoch 47/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0029 - mae: 0.0379 - val_loss: 0.0026 - val_mae: 0.0359 - learning_rate: 3.1250e-05\n",
      "Epoch 48/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0029 - mae: 0.0379 - val_loss: 0.0026 - val_mae: 0.0359 - learning_rate: 3.1250e-05\n",
      "Epoch 49/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0029 - mae: 0.0379 - val_loss: 0.0026 - val_mae: 0.0359 - learning_rate: 3.1250e-05\n",
      "Epoch 50/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0029 - mae: 0.0379 - val_loss: 0.0026 - val_mae: 0.0359 - learning_rate: 3.1250e-05\n",
      "Epoch 51/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0029 - mae: 0.0379 - val_loss: 0.0026 - val_mae: 0.0359 - learning_rate: 3.1250e-05\n",
      "Epoch 52/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0028 - mae: 0.0377 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 1.5625e-05\n",
      "Epoch 53/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0028 - mae: 0.0377 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 1.5625e-05\n",
      "Epoch 54/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0028 - mae: 0.0377 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 1.5625e-05\n",
      "Epoch 55/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0028 - mae: 0.0377 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 1.5625e-05\n",
      "Epoch 56/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0028 - mae: 0.0377 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 1.5625e-05\n",
      "Epoch 57/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0028 - mae: 0.0377 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 1.5625e-05\n",
      "Epoch 58/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0028 - mae: 0.0376 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 7.8125e-06\n",
      "Epoch 59/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0028 - mae: 0.0376 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 7.8125e-06\n",
      "Epoch 60/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0028 - mae: 0.0376 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 7.8125e-06\n",
      "Epoch 61/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0028 - mae: 0.0376 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 7.8125e-06\n",
      "Epoch 62/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0028 - mae: 0.0376 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 7.8125e-06\n",
      "Epoch 63/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0028 - mae: 0.0376 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 7.8125e-06\n",
      "Epoch 64/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0028 - mae: 0.0376 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 3.9063e-06\n",
      "Epoch 65/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0028 - mae: 0.0376 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 3.9063e-06\n",
      "Epoch 66/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0028 - mae: 0.0376 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 3.9063e-06\n",
      "Epoch 67/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0028 - mae: 0.0376 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 3.9063e-06\n",
      "Epoch 68/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0028 - mae: 0.0376 - val_loss: 0.0026 - val_mae: 0.0357 - learning_rate: 3.9063e-06\n",
      "Epoch 69/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0028 - mae: 0.0376 - val_loss: 0.0026 - val_mae: 0.0357 - learning_rate: 3.9063e-06\n",
      "Epoch 70/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0028 - mae: 0.0376 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 1.9531e-06\n",
      "Epoch 71/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0028 - mae: 0.0375 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 1.9531e-06\n",
      "Epoch 72/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0028 - mae: 0.0375 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 1.9531e-06\n",
      "Epoch 73/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0028 - mae: 0.0375 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 1.9531e-06\n",
      "Epoch 74/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0028 - mae: 0.0375 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 1.9531e-06\n",
      "Epoch 75/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0028 - mae: 0.0375 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 1.9531e-06\n",
      "Epoch 76/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0028 - mae: 0.0375 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 1.0000e-06\n",
      "Epoch 77/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0028 - mae: 0.0375 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 1.0000e-06\n",
      "Epoch 78/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0028 - mae: 0.0375 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 1.0000e-06\n",
      "Epoch 79/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0028 - mae: 0.0375 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 1.0000e-06\n",
      "Epoch 80/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0028 - mae: 0.0375 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 1.0000e-06\n",
      "Epoch 81/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0028 - mae: 0.0375 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 1.0000e-06\n",
      "Epoch 82/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0028 - mae: 0.0375 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 1.0000e-06\n",
      "Epoch 83/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0028 - mae: 0.0375 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 1.0000e-06\n",
      "Epoch 84/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0028 - mae: 0.0375 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 1.0000e-06\n",
      "Epoch 85/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0028 - mae: 0.0375 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 1.0000e-06\n",
      "Epoch 86/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0028 - mae: 0.0375 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 1.0000e-06\n",
      "Epoch 87/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0028 - mae: 0.0375 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 1.0000e-06\n",
      "Epoch 88/100\n",
      "242/242 - 8s - 34ms/step - loss: 0.0028 - mae: 0.0375 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 1.0000e-06\n",
      "Epoch 89/100\n",
      "242/242 - 8s - 33ms/step - loss: 0.0028 - mae: 0.0375 - val_loss: 0.0026 - val_mae: 0.0358 - learning_rate: 1.0000e-06\n",
      "‚úÖ lstm_Medium conclu√≠do - Val Loss: 0.002553\n",
      "‚úÖ Modelo TensorFlow salvo em: ./modelos/treinamento/lstm_Medium.keras\n",
      "================================================================================\n",
      "Modelo TFT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 0      | train\n",
      "3  | prescalers                         | ModuleDict                      | 112    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 5.0 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 7.0 K  | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 5.2 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 16.8 K | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 16.8 K | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 16.8 K | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 16.8 K | train\n",
      "11 | lstm_encoder                       | LSTM                            | 33.3 K | train\n",
      "12 | lstm_decoder                       | LSTM                            | 33.3 K | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 8.3 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 128    | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 20.9 K | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 12.4 K | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 8.4 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 16.8 K | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 8.4 K  | train\n",
      "20 | output_layer                       | Linear                          | 65     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "226 K     Trainable params\n",
      "0         Non-trainable params\n",
      "226 K     Total params\n",
      "0.905     Total estimated model params size (MB)\n",
      "304       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ TimeSeriesDataSet (train) criado com 83831 amostras.\n",
      "üì¶ TimeSeriesDataSet (val) criado com 10478 amostras.\n",
      "üì¶ Dados TFT ‚Äî batches: train=323 | val=38\n",
      "\n",
      "üöÄ Treinando TFT preset: tft_Medium [gpu]\n",
      "Epoch 42: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 323/323 [02:48<00:00,  1.92it/s, v_num=10, train_loss_step=0.00771, val_loss=0.0153, train_loss_epoch=0.00718]\n",
      "‚úÖ tft_Medium conclu√≠do ‚Äî melhor checkpoint salvo em modelos/treinamento/TFT/tft_Medium\n",
      "‚úÖ Problema treinamento conclu√≠do ‚Äî mem√≥ria limpa\n",
      "------------------------------------------------------------\n",
      "‚è±Ô∏è  Tempo de treino linear: 66.59 segundos\n",
      "‚è±Ô∏è  Tempo de treino mlp: 38.99 segundos\n",
      "‚è±Ô∏è  Tempo de treino lstm: 721.11 segundos\n",
      "‚è±Ô∏è  Tempo de treino tft: 7280.70 segundos\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gc\n",
    "# Loop de treinamento sequencial: Linear/MLP -> MLP -> LSTM -> TFT\n",
    "\n",
    "tempo_treino = {}\n",
    "\n",
    "# Carrega configura√ß√£o de treinamento\n",
    "cfg = []\n",
    "for tmp in problemas:\n",
    "    if tmp[\"name\"] == \"treinamento\":\n",
    "        cfg = tmp\n",
    "        break\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "if not cfg:\n",
    "    print(\"sem configura√ß√£o de 'treinamento' configurada na lista de problemas\")\n",
    "else:\n",
    "    name = cfg[\"name\"]\n",
    "    print(f\"\\nüöÄ Iniciando treinamento dos modelos ...\")\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"Modelo Linear...\")\n",
    "    # Treinamento Linear\n",
    "    try:\n",
    "        tempo_treino[\"linear\"] = {}\n",
    "        tempo_treino[\"linear\"][\"inicio\"] = time.time()\n",
    "        models_linear = linear_train_pipeline(\n",
    "            problem_name=name,\n",
    "            data_dir=cfg[\"data_dir\"],\n",
    "            batch_size=256,\n",
    "            configs=configs_linear,\n",
    "        )\n",
    "        del models_linear\n",
    "        tempo_treino[\"linear\"][\"fim\"] = time.time()\n",
    "        tempo_treino[\"linear\"][\"duracao\"] = tempo_treino[\"linear\"][\"fim\"] - tempo_treino[\"linear\"][\"inicio\"]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao treinar Linear para {name}: {e}\")\n",
    "    finally:\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"Modelo MLP...\")\n",
    "    # Treinamento MLP (configs_mlp)\n",
    "    try:\n",
    "        tempo_treino[\"mlp\"] = {}\n",
    "        tempo_treino[\"mlp\"][\"inicio\"] = time.time()\n",
    "        models_mlp = linear_train_pipeline(\n",
    "            problem_name=name,\n",
    "            data_dir=cfg[\"data_dir\"],\n",
    "            batch_size=256,\n",
    "            configs=configs_mlp,\n",
    "        )\n",
    "        del models_mlp\n",
    "        tempo_treino[\"mlp\"][\"fim\"] = time.time()\n",
    "        tempo_treino[\"mlp\"][\"duracao\"] = tempo_treino[\"mlp\"][\"fim\"] - tempo_treino[\"mlp\"][\"inicio\"]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao treinar MLP (configs_mlp) para {name}: {e}\")\n",
    "    finally:\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"Modelo LSTM...\")\n",
    "    # Treinamento LSTM\n",
    "    try:\n",
    "        tempo_treino[\"lstm\"] = {}\n",
    "        tempo_treino[\"lstm\"][\"inicio\"] = time.time()\n",
    "        models_lstm = lstm_train_pipeline(\n",
    "            problem_name=name,\n",
    "            data_dir=cfg[\"data_dir\"],\n",
    "            seq_len=cfg.get(\"lag\") or cfg.get(\"seq_len\"),\n",
    "            batch_size=256,\n",
    "            configs=configs_lstm,\n",
    "        )\n",
    "        del models_lstm\n",
    "        tempo_treino[\"lstm\"][\"fim\"] = time.time()\n",
    "        tempo_treino[\"lstm\"][\"duracao\"] = tempo_treino[\"lstm\"][\"fim\"] - tempo_treino[\"lstm\"][\"inicio\"]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao treinar LSTM para {name}: {e}\")\n",
    "    finally:\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"Modelo TFT...\")\n",
    "    # Treinamento TFT (Temporal Fusion Transformer)\n",
    "    try:\n",
    "        tempo_treino[\"tft\"] = {}\n",
    "        tempo_treino[\"tft\"][\"inicio\"] = time.time()\n",
    "        models_tft = tft_train_pipeline(\n",
    "            problem_name=name,\n",
    "            data_dir=cfg[\"data_dir\"],\n",
    "            feature_cols=cfg.get(\"feats\") or cfg.get(\"feature_cols\"),\n",
    "            target_cols=cfg.get(\"tgts\") or cfg.get(\"target_cols\"),\n",
    "            seq_len=cfg.get(\"lag\"),\n",
    "            lead=cfg.get(\"lead\"),\n",
    "            batch_size=256,\n",
    "            configs=config_tft,\n",
    "        )\n",
    "        # libera refer√™ncia ao retorno (modelos serializados internamente)\n",
    "        del models_tft\n",
    "        tempo_treino[\"tft\"][\"fim\"] = time.time()\n",
    "        tempo_treino[\"tft\"][\"duracao\"] = tempo_treino[\"tft\"][\"fim\"] - tempo_treino[\"tft\"][\"inicio\"]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao treinar TFT para {name}: {e}\")\n",
    "\n",
    "    finally:\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "    print(f\"‚úÖ Problema {name} conclu√≠do ‚Äî mem√≥ria limpa\\n{'-'*60}\")\n",
    "    for modelo, tempo in tempo_treino.items():\n",
    "        duracao = tempo.get(\"duracao\", 0)\n",
    "        print(f\"‚è±Ô∏è  Tempo de treino {modelo}: {duracao:.2f} segundos\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfc_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
